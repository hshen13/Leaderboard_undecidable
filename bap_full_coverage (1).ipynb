{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "730b2243",
      "metadata": {
        "id": "730b2243"
      },
      "source": [
        "\n",
        "# BAP Experiment Notebook (Full Coverage): **All Your Models + All Your Datasets**\n",
        "This version updates the merged notebook so that it **actually plugs in everything you used**:\n",
        "\n",
        "## Models (from your v10/v12 notebooks)\n",
        "- `Qwen/Qwen3-4B`\n",
        "- `Qwen/Qwen2.5-Math-7B-Instruct`\n",
        "- `deepseek-ai/deepseek-math-7b-instruct`\n",
        "- `mistralai/Mathstral-7B-v0.1`\n",
        "- `nvidia/AceMath-7B-Instruct`\n",
        "\n",
        "## Datasets / task sources you used\n",
        "### A) Your harmonized JSONL (binary tasks)\n",
        "- `all_tasks_harmonized.jsonl` (task-level split, including `gsm8k_binary`, `aqua_binary`, `arc_challenge_binary`, `group_word_generic_easy`, …)\n",
        "- Optional: **SGU slice** (`__sgu`) driven by `complexity_family`\n",
        "\n",
        "### B) Real public benchmarks (direct HF loading)\n",
        "- `gsm8k` (test split, **answer-mode**)\n",
        "- `aqua_rat` (test split, **multi-choice-mode**)\n",
        "- `ai2_arc` (ARC-Challenge test split, **multi-choice-mode**)\n",
        "\n",
        "### C) Baseline corpora for contamination baselines (train splits)\n",
        "- `gsm8k` train\n",
        "- `aqua_rat` train\n",
        "- `ai2_arc` ARC-Challenge train\n",
        "\n",
        "---\n",
        "\n",
        "## What stays the same (BAP measurement items)\n",
        "We keep the same **evaluation modules** from the BAP design:\n",
        "- **Exp-1 Auditability stress test**: model swap, resampling/cherry-picking, code substitution, post-hoc bit editing\n",
        "- **Exp-2 Contamination evidence**: fast proxy + optional LoRA fine-tuning contamination (realism)\n",
        "- **Exp-3 Discrimination**: complexity-tiered curves using **verifier compute cost**\n",
        "- **Exp-4 Overhead**: wall time + per-item time + verifier-cost distributions\n",
        "\n",
        "> This notebook still runs in **simulation-mode attestation** by default (HMAC / optional Ed25519).\n",
        "Replace that block with real TEE remote attestation when you deploy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec262453",
      "metadata": {
        "id": "ec262453"
      },
      "source": [
        "\n",
        "## 0) Setup\n",
        "If running on Colab, uncomment install lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L0vDUKOJqeka",
      "metadata": {
        "id": "L0vDUKOJqeka"
      },
      "outputs": [],
      "source": "import os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n    print(\"Drive mounted at /content/drive\")\nelse:\n    print(\"Not running in Colab; skipping Drive mount.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a12ce9e",
      "metadata": {
        "id": "2a12ce9e"
      },
      "outputs": [],
      "source": "\n!pip -q install \"transformers>=4.40.0\" \"datasets>=2.16.0\" accelerate scipy pandas matplotlib\n!pip -q install cryptography\n\nimport os, re, json, time, math, hashlib, hmac\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nnp.random.seed(0)\nprint(\"Ready.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iOeMSas6X6BM",
      "metadata": {
        "id": "iOeMSas6X6BM"
      },
      "outputs": [],
      "source": "# --- Progress bar utilities (tqdm) ---\nimport os\nTQDM_ENABLED = os.environ.get('BAP_TQDM', '1') != '0'\ntry:\n    from tqdm.auto import tqdm  # type: ignore\nexcept Exception:\n    try:\n        import sys, subprocess\n        subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'tqdm'])\n        from tqdm.auto import tqdm  # type: ignore\n    except Exception:\n        # Fallback: no-op tqdm\n        def tqdm(it=None, *args, **kwargs):\n            return it if it is not None else range(0)\n\nprint('TQDM_ENABLED:', TQDM_ENABLED)\n"
    },
    {
      "cell_type": "code",
      "source": "import os, json, re\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\n# -------- settings (same style as your notebook) --------\nBASE_DIR = os.environ.get(\"BAP_BASE_DIR\", \"/content/drive/MyDrive/complexity_data6\")\nHARMONIZED_PATH = os.environ.get(\"BAP_HARMONIZED_JSONL\", os.path.join(BASE_DIR, \"all_tasks_harmonized.jsonl\"))\n\nMODEL_ID = os.environ.get(\"BAP_MODEL_ID\", \"Qwen/Qwen3-4B\")\nUSE_4BIT = bool(int(os.environ.get(\"BAP_4BIT\", \"0\")))\nADAPTER_PATH = os.environ.get(\"BAP_ADAPTER_PATH\", \"\").strip() or None  # optional LoRA adapter dir\n\nSGU_SUFFIX = \"__sgu\"\nSGU_COMPLEXITY_FAMILY_PATTERNS = [\n    \"strongly_generically_undecidable\",\n    \"strongly generically undecidable\",\n    \"sgu\",\n    \"undecidable\",\n]\n\nGEN_CFG_BINARY = dict(max_new_tokens=1, do_sample=False, temperature=0.0)\nGEN_CFG_ANSWER = dict(max_new_tokens=256, do_sample=False, temperature=0.0)\nGEN_CFG_SEEN = dict(max_new_tokens=4, do_sample=False, temperature=0.0)\nGEN_CFG_LABEL = dict(max_new_tokens=16, do_sample=False, temperature=0.0)\n\nDATASET_LABELS = [\n    \"gsm8k\",\n    \"sgu\",\n    \"np_hard\",\n    \"easy\",\n    \"aqua_mc_test\",\n    \"arc_challenge_mc_test\",\n    \"unknown\",\n]\n\n# -------- data structs --------\n@dataclass\nclass TaskInstance:\n    instance_id: str\n    task_type: str          # \"binary\" or \"answer\"\n    prompt: str\n    label01: Optional[int] = None\n    ground_truth: Optional[str] = None\n    meta: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass TaskDataset:\n    name: str\n    instances: List[TaskInstance]\n\n# -------- loader (harmonized JSONL) --------\ndef load_harmonized_jsonl_df(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Missing harmonized JSONL: {path}\")\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                rows.append(json.loads(line))\n    df = pd.DataFrame(rows)\n\n    def _to01(x):\n        if isinstance(x, (bool, np.bool_)):\n            return int(x)\n        if isinstance(x, (int, np.integer)):\n            return int(x != 0)\n        s = str(x).strip().lower()\n        if s in [\"1\", \"true\", \"yes\", \"y\", \"t\"]:\n            return 1\n        if s in [\"0\", \"false\", \"no\", \"n\", \"f\"]:\n            return 0\n        m = re.search(r\"[01]\", s)\n        return int(m.group(0)) if m else 0\n\n    # required columns: input, label, task\n    for req in [\"input\", \"label\", \"task\"]:\n        if req not in df.columns:\n            raise ValueError(f\"harmonized JSONL missing required column: {req}\")\n\n    df[\"label\"] = df[\"label\"].apply(_to01).astype(int)\n\n    # optional cols used for SGU slice\n    if \"complexity_family\" not in df.columns:\n        df[\"complexity_family\"] = np.nan\n\n    return df\n\ndef build_harmonized_datasets(df: pd.DataFrame, include_sgu_slice: bool = True) -> Dict[str, TaskDataset]:\n    out: Dict[str, TaskDataset] = {}\n\n    def _make(inst_df: pd.DataFrame, name: str):\n        insts = []\n        for i, r in inst_df.reset_index(drop=True).iterrows():\n            meta = {k: r[k] for k in inst_df.columns if k not in [\"input\", \"label\", \"task\"]}\n            insts.append(TaskInstance(\n                instance_id=f\"{name}-{i}\",\n                task_type=\"binary\",\n                prompt=str(r[\"input\"]),\n                label01=int(r[\"label\"]),\n                meta=meta\n            ))\n        out[name] = TaskDataset(name=name, instances=insts)\n\n    tasks = sorted(df[\"task\"].dropna().astype(str).unique().tolist())\n    for t in tasks:\n        df_t = df[df[\"task\"] == t].copy()\n        if len(df_t) == 0:\n            continue\n        _make(df_t, t)\n\n        if include_sgu_slice:\n            cf = df_t[\"complexity_family\"].fillna(\"\").astype(str).str.lower()\n            mask = np.zeros(len(df_t), dtype=bool)\n            for pat in SGU_COMPLEXITY_FAMILY_PATTERNS:\n                mask |= cf.str.contains(pat.lower(), na=False).to_numpy()\n            if mask.any():\n                _make(df_t.loc[mask].copy(), t + SGU_SUFFIX)\n\n    return out\n\n# -------- model wrapper (HF) --------\n@dataclass\nclass HFModelWrapper:\n    model_id: str\n    tokenizer: Any\n    model: Any\n\n    @staticmethod\n    def load(model_id: str, adapter_path: Optional[str] = None, use_4bit: bool = False):\n        import torch\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n\n        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n        if tok.pad_token is None:\n            tok.pad_token = tok.eos_token\n\n        kwargs = {}\n        if use_4bit:\n            try:\n                from transformers import BitsAndBytesConfig\n                kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n                kwargs[\"device_map\"] = \"auto\"\n            except Exception as e:\n                print(\"[WARN] 4bit requested but BitsAndBytes not available:\", e)\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n            device_map=\"auto\" if torch.cuda.is_available() else None,\n            **kwargs,\n        )\n        model.eval()\n\n        if adapter_path is not None:\n            from peft import PeftModel\n            model = PeftModel.from_pretrained(model, adapter_path)\n            model.eval()\n\n        return HFModelWrapper(model_id=model_id, tokenizer=tok, model=model)\n\n    def _generate(self, prompt: str, gen_cfg: Dict[str, Any]) -> str:\n        import torch\n        from transformers import GenerationConfig\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        gcfg = GenerationConfig(**gen_cfg)\n        with torch.no_grad():\n            out = self.model.generate(**inputs, generation_config=gcfg)\n        text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n        return text[len(prompt):].strip() if text.startswith(prompt) else text.strip()\n\n    # Q1\n    def ask_seen_level(self, question: str) -> str:\n        prompt = (\n            \"You will see a problem. Output ONLY a single digit: 0, 1, or 2.\\n\\n\"\n            \"2 = I have seen this exact problem before.\\n\"\n            \"1 = I have not seen this exact problem, but I have seen very similar problems.\\n\"\n            \"0 = I have never seen this problem and have not seen similar ones.\\n\\n\"\n            \"Do NOT output any other text.\\n\\n\"\n            f\"Problem:\\n{question.strip()}\\n\"\n        )\n        return self._generate(prompt, GEN_CFG_SEEN)\n\n    # Q2\n    def ask_dataset_label(self, question: str) -> str:\n        labels = \"\\n\".join(DATASET_LABELS)\n        prompt = (\n            \"You will see a problem. Output ONLY one label from the list below, EXACTLY as written:\\n\\n\"\n            f\"{labels}\\n\\n\"\n            \"Do NOT output any other text.\\n\\n\"\n            f\"Problem:\\n{question.strip()}\\n\"\n        )\n        return self._generate(prompt, GEN_CFG_LABEL)\n\n    # Q3 (the actual answer for this instance)\n    def answer_instance(self, inst: TaskInstance) -> str:\n        if inst.task_type == \"binary\":\n            prompt = (\n                \"Answer the following decision problem.\\n\"\n                \"Return ONLY a single token:\\n\"\n                \"1 if YES, 0 if NO.\\n\\n\"\n                f\"Problem:\\n{inst.prompt}\\n\\nReturn only a single token (0 or 1):\"\n            )\n            return self._generate(prompt, GEN_CFG_BINARY)\n        else:\n            prompt = (\n                \"Solve the following problem. Return ONLY the final answer.\\n\\n\"\n                f\"Problem:\\n{inst.prompt}\\n\\nFinal answer:\"\n            )\n            return self._generate(prompt, GEN_CFG_ANSWER)\n\n# -------- run: read 1 SGU question, ask the 3 questions, print --------\ndf = load_harmonized_jsonl_df(HARMONIZED_PATH)\ndatasets = build_harmonized_datasets(df, include_sgu_slice=True)\n\nsgu_keys = [k for k in datasets.keys() if k.endswith(SGU_SUFFIX)]\nif not sgu_keys:\n    raise RuntimeError(\"No __sgu slice found. Check complexity_family in your harmonized JSONL.\")\n\nds = datasets[sgu_keys[0]]\ninst = ds.instances[0]  # take one SGU question\n\nmw = HFModelWrapper.load(MODEL_ID, adapter_path=ADAPTER_PATH, use_4bit=USE_4BIT)\n\nprint(\"SGU_DATASET_KEY =\", ds.name)\nprint(\"\\n[SGU QUESTION]\\n\", inst.prompt)\n\nprint(\"\\n[Q1 seen-level 0/1/2]\\n\", mw.ask_seen_level(inst.prompt))\nprint(\"\\n[Q2 dataset-label]\\n\", mw.ask_dataset_label(inst.prompt))\nprint(\"\\n[Q3 answer]\\n\", mw.answer_instance(inst))\n",
      "metadata": {
        "id": "wMEqCFUtDwq0"
      },
      "id": "wMEqCFUtDwq0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import re\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# ==============================================================================\n# ==============================================================================\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0: return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n\n    response = full[len(prompt):] if full.startswith(prompt) else full\n    return response.split('\\n')[0].strip()\n\n# ==============================================================================\n# ==============================================================================\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# ==============================================================================\n# ==============================================================================\n\ndef run_experiment(ds, n=5):\n    print(f\"Starting Experiment on {n} samples...\\n\")\n\n    for i, inst in enumerate(ds.instances[:n]):\n        original_question = inst.prompt\n\n        solver_prompt = build_solver_prompt(original_question)\n        solver_response = generate_strict(solver_prompt, max_tokens=15)\n\n        check_prompt = build_contamination_prompt(original_question)\n        check_response = generate_strict(check_prompt, max_tokens=5)\n\n        print(\"=\" * 80)\n        print(f\"Instance ID: {inst.instance_id} (Index {i})\")\n        print(\"-\" * 40)\n\n        print(\">>> [TASK 1] MATH SOLVER PROMPT:\")\n        print(solver_prompt)\n        print(\"-\" * 20)\n        print(f\">>> [TASK 1] MODEL RESPONSE: {solver_response!r}\")\n\n        print(\"-\" * 40)\n\n        print(\">>> [TASK 2] CONTAMINATION CHECK PROMPT:\")\n        print(check_prompt)\n        print(\"-\" * 20)\n        print(f\">>> [TASK 2] MODEL RESPONSE: {check_response!r}\")\n\n        print(\"=\" * 80 + \"\\n\\n\")\n\n# ==============================================================================\n# ==============================================================================\n\n# gsm_keys = [k for k in datasets.keys() if \"gsm8k\" in k.lower()][:1]\n# gsm_ds = datasets[gsm_keys[0]]\n\nrun_experiment(gsm_ds, n=3)",
      "metadata": {
        "id": "aN2VFX9LGHHH"
      },
      "id": "aN2VFX9LGHHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import re\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# ==============================================================================\n# ==============================================================================\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# ==============================================================================\n# ==============================================================================\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# ==============================================================================\n# ==============================================================================\n\ndef run_two_questions_on_dataset(mw, ds, n=3, tag=\"DATASET\"):\n    print(f\"\\n===== Running on {tag}: {ds.name} | n={n} =====\\n\")\n    for i, inst in enumerate(ds.instances[:n]):\n        q = inst.prompt\n\n        solver_prompt = build_solver_prompt(q)\n        solver_resp = generate_strict(mw, solver_prompt, max_tokens=15)\n\n        check_prompt = build_contamination_prompt(q)\n        check_resp = generate_strict(mw, check_prompt, max_tokens=5)\n\n        print(\"=\" * 90)\n        print(f\"[{tag} #{i}] instance_id={inst.instance_id}\")\n        print(\"\\n>>> [TASK 1] SOLVER PROMPT (instruction -> question):\")\n        print(solver_prompt)\n        print(\">>> [TASK 1] MODEL RESPONSE:\", repr(solver_resp))\n\n        print(\"\\n>>> [TASK 2] CONTAMINATION PROMPT (instruction -> question):\")\n        print(check_prompt)\n        print(\">>> [TASK 2] MODEL RESPONSE:\", repr(check_resp))\n        print(\"=\" * 90, \"\\n\")\n\n# ==============================================================================\n# ==============================================================================\n\ngsm_keys = [k for k in datasets.keys() if \"gsm8k\" in k.lower() and not k.endswith(\"__sgu\")]\nif not gsm_keys:\n    raise RuntimeError(\"没在 datasets 里找到 gsm8k 任务 key。\")\ngsm_ds = datasets[gsm_keys[0]]\n\nsgu_ds = ds\n\nrun_two_questions_on_dataset(mw, gsm_ds, n=3, tag=\"GSM8K\")\nrun_two_questions_on_dataset(mw, sgu_ds, n=3, tag=\"SGU\")\n",
      "metadata": {
        "id": "bl2E7l4HrrC5"
      },
      "id": "bl2E7l4HrrC5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import gc\nimport re\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# ==============================================================================\n# ==============================================================================\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# ==============================================================================\n# ==============================================================================\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\ndef run_two_questions_on_dataset(mw, ds, n=3, tag=\"DATASET\"):\n    print(f\"\\n===== {tag}: {ds.name} | n={n} =====\\n\")\n    for i, inst in enumerate(ds.instances[:n]):\n        q = inst.prompt\n\n        solver_prompt = build_solver_prompt(q)\n        solver_resp = generate_strict(mw, solver_prompt, max_tokens=15)\n\n        check_prompt = build_contamination_prompt(q)\n        check_resp = generate_strict(mw, check_prompt, max_tokens=5)\n\n        print(\"=\" * 90)\n        print(f\"[{tag} #{i}] instance_id={inst.instance_id}\")\n        print(\"\\n>>> [TASK 1] SOLVER PROMPT:\")\n        print(solver_prompt)\n        print(\">>> [TASK 1] MODEL RESPONSE:\", repr(solver_resp))\n\n        print(\"\\n>>> [TASK 2] CONTAMINATION PROMPT:\")\n        print(check_prompt)\n        print(\">>> [TASK 2] MODEL RESPONSE:\", repr(check_resp))\n        print(\"=\" * 90, \"\\n\")\n\n# ==============================================================================\n# ==============================================================================\n\nsgu_ds = ds\n\ngsm_keys = [k for k in datasets.keys() if \"gsm8k\" in k.lower() and not k.endswith(\"__sgu\")]\ngsm_ds = datasets[gsm_keys[0]] if gsm_keys else None\n\n# ==============================================================================\n# ==============================================================================\n\nMODEL_SPECS = [\n    {\"name\": \"qwen3-4b\",        \"model_id\": \"Qwen/Qwen3-4B\",                   \"adapter\": None},\n    {\"name\": \"qwen2.5-math-7b\", \"model_id\": \"Qwen/Qwen2.5-Math-7B-Instruct\",   \"adapter\": None},\n    {\"name\": \"deepseek-math-7b\",\"model_id\": \"deepseek-ai/deepseek-math-7b-instruct\", \"adapter\": None},\n    {\"name\": \"acemath-7b\",      \"model_id\": \"nvidia/AceMath-7B-Instruct\",      \"adapter\": None},\n    {\"name\": \"Mathstral-7B\",      \"model_id\": \"mistralai/Mathstral-7B-v0.1\",      \"adapter\": None},\n    {\"name\": \"gemma-2-9b-math\", \"model_id\": \"google/gemma-2-9b-it\",            \"adapter\": None},\n]\n\n\n# ==============================================================================\n# ==============================================================================\n\ndef run_all_models(model_specs, n_gsm=3, n_sgu=3, use_4bit=False):\n    for spec in model_specs:\n        print(\"\\n\" + \"#\" * 110)\n        print(f\"LOADING MODEL: {spec['name']} | {spec['model_id']} | adapter={spec['adapter']} | 4bit={use_4bit}\")\n        print(\"#\" * 110 + \"\\n\")\n\n        mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec[\"adapter\"], use_4bit=use_4bit)\n\n        if gsm_ds is not None:\n            run_two_questions_on_dataset(mw_local, gsm_ds, n=n_gsm, tag=f\"GSM8K@{spec['name']}\")\n        else:\n            print(f\"[SKIP] No GSM8K dataset found in `datasets`, skipping GSM8K for {spec['name']}\")\n\n        run_two_questions_on_dataset(mw_local, sgu_ds, n=n_sgu, tag=f\"SGU@{spec['name']}\")\n\n        del mw_local\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nrun_all_models(MODEL_SPECS, n_gsm=3, n_sgu=3, use_4bit=USE_4BIT)\n",
      "metadata": {
        "id": "so3pAZ--uX-2"
      },
      "id": "so3pAZ--uX-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import gc\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# ==============================================================================\n# ==============================================================================\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# ==============================================================================\n# ==============================================================================\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# ==============================================================================\n# ==============================================================================\n\n_yes_pat = re.compile(r\"\\bYES\\b\", re.IGNORECASE)\n_no_pat  = re.compile(r\"\\bNO\\b\", re.IGNORECASE)\n\ndef parse_yesno(s: str):\n    if not s or not s.strip():\n        return None\n    t = s.strip().upper()\n    if \"YES\" in t:\n        return \"YES\"\n    if \"NO\" in t:\n        return \"NO\"\n    return None\n\ndef parse_012(s: str):\n    if not s or not s.strip():\n        return None\n    m = re.search(r\"\\b([012])\\b\", s.strip())\n    return m.group(1) if m else None\n\ndef expected_yesno_from_label01(label01: int):\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\n# ==============================================================================\n# ==============================================================================\n\ndef eval_model_on_ds(mw, ds, n, tag):\n    n = min(n, len(ds.instances))\n    solver_ok = 0\n    solver_fmt = 0\n    contam_fmt = 0\n    contam_counts = {\"0\": 0, \"1\": 0, \"2\": 0, \"other\": 0, \"empty\": 0}\n\n    for inst in ds.instances[:n]:\n        q = inst.prompt\n\n        # Task 1: YES/NO\n        solver_prompt = build_solver_prompt(q)\n        solver_resp = generate_strict(mw, solver_prompt, max_tokens=15)\n        pred_yesno = parse_yesno(solver_resp)\n        if pred_yesno is not None:\n            solver_fmt += 1\n            exp = expected_yesno_from_label01(inst.label01)\n            solver_ok += int(pred_yesno == exp)\n\n        # Task 2: 0/1/2\n        contam_prompt = build_contamination_prompt(q)\n        contam_resp = generate_strict(mw, contam_prompt, max_tokens=5)\n        pred012 = parse_012(contam_resp)\n\n        if contam_resp is None or contam_resp == \"\":\n            contam_counts[\"empty\"] += 1\n        elif pred012 is None:\n            contam_counts[\"other\"] += 1\n        else:\n            contam_fmt += 1\n            contam_counts[pred012] += 1\n\n    out = {\n        f\"{tag}_n\": n,\n        f\"{tag}_solver_fmt_rate\": solver_fmt / n if n else 0.0,\n        f\"{tag}_solver_acc\": solver_ok / n if n else 0.0,\n        f\"{tag}_contam_fmt_rate\": contam_fmt / n if n else 0.0,\n        f\"{tag}_contam_0\": contam_counts[\"0\"],\n        f\"{tag}_contam_1\": contam_counts[\"1\"],\n        f\"{tag}_contam_2\": contam_counts[\"2\"],\n        f\"{tag}_contam_other\": contam_counts[\"other\"],\n        f\"{tag}_contam_empty\": contam_counts[\"empty\"],\n    }\n    return out\n\n# ==============================================================================\n# ==============================================================================\n\ndef summarize_all_models(model_specs, gsm_ds, sgu_ds, n_gsm=50, n_sgu=50, use_4bit=False):\n    rows = []\n    for spec in model_specs:\n        print(\"\\n\" + \"#\" * 100)\n        print(f\"LOADING: {spec['name']} | {spec['model_id']} | 4bit={use_4bit} | adapter={spec.get('adapter')}\")\n        print(\"#\" * 100)\n\n        mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n        row = {\n            \"model\": spec[\"name\"],\n            \"model_id\": spec[\"model_id\"],\n            \"4bit\": bool(use_4bit),\n        }\n\n        if gsm_ds is not None:\n            row.update(eval_model_on_ds(mw_local, gsm_ds, n_gsm, \"gsm\"))\n        else:\n            row.update({\"gsm_n\": 0, \"gsm_solver_fmt_rate\": 0.0, \"gsm_solver_acc\": 0.0, \"gsm_contam_fmt_rate\": 0.0})\n\n        row.update(eval_model_on_ds(mw_local, sgu_ds, n_sgu, \"sgu\"))\n        rows.append(row)\n\n        del mw_local\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    df = pd.DataFrame(rows)\n\n    cols = [\n        \"model\",\"model_id\",\"4bit\",\n        \"gsm_n\",\"gsm_solver_acc\",\"gsm_solver_fmt_rate\",\"gsm_contam_fmt_rate\",\"gsm_contam_0\",\"gsm_contam_1\",\"gsm_contam_2\",\"gsm_contam_other\",\"gsm_contam_empty\",\n        \"sgu_n\",\"sgu_solver_acc\",\"sgu_solver_fmt_rate\",\"sgu_contam_fmt_rate\",\"sgu_contam_0\",\"sgu_contam_1\",\"sgu_contam_2\",\"sgu_contam_other\",\"sgu_contam_empty\",\n    ]\n    cols = [c for c in cols if c in df.columns]\n    df = df[cols]\n\n    with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 200, \"display.width\", 200):\n        print(\"\\n\\n===== SUMMARY TABLE =====\")\n        print(df.to_string(index=False))\n\n    return df\n\n# ==============================================================================\n# ==============================================================================\n\nsummary_df = summarize_all_models(\n    MODEL_SPECS,\n    gsm_ds=gsm_ds if \"gsm_ds\" in globals() else None,\n    sgu_ds=sgu_ds,\n    n_gsm=50,\n    n_sgu=50,\n    use_4bit=USE_4BIT\n)\n",
      "metadata": {
        "id": "GrbWNWj-x_Xh"
      },
      "id": "GrbWNWj-x_Xh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import gc\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\nMODEL_SPECS = [\n    {\"name\": \"qwen3-4b\",        \"model_id\": \"Qwen/Qwen3-4B\",                   \"adapter\": None},\n    {\"name\": \"deepseek-llama-8b\", \"model_id\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",   \"adapter\": None},\n    {\"name\": \"deepseek-math-7b\",\"model_id\": \"deepseek-ai/deepseek-math-7b-instruct\", \"adapter\": None},\n    {\"name\": \"acemath-7b\",      \"model_id\": \"nvidia/AceMath-7B-Instruct\",      \"adapter\": None},\n    {\"name\": \"Mathstral-7B\",      \"model_id\": \"mistralai/Mathstral-7B-v0.1\",      \"adapter\": None},\n]\n\n\n# ==============================================================================\n# ==============================================================================\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# ==============================================================================\n# ==============================================================================\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# ==============================================================================\n# ==============================================================================\n\ndef parse_yesno(s: str):\n    if not s or not s.strip():\n        return None\n    t = s.strip().upper()\n    if \"YES\" in t:\n        return \"YES\"\n    if \"NO\" in t:\n        return \"NO\"\n    return None\n\ndef parse_012(s: str):\n    if not s or not s.strip():\n        return None\n    m = re.search(r\"\\b([012])\\b\", s.strip())\n    return m.group(1) if m else None\n\ndef expected_yesno_from_label01(label01: int):\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\n# ==============================================================================\n# ==============================================================================\n\ndef eval_model_on_ds(mw, ds, n, tag, pbar=None):\n    n = min(n, len(ds.instances))\n\n    solver_ok = 0\n    solver_fmt = 0\n    solver_has_gold = 0\n\n    contam_fmt = 0\n    contam_counts = {\"0\": 0, \"1\": 0, \"2\": 0, \"other\": 0, \"empty\": 0}\n\n    it = ds.instances[:n]\n    if pbar is None:\n        it = tqdm(it, desc=f\"{tag}\", leave=False)\n\n    for inst in it:\n        q = inst.prompt\n\n        # Task 1\n        solver_prompt = build_solver_prompt(q)\n        solver_resp = generate_strict(mw, solver_prompt, max_tokens=15)\n        pred_yesno = parse_yesno(solver_resp)\n        if pred_yesno is not None:\n            solver_fmt += 1\n            if inst.label01 is not None:\n                solver_has_gold += 1\n                exp = expected_yesno_from_label01(inst.label01)\n                solver_ok += int(pred_yesno == exp)\n\n        # Task 2\n        contam_prompt = build_contamination_prompt(q)\n        contam_resp = generate_strict(mw, contam_prompt, max_tokens=5)\n        pred012 = parse_012(contam_resp)\n\n        if contam_resp is None or contam_resp == \"\":\n            contam_counts[\"empty\"] += 1\n        elif pred012 is None:\n            contam_counts[\"other\"] += 1\n        else:\n            contam_fmt += 1\n            contam_counts[pred012] += 1\n\n        if pbar is not None:\n            pbar.update(1)\n\n    solver_acc = (solver_ok / solver_has_gold) if solver_has_gold > 0 else float(\"nan\")\n\n    out = {\n        f\"{tag}_n\": n,\n        f\"{tag}_solver_fmt_rate\": solver_fmt / n if n else 0.0,\n        f\"{tag}_solver_acc\": solver_acc,\n        f\"{tag}_contam_fmt_rate\": contam_fmt / n if n else 0.0,\n        f\"{tag}_contam_0\": contam_counts[\"0\"],\n        f\"{tag}_contam_1\": contam_counts[\"1\"],\n        f\"{tag}_contam_2\": contam_counts[\"2\"],\n        f\"{tag}_contam_other\": contam_counts[\"other\"],\n        f\"{tag}_contam_empty\": contam_counts[\"empty\"],\n    }\n    return out\n\n# ==============================================================================\n# ==============================================================================\n\nfixed_groups: dict[str, list[str]] = {\n    \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n    \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n}\n\ndef pick_first_existing_dataset(group_tasks):\n    for k in group_tasks:\n        if k in datasets:\n            return datasets[k], k\n    return None, None\n\n# ==============================================================================\n# ==============================================================================\n\ndef summarize_all_models_groups_with_progress(model_specs, fixed_groups, n_each=50, use_4bit=False):\n    group_selected = {}\n    for g, task_list in fixed_groups.items():\n        ds_obj, task_key = pick_first_existing_dataset(task_list)\n        group_selected[g] = (ds_obj, task_key)\n\n    print(\"\\n[GROUP SELECTION]\")\n    active_groups = []\n    for g, (ds_obj, task_key) in group_selected.items():\n        if ds_obj is None:\n            print(f\"  - {g}: (MISSING) none of {fixed_groups[g]}\")\n        else:\n            print(f\"  - {g}: using task='{task_key}' (N={len(ds_obj.instances)})\")\n            active_groups.append(g)\n\n    total_steps = 0\n    for _ in model_specs:\n        for g in active_groups:\n            ds_obj, _ = group_selected[g]\n            total_steps += min(n_each, len(ds_obj.instances))\n\n    rows = []\n    pbar = tqdm(total=total_steps, desc=\"EVAL progress\", leave=True)\n\n    for spec in model_specs:\n        pbar.set_postfix_str(f\"loading {spec['name']}\")\n        print(\"\\n\" + \"#\" * 100)\n        print(f\"LOADING: {spec['name']} | {spec['model_id']} | 4bit={use_4bit} | adapter={spec.get('adapter')}\")\n        print(\"#\" * 100)\n\n        mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n        row = {\"model\": spec[\"name\"], \"model_id\": spec[\"model_id\"], \"4bit\": bool(use_4bit)}\n        for g in fixed_groups.keys():\n            ds_obj, task_key = group_selected[g]\n            row[f\"{g}_task\"] = task_key\n\n            if ds_obj is None:\n                row.update({\n                    f\"{g}_n\": 0,\n                    f\"{g}_solver_acc\": float(\"nan\"),\n                    f\"{g}_solver_fmt_rate\": 0.0,\n                    f\"{g}_contam_fmt_rate\": 0.0,\n                    f\"{g}_contam_0\": 0, f\"{g}_contam_1\": 0, f\"{g}_contam_2\": 0,\n                    f\"{g}_contam_other\": 0, f\"{g}_contam_empty\": 0,\n                })\n                continue\n\n            pbar.set_postfix_str(f\"{spec['name']} | {g}\")\n            stats = eval_model_on_ds(mw_local, ds_obj, n_each, g, pbar=pbar)\n            row.update(stats)\n\n        rows.append(row)\n\n        del mw_local\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    pbar.close()\n\n    df = pd.DataFrame(rows)\n\n    cols = [\"model\", \"model_id\", \"4bit\"]\n    for g in fixed_groups.keys():\n        cols += [\n            f\"{g}_task\", f\"{g}_n\",\n            f\"{g}_solver_acc\", f\"{g}_solver_fmt_rate\",\n            f\"{g}_contam_fmt_rate\",\n            f\"{g}_contam_0\", f\"{g}_contam_1\", f\"{g}_contam_2\",\n            f\"{g}_contam_other\", f\"{g}_contam_empty\",\n        ]\n    cols = [c for c in cols if c in df.columns]\n    df = df[cols]\n\n    with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 200, \"display.width\", 260):\n        print(\"\\n\\n===== SUMMARY TABLE (GROUPS) =====\")\n        print(df.to_string(index=False))\n\n    return df\n\n# ==============================================================================\n# ==============================================================================\n\nsummary_groups_df = summarize_all_models_groups_with_progress(\n    MODEL_SPECS,\n    fixed_groups=fixed_groups,\n    n_each=30,\n    use_4bit=USE_4BIT\n)\n",
      "metadata": {
        "id": "gXzhm69007Ay"
      },
      "id": "gXzhm69007Ay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import gc\nimport re\nimport pandas as pd\nimport torch\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\nfrom tqdm.auto import tqdm\n\n# =========================\n# =========================\nsgu_keys = [k for k in datasets.keys() if k.endswith(\"__sgu\")]\nif not sgu_keys:\n    raise RuntimeError(\"No __sgu dataset found in `datasets`.\")\nsgu_ds = datasets[sgu_keys[0]]\nprint(\"SGU_DATASET_KEY =\", sgu_ds.name, \"| N =\", len(sgu_ds.instances))\n\n# =========================\n# =========================\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.tokenizer = tokenizer\n        self.stop_token_ids = []\n        for s in stop_strings:\n            encoded = tokenizer.encode(s, add_special_tokens=False)\n            if encoded:\n                self.stop_token_ids.append(encoded[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok = mw.tokenizer\n    model = mw.model\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper])\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# =========================\n# =========================\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following math problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit:\\n\"\n        \"0 = SEEN the same question before\\n\"\n        \"1 = See similiar question before but not the same question\\n\"\n        \"2 = Not seen similar question before\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# =========================\n# =========================\ndef parse_yesno(s: str):\n    if not s or not s.strip():\n        return None\n    t = s.strip().upper()\n    if \"YES\" in t:\n        return \"YES\"\n    if \"NO\" in t:\n        return \"NO\"\n    return None\n\ndef parse_012(s: str):\n    if not s or not s.strip():\n        return None\n    m = re.search(r\"\\b([012])\\b\", s.strip())\n    return m.group(1) if m else None\n\ndef expected_yesno_from_label01(label01: int):\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\n# =========================\n# =========================\ndef eval_one_model_on_sgu(spec, sgu_ds, n=50, use_4bit=False):\n    mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n    n = min(n, len(sgu_ds.instances))\n    solver_ok = 0\n    solver_fmt = 0\n    solver_has_gold = 0\n\n    contam_fmt = 0\n    contam_counts = {\"0\": 0, \"1\": 0, \"2\": 0, \"other\": 0, \"empty\": 0}\n\n    for inst in tqdm(sgu_ds.instances[:n], desc=f\"SGU@{spec['name']}\", leave=False):\n        q = inst.prompt\n\n        # Task 1\n        solver_resp = generate_strict(mw_local, build_solver_prompt(q), max_tokens=15)\n        pred_yesno = parse_yesno(solver_resp)\n        if pred_yesno is not None:\n            solver_fmt += 1\n            if inst.label01 is not None:\n                solver_has_gold += 1\n                solver_ok += int(pred_yesno == expected_yesno_from_label01(inst.label01))\n\n        # Task 2\n        contam_resp = generate_strict(mw_local, build_contamination_prompt(q), max_tokens=5)\n        pred012 = parse_012(contam_resp)\n        if contam_resp is None or contam_resp == \"\":\n            contam_counts[\"empty\"] += 1\n        elif pred012 is None:\n            contam_counts[\"other\"] += 1\n        else:\n            contam_fmt += 1\n            contam_counts[pred012] += 1\n\n    solver_acc = (solver_ok / solver_has_gold) if solver_has_gold > 0 else float(\"nan\")\n\n    row = {\n        \"model\": spec[\"name\"],\n        \"model_id\": spec[\"model_id\"],\n        \"4bit\": bool(use_4bit),\n        \"sgu_task\": sgu_ds.name,\n        \"sgu_n\": n,\n        \"sgu_solver_acc\": solver_acc,\n        \"sgu_solver_fmt_rate\": solver_fmt / n if n else 0.0,\n        \"sgu_contam_fmt_rate\": contam_fmt / n if n else 0.0,\n        \"sgu_contam_0\": contam_counts[\"0\"],\n        \"sgu_contam_1\": contam_counts[\"1\"],\n        \"sgu_contam_2\": contam_counts[\"2\"],\n        \"sgu_contam_other\": contam_counts[\"other\"],\n        \"sgu_contam_empty\": contam_counts[\"empty\"],\n    }\n\n    # cleanup\n    del mw_local\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return row\n\ndef run_sgu_only(model_specs, sgu_ds, n_each=50, use_4bit=False):\n    rows = []\n    for spec in model_specs:\n        print(\"\\n\" + \"#\" * 100)\n        print(f\"LOADING SGU MODEL: {spec['name']} | {spec['model_id']} | 4bit={use_4bit}\")\n        print(\"#\" * 100)\n        rows.append(eval_one_model_on_sgu(spec, sgu_ds, n=n_each, use_4bit=use_4bit))\n\n    df = pd.DataFrame(rows)\n    with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 200, \"display.width\", 200):\n        print(\"\\n\\n===== SGU ONLY SUMMARY =====\")\n        print(df.to_string(index=False))\n    return df\n\nsgu_only_df = run_sgu_only(MODEL_SPECS, sgu_ds, n_each=50, use_4bit=USE_4BIT)\n",
      "metadata": {
        "id": "tod0UJ6hTEWv"
      },
      "id": "tod0UJ6hTEWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\nimport pandas as pd\nfixed_groups: dict[str, list[str]] = {\n    \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n    \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n}\nassert \"summary_groups_df\" in globals(), \"找不到 summary_groups_df（你的 GROUPS 汇总表 DataFrame）\"\nassert \"sgu_only_df\" in globals(), \"找不到 sgu_only_df（你的 SGU ONLY 汇总表 DataFrame）\"\n\nfor df in [summary_groups_df, sgu_only_df]:\n    for c in [\"model\", \"model_id\", \"4bit\"]:\n        assert c in df.columns, f\"缺列 {c} in {df.columns}\"\n    df[\"model\"] = df[\"model\"].astype(str)\n    df[\"model_id\"] = df[\"model_id\"].astype(str)\n    df[\"4bit\"] = df[\"4bit\"].astype(bool)\n\nsgu_cols = [c for c in sgu_only_df.columns if c.startswith(\"sgu_\")] + [\"model\", \"model_id\", \"4bit\"]\nsgu_small = sgu_only_df[sgu_cols].copy()\n\nmerged = summary_groups_df.merge(\n    sgu_small,\n    on=[\"model\", \"model_id\", \"4bit\"],\n    how=\"left\",\n    suffixes=(\"\", \"_sgu_dup\")\n)\n\ndup_cols = [c for c in merged.columns if c.endswith(\"_sgu_dup\")]\nif dup_cols:\n    merged.drop(columns=dup_cols, inplace=True)\n\ncsv_text = merged.to_csv(index=False)\nprint(csv_text)\n\nprint(merged.to_string(index=False))\n",
      "metadata": {
        "id": "41XuNePCqqOs"
      },
      "id": "41XuNePCqqOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import gc\n# import re\n# import pandas as pd\n# import torch\n# from tqdm.auto import tqdm\n# from transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# # =========================\n# # =========================\n\n# class StopOnTokens(StoppingCriteria):\n#     def __init__(self, tokenizer, stop_strings):\n#         self.stop_token_ids = []\n#         for s in stop_strings:\n#             ids = tokenizer.encode(s, add_special_tokens=False)\n#             if ids:\n#                 self.stop_token_ids.append(ids[-1])\n\n#     def __call__(self, input_ids, scores, **kwargs):\n#         if input_ids.shape[1] == 0:\n#             return False\n#         return input_ids[0, -1].item() in self.stop_token_ids\n\n# def generate_strict(mw, prompt: str, max_tokens: int = 10):\n#     tok, model = mw.tokenizer, mw.model\n#     inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n#     stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n#     gen_config = GenerationConfig(\n#         do_sample=False, temperature=0.0, max_new_tokens=max_tokens,\n#         pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id\n#     )\n\n#     with torch.no_grad():\n#         out = model.generate(**inputs, generation_config=gen_config,\n#                              stopping_criteria=StoppingCriteriaList([stopper]))\n#     full = tok.decode(out[0], skip_special_tokens=True)\n#     resp = full[len(prompt):] if full.startswith(prompt) else full\n#     return resp.split(\"\\n\")[0].strip()\n\n# def build_solver_prompt(question: str) -> str:\n#     instruction = (\n#         \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n#         \"Do NOT explain. Do NOT show work.\\n\"\n#     )\n#     return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\n# def build_contamination_prompt(question: str) -> str:\n#     instruction = (\n#         \"You are an AI assistant. Analyze the following math problem.\\n\"\n#         \"Tell me if you have seen this problem during your training data.\\n\"\n#         \"Reply with strictly ONLY a single digit:\\n\"\n#         \"0 = SEEN the same question before\\n\"\n#         \"1 = See similiar question before but not the same question\\n\"\n#         \"2 = Not seen similar question before\\n\"\n#         \"Ignore the YES/NO question in the following question even if it asks. \"\n#     )\n#     return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (0, 1, or 2):\"\n\n# def parse_yesno(s: str):\n#     if not s: return None\n#     t = s.strip().upper()\n#     if \"YES\" in t: return \"YES\"\n#     if \"NO\" in t:  return \"NO\"\n#     return None\n\n# def parse_012(s: str):\n#     if not s: return None\n#     m = re.search(r\"\\b([012])\\b\", s.strip())\n#     return m.group(1) if m else None\n\n# def expected_yesno_from_label01(label01: int):\n#     return \"YES\" if int(label01) == 1 else \"NO\"\n\n\n# fixed_groups: dict[str, list[str]] = {\n#     \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n#     \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n#     \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n#     \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n# }\n# # =========================\n# # =========================\n# def pick_first_existing_dataset(task_list):\n#     for k in task_list:\n#         if k in datasets:\n#             return datasets[k], k\n#     return None, None\n\n# group_selected = {}\n# for g, task_list in fixed_groups.items():\n#     ds_obj, task_key = pick_first_existing_dataset(task_list)\n#     group_selected[g] = (ds_obj, task_key)\n\n# print(\"[GROUP SELECTION]\")\n# for g, (ds_obj, task_key) in group_selected.items():\n#     print(f\"  - {g}: {task_key if task_key else 'MISSING'}\")\n\n# # =========================\n# # =========================\n# def collect_records_for_model(spec, n_each=30, use_4bit=False):\n#     mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n#     recs = []\n#     for g, (ds_obj, task_key) in group_selected.items():\n#         if ds_obj is None:\n#             continue\n#         n = min(n_each, len(ds_obj.instances))\n#         for inst in tqdm(ds_obj.instances[:n], desc=f\"{spec['name']}|{g}\", leave=False):\n#             q = inst.prompt\n\n#             # solver\n#             solver_resp = generate_strict(mw_local, build_solver_prompt(q), max_tokens=15)\n#             pred_yesno = parse_yesno(solver_resp)\n\n#             correct = None\n#             if pred_yesno is not None and inst.label01 is not None:\n#                 correct = int(pred_yesno == expected_yesno_from_label01(inst.label01))\n\n#             # contamination\n#             contam_resp = generate_strict(mw_local, build_contamination_prompt(q), max_tokens=5)\n#             contam = parse_012(contam_resp)  # \"0\"/\"1\"/\"2\" or None\n\n#             recs.append({\n#                 \"model\": spec[\"name\"],\n#                 \"model_id\": spec[\"model_id\"],\n#                 \"group\": g,\n#                 \"task\": task_key,\n#                 \"solver_pred\": pred_yesno,\n#                 \"solver_correct\": correct,   # 1/0/None\n#                 \"contam\": contam,            # \"0\"/\"1\"/\"2\"/None\n#                 \"solver_raw\": solver_resp,\n#                 \"contam_raw\": contam_resp,\n#             })\n\n#     # cleanup\n#     del mw_local\n#     gc.collect()\n#     if torch.cuda.is_available():\n#         torch.cuda.empty_cache()\n\n#     return pd.DataFrame(recs)\n\n# def run_conditional_accuracy(model_specs, n_each=30, use_4bit=False):\n#     all_df = []\n#     for spec in model_specs:\n#         print(\"\\n\" + \"#\" * 90)\n#         print(f\"Collecting joint records: {spec['name']} | {spec['model_id']} | 4bit={use_4bit}\")\n#         dfm = collect_records_for_model(spec, n_each=n_each, use_4bit=use_4bit)\n#         all_df.append(dfm)\n#     df = pd.concat(all_df, ignore_index=True)\n\n#     df_valid = df[df[\"solver_correct\"].notna() & df[\"contam\"].isin([\"0\",\"1\",\"2\"])].copy()\n#     df_valid[\"solver_correct\"] = df_valid[\"solver_correct\"].astype(int)\n\n#     cover = df.groupby([\"model\",\"group\"]).size().rename(\"total\").reset_index()\n#     used  = df_valid.groupby([\"model\",\"group\"]).size().rename(\"usable\").reset_index()\n#     cov = cover.merge(used, on=[\"model\",\"group\"], how=\"left\").fillna({\"usable\":0})\n#     cov[\"usable\"] = cov[\"usable\"].astype(int)\n#     cov[\"usable_rate\"] = cov[\"usable\"] / cov[\"total\"]\n\n#     overall = (\n#         df_valid.groupby([\"model\",\"contam\"])[\"solver_correct\"]\n#         .agg(acc=\"mean\", n=\"count\")\n#         .reset_index()\n#         .sort_values([\"model\",\"contam\"])\n#     )\n\n#     by_group = (\n#         df_valid.groupby([\"model\",\"group\",\"contam\"])[\"solver_correct\"]\n#         .agg(acc=\"mean\", n=\"count\")\n#         .reset_index()\n#         .sort_values([\"model\",\"group\",\"contam\"])\n#     )\n\n#     print(\"\\n===== COVERAGE (how many samples have BOTH valid solver and valid contam) =====\")\n#     with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 50, \"display.width\", 220):\n#         print(cov.sort_values([\"model\",\"group\"]).to_string(index=False))\n\n#     print(\"\\n===== OVERALL conditional accuracy by self-report (0/1/2) =====\")\n#     with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 50, \"display.width\", 220):\n#         print(overall.to_string(index=False))\n\n#     print(\"\\n===== BY-GROUP conditional accuracy by self-report (0/1/2) =====\")\n#     with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 80, \"display.width\", 260):\n#         print(by_group.to_string(index=False))\n\n#     return df, df_valid, cov, overall, by_group\n\n# raw_df, valid_df, coverage_df, overall_df, by_group_df = run_conditional_accuracy(\n#     MODEL_SPECS,\n#     n_each=50,\n#     use_4bit=USE_4BIT\n# )\n",
      "metadata": {
        "id": "I82CLiSGscyJ"
      },
      "id": "I82CLiSGscyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import random\n# import pandas as pd\n# from collections import Counter\n# from tqdm.auto import tqdm\n# import gc\n# import torch\n\n\n# import re\n\n# # =========================\n# # =========================\n# def build_contamination_prompt(question: str) -> str:\n#     instruction = (\n#         \"You are an AI assistant. Analyze the following problem.\\n\"\n#         \"Tell me if you have seen this problem during your training data.\\n\"\n#         \"Reply with strictly ONLY a single digit from {3, 5, 7}:\\n\"\n#         \"3 = SEEN the same question before\\n\"\n#         \"5 = Seen very similar questions before but not the same question\\n\"\n#         \"7 = Not seen similar question before\\n\"\n#         \"Ignore any YES/NO question in the content even if it asks.\\n\"\n#     )\n#     return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (3, 5, or 7):\"\n\n# # =========================\n# # =========================\n# def parse_357(s: str):\n#     if not s:\n#         return None\n#     m = re.search(r\"\\b([357])\\b\", s.strip())\n#     return m.group(1) if m else None\n\n\n# # =========================\n# # =========================\n# SGU_TASKS = [\n#     \"sgu_collatz_aligned\", \"sgu_collatz_aligned__sgu\",\n#     \"sgu_index_empty_language\", \"sgu_index_empty_language__sgu\",\n#     \"sgu_index_total_halt\", \"sgu_index_total_halt__sgu\",\n#     \"sgu_semigroup_wp_amp\", \"sgu_semigroup_wp_amp__sgu\",\n#     \"tm_generic_halt\", \"tm_hard_halt\",\n# ]\n\n# # =========================\n# # =========================\n# fixed_groups = {\n#     \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n#     \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n#     \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n#     \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n#     \"sgu\": SGU_TASKS,\n# }\n\n# def pick_first_existing_dataset(task_list):\n#     for k in task_list:\n#         if k in datasets:\n#             return k\n#     return None\n\n# # =========================\n# # =========================\n\n# def sample_group_instances(group_name, n_each, seed=0, sgu_sampling=\"mix_pool\"):\n#     \"\"\"\n#       {group, task, inst}\n#     \"\"\"\n#     rng = random.Random(seed)\n\n#     if group_name != \"sgu\":\n#         task_key = pick_first_existing_dataset(fixed_groups[group_name])\n#         if task_key is None:\n#             return [], Counter()\n#         ds = datasets[task_key]\n#         n = min(n_each, len(ds.instances))\n#         idxs = list(range(len(ds.instances)))\n#         rng.shuffle(idxs)\n#         idxs = idxs[:n]\n#         samples = [{\"group\": group_name, \"task\": task_key, \"inst\": ds.instances[i]} for i in idxs]\n#         return samples, Counter([task_key]*len(samples))\n\n#     sgu_existing = [k for k in fixed_groups[\"sgu\"] if k in datasets]\n#     if not sgu_existing:\n#         return [], Counter()\n\n#     if sgu_sampling == \"mix_pool\":\n#         pool = []\n#         for tk in sgu_existing:\n#             ds = datasets[tk]\n#             for inst in ds.instances:\n#                 pool.append((tk, inst))\n#         rng.shuffle(pool)\n#         pool = pool[:min(n_each, len(pool))]\n#         samples = [{\"group\": \"sgu\", \"task\": tk, \"inst\": inst} for (tk, inst) in pool]\n#         return samples, Counter([tk for tk, _ in pool])\n\n#     elif sgu_sampling == \"uniform_task\":\n#         per_task_idxs = {}\n#         for tk in sgu_existing:\n#             idxs = list(range(len(datasets[tk].instances)))\n#             rng.shuffle(idxs)\n\n#         samples = []\n#         counts = Counter()\n#         for _ in range(n_each * 20):\n#             if len(samples) >= n_each:\n#                 break\n#             tk = rng.choice(sgu_existing)\n#             if not per_task_idxs[tk]:\n#                 continue\n#             i = per_task_idxs[tk].pop()\n#             samples.append({\"group\": \"sgu\", \"task\": tk, \"inst\": datasets[tk].instances[i]})\n#             counts[tk] += 1\n\n#         return samples, counts\n\n#     else:\n#         raise ValueError(\"sgu_sampling must be 'mix_pool' or 'uniform_task'\")\n\n# # =========================\n# # =========================\n# def eval_samples_for_model(mw_local, samples, model_name):\n#     \"\"\"\n#     samples: list of dict {group, task, inst}\n#     \"\"\"\n#     recs = []\n#     if not samples:\n#         return recs\n\n#     groups = {s[\"group\"] for s in samples}\n#     group_tag = list(groups)[0] if len(groups) == 1 else \"mixed\"\n\n#     for s in tqdm(samples, desc=f\"{model_name}|{group_tag}\", leave=False):\n#         g = s[\"group\"]\n#         tk = s[\"task\"]\n#         inst = s[\"inst\"]\n#         q = inst.prompt\n\n#         # Task 1: solver\n#         solver_resp = generate_strict(mw_local, build_solver_prompt(q), max_tokens=15)\n#         pred_yesno = parse_yesno(solver_resp)\n\n#         correct = None\n#         if pred_yesno is not None and inst.label01 is not None:\n#             correct = int(pred_yesno == expected_yesno_from_label01(inst.label01))\n\n#         # Task 2: contamination\n#         contam_resp = generate_strict(mw_local, build_contamination_prompt(q), max_tokens=5)\n#         contam = parse_012(contam_resp)\n\n#         recs.append({\n#             \"model\": model_name,\n#             \"group\": g,\n#             \"task\": tk,\n#             \"solver_pred\": pred_yesno,\n#             \"solver_correct\": correct,\n#             \"contam\": contam,\n#             \"solver_raw\": solver_resp,\n#             \"contam_raw\": contam_resp,\n#         })\n\n#     return recs\n\n# # =========================\n# # =========================\n# def run_all_models_with_sgu_mixed(model_specs, n_each=50, seed=0, sgu_sampling=\"mix_pool\", use_4bit=False):\n#     group_samples = {}\n#     group_task_counts = {}\n\n#     for g in fixed_groups.keys():\n#         samples, counts = sample_group_instances(g, n_each=n_each, seed=seed + hash(g) % 100000, sgu_sampling=sgu_sampling)\n#         group_samples[g] = samples\n#         group_task_counts[g] = counts\n\n#     print(\"\\n===== SAMPLING SUMMARY =====\")\n#     for g in fixed_groups.keys():\n#         print(f\"\\n[{g}] sampled={len(group_samples[g])}\")\n#         if g == \"sgu\":\n#             print(\"SGU subtask counts:\")\n#             for k, v in group_task_counts[g].most_common():\n#                 print(f\"  - {k}: {v}\")\n\n#     all_records = []\n\n#     for spec in model_specs:\n#         print(\"\\n\" + \"#\" * 90)\n#         print(f\"LOADING: {spec['name']} | {spec['model_id']} | 4bit={use_4bit} | SGU_sampling={sgu_sampling}\")\n#         print(\"#\" * 90)\n\n#         mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n#         for g, samples in group_samples.items():\n#             if not samples:\n#                 continue\n#             recs = eval_samples_for_model(mw_local, samples, spec[\"name\"])\n#             all_records.extend(recs)\n\n#         # cleanup\n#         del mw_local\n#         gc.collect()\n#         if torch.cuda.is_available():\n#             torch.cuda.empty_cache()\n\n#     df = pd.DataFrame(all_records)\n\n#     task_freq = df.groupby([\"group\",\"task\"]).size().rename(\"count\").reset_index().sort_values([\"group\",\"count\"], ascending=[True, False])\n#     print(\"\\n===== ACTUAL TASK FREQUENCY IN EVAL (all models concatenated; per-model same samples) =====\")\n#     with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 140):\n#         print(task_freq.to_string(index=False))\n\n#     return df\n\n# # =========================\n# # =========================\n# raw_df_mixed = run_all_models_with_sgu_mixed(\n#     MODEL_SPECS,\n#     n_each=50,\n#     seed=123,\n#     use_4bit=USE_4BIT\n# )\n\n# df_valid = raw_df_mixed[ raw_df_mixed[\"solver_correct\"].notna() & raw_df_mixed[\"contam\"].isin([\"0\",\"1\",\"2\"]) ].copy()\n# # =========================\n# # =========================\n\n# import pandas as pd\n\n\n# df = raw_df_mixed.copy()\n\n# # solver_correct: None/0/1\n# # contam: \"0\"/\"1\"/\"2\"/None\n# df_valid = df[df[\"solver_correct\"].notna() & df[\"contam\"].isin([\"0\",\"1\",\"2\"])].copy()\n# df_valid[\"solver_correct\"] = df_valid[\"solver_correct\"].astype(int)\n\n# print(\"TOTAL records:\", len(df), \"| VALID records:\", len(df_valid), \"| valid_rate:\", len(df_valid)/max(1,len(df)))\n\n# cover = df.groupby([\"model\",\"group\",\"task\"]).size().rename(\"total\").reset_index()\n# used  = df_valid.groupby([\"model\",\"group\",\"task\"]).size().rename(\"usable\").reset_index()\n# coverage = cover.merge(used, on=[\"model\",\"group\",\"task\"], how=\"left\").fillna({\"usable\":0})\n# coverage[\"usable\"] = coverage[\"usable\"].astype(int)\n# coverage[\"usable_rate\"] = coverage[\"usable\"] / coverage[\"total\"]\n\n# print(\"\\n===== COVERAGE by model/group/task (valid solver + valid contam) =====\")\n# with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 20, \"display.width\", 220):\n#     print(coverage.sort_values([\"model\",\"group\",\"task\"]).to_string(index=False))\n\n# overall = (\n#     df_valid.groupby([\"model\",\"contam\"])[\"solver_correct\"]\n#     .agg(acc=\"mean\", n=\"count\")\n#     .reset_index()\n#     .sort_values([\"model\",\"contam\"])\n# )\n\n# print(\"\\n===== OVERALL conditional accuracy by self-report (0/1/2) =====\")\n# with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 120):\n#     print(overall.to_string(index=False))\n\n# by_group = (\n#     df_valid.groupby([\"model\",\"group\",\"contam\"])[\"solver_correct\"]\n#     .agg(acc=\"mean\", n=\"count\")\n#     .reset_index()\n#     .sort_values([\"model\",\"group\",\"contam\"])\n# )\n\n# print(\"\\n===== BY-GROUP conditional accuracy by self-report (0/1/2) =====\")\n# with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 12, \"display.width\", 160):\n#     print(by_group.to_string(index=False))\n\n# by_task = (\n#     df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"])[\"solver_correct\"]\n#     .agg(acc=\"mean\", n=\"count\")\n#     .reset_index()\n#     .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n# )\n\n# print(\"\\n===== BY-TASK conditional accuracy by self-report (0/1/2) =====\")\n# with pd.option_context(\"display.max_rows\", 20000, \"display.max_columns\", 14, \"display.width\", 220):\n#     print(by_task.to_string(index=False))\n\n# dist_by_group = (\n#     df_valid.groupby([\"model\",\"group\",\"contam\"]).size()\n#     .rename(\"count\")\n#     .reset_index()\n#     .sort_values([\"model\",\"group\",\"contam\"])\n# )\n\n# print(\"\\n===== BY-GROUP self-report distribution counts (VALID samples) =====\")\n# with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 10, \"display.width\", 140):\n#     print(dist_by_group.to_string(index=False))\n\n# dist_by_task = (\n#     df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"]).size()\n#     .rename(\"count\")\n#     .reset_index()\n#     .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n# )\n\n# print(\"\\n===== BY-TASK self-report distribution counts (VALID samples) =====\")\n# with pd.option_context(\"display.max_rows\", 20000, \"display.max_columns\", 12, \"display.width\", 180):\n#     print(dist_by_task.to_string(index=False))\n\n# first_model = df[\"model\"].iloc[0]\n# df_one = df[df[\"model\"] == first_model].copy()\n\n# sampled_task_counts = (\n#     df_one.groupby([\"group\",\"task\"]).size()\n#     .rename(\"sampled_n\")\n#     .reset_index()\n#     .sort_values([\"group\",\"sampled_n\"], ascending=[True, False])\n# )\n\n# print(\"\\n===== SAMPLED COUNTS per group/task (from one model; represents the shared sampled set) =====\")\n# with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 10, \"display.width\", 140):\n#     print(sampled_task_counts.to_string(index=False))\n\n# overall_wide = overall.pivot(index=\"model\", columns=\"contam\", values=\"acc\").reset_index()\n# overall_wide = overall_wide.rename(columns={\"0\":\"acc_if_0\", \"1\":\"acc_if_1\", \"2\":\"acc_if_2\"})\n\n# print(\"\\n===== OVERALL (wide) =====\")\n# with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 20, \"display.width\", 120):\n#     print(overall_wide.to_string(index=False))\n\n",
      "metadata": {
        "id": "VqRIkIp8zbQI"
      },
      "id": "VqRIkIp8zbQI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# =========================\n# FULL SCRIPT: SGU-mixed evaluation + contamination self-report as {3,5,7}\n# - mixes SGU subtasks into one pool and samples n_each questions total for SGU\n# - for other groups: picks the first existing task and samples n_each\n# - runs all models, collects per-question joint records:\n#     (solver YES/NO correctness) + (contam self-report 3/5/7)\n# - prints: sampling summary, task frequency, coverage, conditional accuracies,\n#           distribution counts, and wide overall table\n# - DOES NOT SAVE anything\n#\n# PREREQS expected in your notebook:\n#   - datasets: Dict[str, TaskDataset] with TaskDataset.instances of TaskInstance(prompt,label01,...)\n#   - HFModelWrapper with .load() and mw.tokenizer / mw.model\n#   - MODEL_SPECS list\n#   - USE_4BIT boolean\n#   - generate_strict(...) already defined OR define it below (recommended)\n#   - build_solver_prompt(...) and parse_yesno(...) already defined OR define below\n#   If you already defined generate_strict/build_solver_prompt/parse_yesno earlier,\n#   you can keep them; this script defines them for completeness.\n# =========================\n\nimport random\nimport re\nimport gc\nimport torch\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm.auto import tqdm\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# =========================\n# 0) Stop + strict generation (first-line cut)\n# =========================\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.stop_token_ids = []\n        for s in stop_strings:\n            ids = tokenizer.encode(s, add_special_tokens=False)\n            if ids:\n                self.stop_token_ids.append(ids[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok, model = mw.tokenizer, mw.model\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper]),\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# =========================\n# 1) Prompts + parsers\n# =========================\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef parse_yesno(s: str):\n    if not s:\n        return None\n    t = s.strip().upper()\n    if \"YES\" in t:\n        return \"YES\"\n    if \"NO\" in t:\n        return \"NO\"\n    return None\n\ndef expected_yesno_from_label01(label01: int):\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\n# ---- contamination: use 3/5/7 ----\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit from {3, 5, 7}:\\n\"\n        \"3 = SEEN the same question before\\n\"\n        \"5 = Seen very similar questions before but not the same question\\n\"\n        \"7 = Not seen similar question before\\n\"\n        \"Ignore any YES/NO question in the content even if it asks.\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (3, 5, or 7):\"\n\ndef parse_357(s: str):\n    if not s:\n        return None\n    m = re.search(r\"\\b([357])\\b\", s.strip())\n    return m.group(1) if m else None\n\n# =========================\n# 2) Groups + SGU subtasks (SGU combined)\n# =========================\nSGU_TASKS = [\n    \"sgu_collatz_aligned\", \"sgu_collatz_aligned__sgu\",\n    \"sgu_index_empty_language\", \"sgu_index_empty_language__sgu\",\n    \"sgu_index_total_halt\", \"sgu_index_total_halt__sgu\",\n    \"sgu_semigroup_wp_amp\", \"sgu_semigroup_wp_amp__sgu\",\n    \"tm_generic_halt\", \"tm_hard_halt\",\n]\n\nfixed_groups = {\n    \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n    \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n    \"sgu\": SGU_TASKS,\n}\n\ndef pick_first_existing_dataset(task_list):\n    for k in task_list:\n        if k in datasets:\n            return k\n    return None\n\n# =========================\n# 3) Sampling\n#    - non-SGU: pick first existing task and sample n_each w/o replacement\n#    - SGU: mix subtasks into one pool and sample n_each total (mix_pool) OR\n#           choose task uniformly then sample inside (uniform_task)\n# =========================\nMIX_GROUPS = {\"sgu\", \"np_time\", \"p_time\"}\n\ndef sample_group_instances(group_name, n_each, seed=0, sgu_sampling=\"mix_pool\"):\n    rng = random.Random(seed)\n\n    if group_name in MIX_GROUPS:\n        existing = [k for k in fixed_groups[group_name] if k in datasets]\n        if not existing:\n            return [], Counter()\n\n        if sgu_sampling == \"mix_pool\":\n            pool = []\n            for tk in existing:\n                for inst in datasets[tk].instances:\n                    pool.append((tk, inst))\n            rng.shuffle(pool)\n            pool = pool[:min(n_each, len(pool))]\n            samples = [{\"group\": group_name, \"task\": tk, \"inst\": inst} for (tk, inst) in pool]\n            return samples, Counter([tk for tk, _ in pool])\n\n        elif sgu_sampling == \"uniform_task\":\n            per_task_idxs = {}\n            for tk in existing:\n                idxs = list(range(len(datasets[tk].instances)))\n                rng.shuffle(idxs)\n                per_task_idxs[tk] = idxs\n\n            samples = []\n            counts = Counter()\n            for _ in range(n_each * 30):\n                if len(samples) >= n_each:\n                    break\n                tk = rng.choice(existing)\n                if not per_task_idxs[tk]:\n                    continue\n                i = per_task_idxs[tk].pop()\n                samples.append({\"group\": group_name, \"task\": tk, \"inst\": datasets[tk].instances[i]})\n                counts[tk] += 1\n            return samples, counts\n\n        else:\n            raise ValueError(\"sgu_sampling must be 'mix_pool' or 'uniform_task'\")\n\n    task_key = pick_first_existing_dataset(fixed_groups[group_name])\n    if task_key is None:\n        return [], Counter()\n    ds = datasets[task_key]\n    n = min(n_each, len(ds.instances))\n    idxs = list(range(len(ds.instances)))\n    rng.shuffle(idxs)\n    idxs = idxs[:n]\n    samples = [{\"group\": group_name, \"task\": task_key, \"inst\": ds.instances[i]} for i in idxs]\n    return samples, Counter([task_key] * len(samples))\n\n# =========================\n# 4) Eval one model on a sample list\n# =========================\ndef eval_samples_for_model(mw_local, samples, model_name):\n    recs = []\n    if not samples:\n        return recs\n\n    groups = {s[\"group\"] for s in samples}\n    group_tag = list(groups)[0] if len(groups) == 1 else \"mixed\"\n\n    for s in tqdm(samples, desc=f\"{model_name}|{group_tag}\", leave=False):\n        g = s[\"group\"]\n        tk = s[\"task\"]\n        inst = s[\"inst\"]\n        q = inst.prompt\n\n        # solver\n        solver_resp = generate_strict(mw_local, build_solver_prompt(q), max_tokens=15)\n        pred_yesno = parse_yesno(solver_resp)\n\n        correct = None\n        if pred_yesno is not None and inst.label01 is not None:\n            correct = int(pred_yesno == expected_yesno_from_label01(inst.label01))\n\n        # contamination (3/5/7)\n        contam_resp = generate_strict(mw_local, build_contamination_prompt(q), max_tokens=5)\n        contam = parse_357(contam_resp)\n\n        recs.append({\n            \"model\": model_name,\n            \"group\": g,\n            \"task\": tk,\n            \"solver_pred\": pred_yesno,\n            \"solver_correct\": correct,\n            \"contam\": contam,         # \"3\"/\"5\"/\"7\"/None\n            \"solver_raw\": solver_resp,\n            \"contam_raw\": contam_resp,\n        })\n    return recs\n\n# =========================\n# 5) Run all models\n# =========================\ndef run_all_models_with_sgu_mixed(model_specs, n_each=50, seed=123, sgu_sampling=\"mix_pool\", use_4bit=False):\n    # sample once (shared across models)\n    group_samples = {}\n    group_task_counts = {}\n    for g in fixed_groups.keys():\n        samples, counts = sample_group_instances(\n            g, n_each=n_each, seed=seed + (hash(g) % 100000), sgu_sampling=sgu_sampling\n        )\n        group_samples[g] = samples\n        group_task_counts[g] = counts\n\n    print(\"\\n===== SAMPLING SUMMARY =====\")\n    for g in fixed_groups.keys():\n        print(f\"\\n[{g}] sampled={len(group_samples[g])}\")\n        if g == \"sgu\":\n            print(\"SGU subtask counts:\")\n            for k, v in group_task_counts[g].most_common():\n                print(f\"  - {k}: {v}\")\n\n    all_records = []\n    for spec in model_specs:\n        print(\"\\n\" + \"#\" * 90)\n        print(f\"LOADING: {spec['name']} | {spec['model_id']} | 4bit={use_4bit} | SGU_sampling={sgu_sampling}\")\n        print(\"#\" * 90)\n\n        mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n        for g, samples in group_samples.items():\n            if not samples:\n                continue\n            all_records.extend(eval_samples_for_model(mw_local, samples, spec[\"name\"]))\n\n        del mw_local\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    df = pd.DataFrame(all_records)\n\n    task_freq = (\n        df.groupby([\"group\", \"task\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"group\", \"count\"], ascending=[True, False])\n    )\n    print(\"\\n===== ACTUAL TASK FREQUENCY IN EVAL (all models concatenated; per-model same samples) =====\")\n    with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 140):\n        print(task_freq.to_string(index=False))\n\n    return df\n\n# =========================\n# 6) Aggregations (conditional accuracies + distributions)\n# =========================\ndef analyze(df):\n    df_valid = df[df[\"solver_correct\"].notna() & df[\"contam\"].isin([\"3\", \"5\", \"7\"])].copy()\n    df_valid[\"solver_correct\"] = df_valid[\"solver_correct\"].astype(int)\n\n    print(f\"\\nTOTAL records: {len(df)} | VALID records: {len(df_valid)} | valid_rate: {len(df_valid)/max(1,len(df)):.4f}\")\n\n    # coverage by model/group/task\n    cover = df.groupby([\"model\",\"group\",\"task\"]).size().rename(\"total\").reset_index()\n    used  = df_valid.groupby([\"model\",\"group\",\"task\"]).size().rename(\"usable\").reset_index()\n    coverage = cover.merge(used, on=[\"model\",\"group\",\"task\"], how=\"left\").fillna({\"usable\":0})\n    coverage[\"usable\"] = coverage[\"usable\"].astype(int)\n    coverage[\"usable_rate\"] = coverage[\"usable\"] / coverage[\"total\"]\n\n    print(\"\\n===== COVERAGE by model/group/task (valid solver + valid contam {3,5,7}) =====\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 20, \"display.width\", 220):\n        print(coverage.sort_values([\"model\",\"group\",\"task\"]).to_string(index=False))\n\n    # overall conditional accuracy\n    overall = (\n        df_valid.groupby([\"model\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"contam\"])\n    )\n    print(\"\\n===== OVERALL conditional accuracy by self-report (3/5/7) =====\")\n    with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 120):\n        print(overall.to_string(index=False))\n\n    # by-group conditional accuracy\n    by_group = (\n        df_valid.groupby([\"model\",\"group\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"contam\"])\n    )\n    print(\"\\n===== BY-GROUP conditional accuracy by self-report (3/5/7) =====\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 12, \"display.width\", 160):\n        print(by_group.to_string(index=False))\n\n    # by-task conditional accuracy\n    by_task = (\n        df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n    )\n    print(\"\\n===== BY-TASK conditional accuracy by self-report (3/5/7) =====\")\n    with pd.option_context(\"display.max_rows\", 20000, \"display.max_columns\", 14, \"display.width\", 220):\n        print(by_task.to_string(index=False))\n\n    # distributions (counts) within VALID\n    dist_by_group = (\n        df_valid.groupby([\"model\",\"group\",\"contam\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"contam\"])\n    )\n    print(\"\\n===== BY-GROUP self-report distribution counts (VALID samples) =====\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 10, \"display.width\", 140):\n        print(dist_by_group.to_string(index=False))\n\n    dist_by_task = (\n        df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n    )\n    print(\"\\n===== BY-TASK self-report distribution counts (VALID samples) =====\")\n    with pd.option_context(\"display.max_rows\", 20000, \"display.max_columns\", 12, \"display.width\", 180):\n        print(dist_by_task.to_string(index=False))\n\n    # sampled counts (from one model) to show SGU mix distribution\n    first_model = df[\"model\"].iloc[0]\n    df_one = df[df[\"model\"] == first_model].copy()\n    sampled_task_counts = (\n        df_one.groupby([\"group\",\"task\"]).size()\n        .rename(\"sampled_n\")\n        .reset_index()\n        .sort_values([\"group\",\"sampled_n\"], ascending=[True, False])\n    )\n    print(\"\\n===== SAMPLED COUNTS per group/task (from one model; represents shared sampled set) =====\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 10, \"display.width\", 140):\n        print(sampled_task_counts.to_string(index=False))\n\n    # wide summary\n    overall_wide = overall.pivot(index=\"model\", columns=\"contam\", values=\"acc\").reset_index()\n    overall_wide = overall_wide.rename(columns={\"3\":\"acc_if_3\", \"5\":\"acc_if_5\", \"7\":\"acc_if_7\"})\n    print(\"\\n===== OVERALL (wide) =====\")\n    with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 20, \"display.width\", 140):\n        print(overall_wide.to_string(index=False))\n\n    return df_valid, coverage, overall, by_group, by_task, dist_by_group, dist_by_task, sampled_task_counts, overall_wide\n\n# =========================\n# 7) RUN (edit params here)\n# =========================\nraw_df_mixed_357 = run_all_models_with_sgu_mixed(\n    MODEL_SPECS,\n    n_each=300,\n    seed=123,\n    sgu_sampling=\"mix_pool\",   # or \"uniform_task\"\n    use_4bit=USE_4BIT\n)\n\ndf_valid_357, coverage_357, overall_357, by_group_357, by_task_357, dist_by_group_357, dist_by_task_357, sampled_task_counts_357, overall_wide_357 = analyze(raw_df_mixed_357)\n",
      "metadata": {
        "id": "DPIwiD9X46ya"
      },
      "id": "DPIwiD9X46ya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# =========================\n# FULL SCRIPT (UPDATED + SAVE):\n# - contamination self-report as {3,5,7}\n# - SGU / P-time / NP-time ALL do multi-subtask mixed sampling (mix_pool or uniform_task)\n# - sampling summary prints subtask breakdown for SGU, P-time, NP-time (sampled vs total)\n# - runs all models, collects joint records, prints all aggregations\n# - SAVES outputs to CSV (raw + valid + summary tables) to your chosen folder\n# =========================\n\nimport os\nimport random\nimport re\nimport gc\nimport torch\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm.auto import tqdm\nfrom transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n\n# =========================\n# 0) Stop + strict generation (first-line cut)\n# =========================\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.stop_token_ids = []\n        for s in stop_strings:\n            ids = tokenizer.encode(s, add_special_tokens=False)\n            if ids:\n                self.stop_token_ids.append(ids[-1])\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0:\n            return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(mw, prompt: str, max_tokens: int = 10):\n    tok, model = mw.tokenizer, mw.model\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n    gen_config = GenerationConfig(\n        do_sample=False,\n        temperature=0.0,\n        max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            generation_config=gen_config,\n            stopping_criteria=StoppingCriteriaList([stopper]),\n        )\n\n    full = tok.decode(out[0], skip_special_tokens=True)\n    resp = full[len(prompt):] if full.startswith(prompt) else full\n    return resp.split(\"\\n\")[0].strip()\n\n# =========================\n# 1) Prompts + parsers\n# =========================\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef parse_yesno(s: str):\n    if not s:\n        return None\n    t = s.strip().upper()\n    if \"YES\" in t:\n        return \"YES\"\n    if \"NO\" in t:\n        return \"NO\"\n    return None\n\ndef expected_yesno_from_label01(label01: int):\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\n# ---- contamination: 3/5/7 ----\ndef build_contamination_prompt(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit from {3, 5, 7}:\\n\"\n        \"3 = SEEN the same question before\\n\"\n        \"5 = Seen very similar questions before but not the same question\\n\"\n        \"7 = Not seen similar question before\\n\"\n        \"Ignore any YES/NO question in the content even if it asks.\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (3, 5, or 7):\"\n\ndef parse_357(s: str):\n    if not s:\n        return None\n    m = re.search(r\"\\b([357])\\b\", s.strip())\n    return m.group(1) if m else None\n\n# =========================\n# 2) Groups + subtasks\n# =========================\nSGU_TASKS = [\n    \"sgu_collatz_aligned\", \"sgu_collatz_aligned__sgu\",\n    \"sgu_index_empty_language\", \"sgu_index_empty_language__sgu\",\n    \"sgu_index_total_halt\", \"sgu_index_total_halt__sgu\",\n    \"sgu_semigroup_wp_amp\", \"sgu_semigroup_wp_amp__sgu\",\n    \"tm_generic_halt\", \"tm_hard_halt\",\n]\n\nfixed_groups = {\n    \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n    \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n    \"sgu\": SGU_TASKS,\n}\n\ndef pick_first_existing_dataset(task_list):\n    for k in task_list:\n        if k in datasets:\n            return k\n    return None\n\n# ✅ These groups do multi-subtask mixed sampling\nMIX_GROUPS = {\"sgu\", \"np_time\", \"p_time\"}\n\n# =========================\n# 3) Sampling (mixed for SGU/P/NP)\n# =========================\ndef sample_group_instances(group_name, n_each, seed=0, mix_sampling=\"mix_pool\"):\n    rng = random.Random(seed)\n\n    # ---- mixed groups: sgu / p_time / np_time ----\n    if group_name in MIX_GROUPS:\n        existing = [k for k in fixed_groups[group_name] if k in datasets]\n        if not existing:\n            return [], Counter()\n\n        if mix_sampling == \"mix_pool\":\n            pool = []\n            for tk in existing:\n                for inst in datasets[tk].instances:\n                    pool.append((tk, inst))\n            rng.shuffle(pool)\n            pool = pool[:min(n_each, len(pool))]\n            samples = [{\"group\": group_name, \"task\": tk, \"inst\": inst} for (tk, inst) in pool]\n            return samples, Counter([tk for tk, _ in pool])\n\n        elif mix_sampling == \"uniform_task\":\n            per_task_idxs = {}\n            for tk in existing:\n                idxs = list(range(len(datasets[tk].instances)))\n                rng.shuffle(idxs)\n                per_task_idxs[tk] = idxs\n\n            samples = []\n            counts = Counter()\n            for _ in range(n_each * 50):\n                if len(samples) >= n_each:\n                    break\n                tk = rng.choice(existing)\n                if not per_task_idxs[tk]:\n                    continue\n                i = per_task_idxs[tk].pop()\n                samples.append({\"group\": group_name, \"task\": tk, \"inst\": datasets[tk].instances[i]})\n                counts[tk] += 1\n            return samples, counts\n\n        else:\n            raise ValueError(\"mix_sampling must be 'mix_pool' or 'uniform_task'\")\n\n    # ---- other groups: pick first existing task ----\n    task_key = pick_first_existing_dataset(fixed_groups[group_name])\n    if task_key is None:\n        return [], Counter()\n    ds = datasets[task_key]\n    n = min(n_each, len(ds.instances))\n    idxs = list(range(len(ds.instances)))\n    rng.shuffle(idxs)\n    idxs = idxs[:n]\n    samples = [{\"group\": group_name, \"task\": task_key, \"inst\": ds.instances[i]} for i in idxs]\n    return samples, Counter([task_key] * len(samples))\n\ndef _print_subtask_breakdown(group_name, sampled_counts: Counter):\n    existing = [k for k in fixed_groups[group_name] if k in datasets]\n    if not existing:\n        print(f\"\\n[{group_name}] MISSING (no tasks found)\")\n        return\n\n    rows = []\n    for tk in existing:\n        rows.append({\n            \"task\": tk,\n            \"sampled\": int(sampled_counts.get(tk, 0)),\n            \"total_available\": len(datasets[tk].instances),\n        })\n    dfb = pd.DataFrame(rows).sort_values([\"sampled\", \"total_available\"], ascending=[False, False])\n\n    print(f\"\\n[{group_name}] subtask breakdown (sampled vs total)\")\n    with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 160):\n        print(dfb.to_string(index=False))\n\n# =========================\n# 4) Eval one model on a sample list\n# =========================\ndef eval_samples_for_model(mw_local, samples, model_name):\n    recs = []\n    if not samples:\n        return recs\n\n    groups = {s[\"group\"] for s in samples}\n    group_tag = list(groups)[0] if len(groups) == 1 else \"mixed\"\n\n    for s in tqdm(samples, desc=f\"{model_name}|{group_tag}\", leave=False):\n        g = s[\"group\"]\n        tk = s[\"task\"]\n        inst = s[\"inst\"]\n        q = inst.prompt\n\n        solver_resp = generate_strict(mw_local, build_solver_prompt(q), max_tokens=15)\n        pred_yesno = parse_yesno(solver_resp)\n\n        correct = None\n        if pred_yesno is not None and inst.label01 is not None:\n            correct = int(pred_yesno == expected_yesno_from_label01(inst.label01))\n\n        contam_resp = generate_strict(mw_local, build_contamination_prompt(q), max_tokens=5)\n        contam = parse_357(contam_resp)\n\n        recs.append({\n            \"model\": model_name,\n            \"group\": g,\n            \"task\": tk,\n            \"solver_pred\": pred_yesno,\n            \"solver_correct\": correct,\n            \"contam\": contam,         # \"3\"/\"5\"/\"7\"/None\n            \"solver_raw\": solver_resp,\n            \"contam_raw\": contam_resp,\n        })\n    return recs\n\n# =========================\n# 5) Run all models + SAVE raw\n# =========================\ndef run_all_models_mixed(model_specs, n_each=50, seed=123, mix_sampling=\"mix_pool\", use_4bit=False):\n    group_samples = {}\n    group_task_counts = {}\n\n    for g in fixed_groups.keys():\n        samples, counts = sample_group_instances(\n            g, n_each=n_each, seed=seed + (hash(g) % 100000), mix_sampling=mix_sampling\n        )\n        group_samples[g] = samples\n        group_task_counts[g] = counts\n\n    print(\"\\n===== SAMPLING SUMMARY =====\")\n    for g in fixed_groups.keys():\n        print(f\"\\n[{g}] sampled={len(group_samples[g])}\")\n        if g in MIX_GROUPS:\n            _print_subtask_breakdown(g, group_task_counts[g])\n\n    all_records = []\n    for spec in model_specs:\n        print(\"\\n\" + \"#\" * 90)\n        print(f\"LOADING: {spec['name']} | {spec['model_id']} | 4bit={use_4bit} | mix_sampling={mix_sampling}\")\n        print(\"#\" * 90)\n\n        mw_local = HFModelWrapper.load(spec[\"model_id\"], adapter_path=spec.get(\"adapter\"), use_4bit=use_4bit)\n\n        for g, samples in group_samples.items():\n            if not samples:\n                continue\n            all_records.extend(eval_samples_for_model(mw_local, samples, spec[\"name\"]))\n\n        del mw_local\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    df = pd.DataFrame(all_records)\n\n    task_freq = (\n        df.groupby([\"group\", \"task\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"group\", \"count\"], ascending=[True, False])\n    )\n    print(\"\\n===== ACTUAL TASK FREQUENCY IN EVAL (all models concatenated; per-model same samples) =====\")\n    with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 140):\n        print(task_freq.to_string(index=False))\n\n    return df\n\n# =========================\n# 6) Aggregations + SAVE tables\n# =========================\ndef analyze_and_save(df, out_dir, prefix):\n    os.makedirs(out_dir, exist_ok=True)\n\n    df_valid = df[df[\"solver_correct\"].notna() & df[\"contam\"].isin([\"3\", \"5\", \"7\"])].copy()\n    df_valid[\"solver_correct\"] = df_valid[\"solver_correct\"].astype(int)\n\n    print(f\"\\nTOTAL records: {len(df)} | VALID records: {len(df_valid)} | valid_rate: {len(df_valid)/max(1,len(df)):.4f}\")\n\n    cover = df.groupby([\"model\",\"group\",\"task\"]).size().rename(\"total\").reset_index()\n    used  = df_valid.groupby([\"model\",\"group\",\"task\"]).size().rename(\"usable\").reset_index()\n    coverage = cover.merge(used, on=[\"model\",\"group\",\"task\"], how=\"left\").fillna({\"usable\":0})\n    coverage[\"usable\"] = coverage[\"usable\"].astype(int)\n    coverage[\"usable_rate\"] = coverage[\"usable\"] / coverage[\"total\"]\n\n    overall = (\n        df_valid.groupby([\"model\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"contam\"])\n    )\n\n    by_group = (\n        df_valid.groupby([\"model\",\"group\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"contam\"])\n    )\n\n    by_task = (\n        df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"])[\"solver_correct\"]\n        .agg(acc=\"mean\", n=\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n    )\n\n    dist_by_group = (\n        df_valid.groupby([\"model\",\"group\",\"contam\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"contam\"])\n    )\n\n    dist_by_task = (\n        df_valid.groupby([\"model\",\"group\",\"task\",\"contam\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"model\",\"group\",\"task\",\"contam\"])\n    )\n\n    task_freq = (\n        df.groupby([\"group\", \"task\"]).size()\n        .rename(\"count\")\n        .reset_index()\n        .sort_values([\"group\", \"count\"], ascending=[True, False])\n    )\n\n    overall_wide = overall.pivot(index=\"model\", columns=\"contam\", values=\"acc\").reset_index()\n    overall_wide = overall_wide.rename(columns={\"3\":\"acc_if_3\", \"5\":\"acc_if_5\", \"7\":\"acc_if_7\"})\n\n    # ---------- print ----------\n    print(\"\\n===== COVERAGE by model/group/task =====\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 20, \"display.width\", 220):\n        print(coverage.sort_values([\"model\",\"group\",\"task\"]).to_string(index=False))\n\n    print(\"\\n===== OVERALL conditional accuracy (3/5/7) =====\")\n    with pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 10, \"display.width\", 120):\n        print(overall.to_string(index=False))\n\n    print(\"\\n===== OVERALL (wide) =====\")\n    with pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 20, \"display.width\", 140):\n        print(overall_wide.to_string(index=False))\n\n    # ---------- save ----------\n    paths = {}\n    def _save(df_, name):\n        path = os.path.join(out_dir, f\"{prefix}_{name}.csv\")\n        df_.to_csv(path, index=False)\n        paths[name] = path\n\n    _save(df, \"raw_records\")\n    _save(df_valid, \"valid_records\")\n    _save(task_freq, \"task_frequency\")\n    _save(coverage, \"coverage\")\n    _save(overall, \"overall_long\")\n    _save(overall_wide, \"overall_wide\")\n    _save(by_group, \"by_group_acc\")\n    _save(by_task, \"by_task_acc\")\n    _save(dist_by_group, \"by_group_dist\")\n    _save(dist_by_task, \"by_task_dist\")\n\n    print(\"\\n===== SAVED FILES =====\")\n    for k, v in paths.items():\n        print(f\"{k}: {v}\")\n\n    return df_valid, coverage, overall, by_group, by_task, dist_by_group, dist_by_task, task_freq, overall_wide, paths\n\n# =========================\n# 7) RUN (edit params here)\n# =========================\n\n\nimport os\nimport pandas as pd\n\n# ----------------------------\n# ----------------------------\ndef save_raw_and_tables(\n    df_raw: pd.DataFrame,\n    out_dir: str,\n    prefix: str,\n    extra_tables: dict | None = None\n):\n    os.makedirs(out_dir, exist_ok=True)\n    paths = {}\n\n    raw_path = os.path.join(out_dir, f\"{prefix}_RAW_RECORDS.csv\")\n    df_raw.to_csv(raw_path, index=False)\n    paths[\"RAW_RECORDS\"] = raw_path\n\n    cols_keep = [c for c in [\n        \"model\",\"group\",\"task\",\n        \"solver_pred\",\"solver_correct\",\"solver_raw\",\n        \"contam\",\"contam_raw\"\n    ] if c in df_raw.columns]\n    raw_min_path = os.path.join(out_dir, f\"{prefix}_RAW_MIN.csv\")\n    df_raw[cols_keep].to_csv(raw_min_path, index=False)\n    paths[\"RAW_MIN\"] = raw_min_path\n\n    if extra_tables:\n        for name, df_tab in extra_tables.items():\n            if df_tab is None:\n                continue\n            p = os.path.join(out_dir, f\"{prefix}_{name}.csv\")\n            df_tab.to_csv(p, index=False)\n            paths[name] = p\n\n    print(\"\\n===== SAVED FILES =====\")\n    for k, v in paths.items():\n        print(f\"{k}: {v}\")\n    return paths\n\n\n# ----------------------------\n# ----------------------------\nOUT_DIR = os.environ.get(\"EVAL_OUT_DIR\", \"/content/drive/MyDrive/complexity7/eval_outputs\")\nPREFIX = \"mixed357_n300_seed123_mixpool\"\n\nraw_df_mixed_357 = run_all_models_mixed(\n    MODEL_SPECS,\n    n_each=300,\n    seed=123,\n    mix_sampling=\"mix_pool\",   # or \"uniform_task\"\n    use_4bit=USE_4BIT\n)\n\ndf_valid_357, coverage_357, overall_357, by_group_357, by_task_357, dist_by_group_357, dist_by_task_357, sampled_task_counts_357, overall_wide_357 = analyze(raw_df_mixed_357)\n\ntask_freq_357 = (\n    raw_df_mixed_357.groupby([\"group\",\"task\"]).size()\n    .rename(\"count\").reset_index()\n    .sort_values([\"group\",\"count\"], ascending=[True, False])\n)\n\npaths = save_raw_and_tables(\n    df_raw=raw_df_mixed_357,\n    out_dir=OUT_DIR,\n    prefix=PREFIX,\n    extra_tables={\n        \"VALID_RECORDS\": df_valid_357,\n        \"task_frequency\": task_freq_357,\n        \"coverage\": coverage_357,\n        \"overall_long\": overall_357,\n        \"overall_wide\": overall_wide_357,\n        \"by_group_acc\": by_group_357,\n        \"by_task_acc\": by_task_357,\n        \"by_group_dist\": dist_by_group_357,\n        \"by_task_dist\": dist_by_task_357,\n        \"sampled_task_counts\": sampled_task_counts_357,\n    }\n)\n\n",
      "metadata": {
        "id": "UU99NW49MwiY"
      },
      "id": "UU99NW49MwiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\ndf = raw_df_mixed_357.copy()\n\ndef agg_acc(g: pd.DataFrame) -> pd.Series:\n    total = len(g)\n    fmt_n = int(g[\"solver_correct\"].notna().sum())\n    acc_on_fmt = float(g.loc[g[\"solver_correct\"].notna(), \"solver_correct\"].mean()) if fmt_n > 0 else np.nan\n    acc_overall = float(g[\"solver_correct\"].fillna(0).mean()) if total > 0 else np.nan\n    fmt_rate = fmt_n / total if total > 0 else 0.0\n    return pd.Series({\n        \"n_total\": total,\n        \"n_fmt\": fmt_n,\n        \"fmt_rate\": fmt_rate,\n        \"acc_on_fmt\": acc_on_fmt,\n        \"acc_overall\": acc_overall,\n    })\n\ngroup_acc = (\n    df.groupby([\"group\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"group\"])\n)\n\nprint(\"\\n===== ACCURACY BY GROUP =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 160):\n    print(group_acc.to_string(index=False))\n\ntask_acc = (\n    df.groupby([\"group\",\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"group\",\"acc_overall\"], ascending=[True, False])\n)\n\nprint(\"\\n===== ACCURACY BY GROUP + TASK =====\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 50, \"display.width\", 200):\n    print(task_acc.to_string(index=False))\n\nmodel_task_acc = (\n    df.groupby([\"model\",\"group\",\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"model\",\"group\",\"acc_overall\"], ascending=[True, True, False])\n)\n\nprint(\"\\n===== (OPTIONAL) ACCURACY BY MODEL + GROUP + TASK =====\")\nwith pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 50, \"display.width\", 220):\n    print(model_task_acc.to_string(index=False))\ntask_only_acc = (\n    df.groupby([\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"acc_overall\"], ascending=False)\n)\n\nprint(\"\\n===== ACCURACY BY TASK (overall, across all groups/models) =====\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 50, \"display.width\", 200):\n    print(task_only_acc.to_string(index=False))\n",
      "metadata": {
        "id": "O-CuAbzEa83B"
      },
      "id": "O-CuAbzEa83B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "0k5exxX2csTF"
      },
      "id": "0k5exxX2csTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\ndf = raw_df_mixed_357.copy()\n\ndef agg_acc(g: pd.DataFrame) -> pd.Series:\n    total = len(g)\n    fmt_n = int(g[\"solver_correct\"].notna().sum())\n    acc_on_fmt = float(g.loc[g[\"solver_correct\"].notna(), \"solver_correct\"].mean()) if fmt_n > 0 else np.nan\n    acc_overall = float(g[\"solver_correct\"].fillna(0).mean()) if total > 0 else np.nan\n    fmt_rate = fmt_n / total if total > 0 else 0.0\n    return pd.Series({\n        \"n_total\": total,\n        \"n_fmt\": fmt_n,\n        \"fmt_rate\": fmt_rate,\n        \"acc_on_fmt\": acc_on_fmt,\n        \"acc_overall\": acc_overall,\n    })\n\ngroup_acc = (\n    df.groupby([\"group\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"group\"])\n)\n\nprint(\"\\n===== ACCURACY BY GROUP =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 160):\n    print(group_acc.to_string(index=False))\n\ntask_acc = (\n    df.groupby([\"group\",\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"group\",\"acc_overall\"], ascending=[True, False])\n)\n\nprint(\"\\n===== ACCURACY BY GROUP + TASK =====\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 50, \"display.width\", 200):\n    print(task_acc.to_string(index=False))\n\nmodel_task_acc = (\n    df.groupby([\"model\",\"group\",\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"model\",\"group\",\"acc_overall\"], ascending=[True, True, False])\n)\n\nprint(\"\\n===== (OPTIONAL) ACCURACY BY MODEL + GROUP + TASK =====\")\nwith pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 50, \"display.width\", 220):\n    print(model_task_acc.to_string(index=False))\ntask_only_acc = (\n    df.groupby([\"task\"])\n      .apply(agg_acc)\n      .reset_index()\n      .sort_values([\"acc_overall\"], ascending=False)\n)\n\nprint(\"\\n===== ACCURACY BY TASK (overall, across all groups/models) =====\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 50, \"display.width\", 200):\n    print(task_only_acc.to_string(index=False))\n",
      "metadata": {
        "id": "pNxIwx7Ucsbm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pNxIwx7Ucsbm"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\ndf = raw_df_mixed_357.copy()\n\nd = df[df[\"solver_correct\"].notna()].copy()\nd[\"y_pred\"] = d[\"solver_pred\"].astype(str).str.upper().map({\"YES\": 1, \"NO\": 0}).astype(int)\nd[\"correct\"] = d[\"solver_correct\"].astype(int)\nd[\"y_true\"] = np.where(d[\"correct\"] == 1, d[\"y_pred\"], 1 - d[\"y_pred\"])\n\ndef metrics(g):\n    y = g[\"y_true\"].to_numpy()\n    p = g[\"y_pred\"].to_numpy()\n    pos = y.mean()\n    tpr = ((p==1) & (y==1)).sum() / max(1, (y==1).sum())\n    tnr = ((p==0) & (y==0)).sum() / max(1, (y==0).sum())\n    return pd.Series({\"n\": len(g), \"pos_rate\": pos, \"balanced_acc\": 0.5*(tpr+tnr)})\n\nout = (d.groupby(\"task\").apply(metrics).reset_index()\n       .sort_values(\"balanced_acc\", ascending=False))\n\nprint(out.to_string(index=False))\nimport numpy as np\nimport pandas as pd\n\ndf = raw_df_mixed_357.copy()\n\n\n",
      "metadata": {
        "id": "OKMyuUPZdWjj"
      },
      "id": "OKMyuUPZdWjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "d = df[df[\"solver_correct\"].notna()].copy()\nd[\"y_pred\"] = d[\"solver_pred\"].astype(str).str.upper().map({\"YES\": 1, \"NO\": 0})\nd = d[d[\"y_pred\"].notna()].copy()\nd[\"y_pred\"] = d[\"y_pred\"].astype(int)\nd[\"correct\"] = d[\"solver_correct\"].astype(int)\nd[\"y_true\"] = np.where(d[\"correct\"] == 1, d[\"y_pred\"], 1 - d[\"y_pred\"]).astype(int)\n\n# ---------- 2) balanced accuracy ----------\ndef balanced_acc(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=int)\n    y_pred = np.asarray(y_pred, dtype=int)\n    pos = (y_true == 1)\n    neg = (y_true == 0)\n    tpr = ((y_pred == 1) & pos).sum() / max(1, pos.sum())\n    tnr = ((y_pred == 0) & neg).sum() / max(1, neg.sum())\n    return 0.5 * (tpr + tnr)\n\ndef task_stat(df_task):\n    return balanced_acc(df_task[\"y_true\"].values, df_task[\"y_pred\"].values)\n\ndef bootstrap_ci_task(d, task, B=2000, seed=0):\n    rng = np.random.default_rng(seed)\n    sub = d[d[\"task\"] == task].copy()\n    if len(sub) == 0:\n        return np.nan, (np.nan, np.nan), 0\n\n    by_model = {m: g for m, g in sub.groupby(\"model\")}\n    point = task_stat(sub)\n\n    boots = []\n    for _ in range(B):\n        parts = []\n        for m, g in by_model.items():\n            idx = rng.integers(0, len(g), size=len(g))\n            parts.append(g.iloc[idx])\n        bb = pd.concat(parts, ignore_index=True)\n        boots.append(task_stat(bb))\n    boots = np.array(boots)\n    lo, hi = np.quantile(boots, [0.025, 0.975])\n    return point, (lo, hi), len(sub)\n\ndef perm_test_diff(d, task_a, task_b, B=20000, seed=0):\n    rng = np.random.default_rng(seed)\n    a = d[d[\"task\"] == task_a].copy()\n    b = d[d[\"task\"] == task_b].copy()\n    if len(a) == 0 or len(b) == 0:\n        return np.nan, np.nan\n\n    obs = task_stat(a) - task_stat(b)\n\n    models = sorted(set(a[\"model\"]).intersection(set(b[\"model\"])))\n    if not models:\n        pool = pd.concat([a.assign(_t=0), b.assign(_t=1)], ignore_index=True)\n        n_a = len(a)\n        diffs = []\n        for _ in range(B):\n            perm = rng.permutation(len(pool))\n            aa = pool.iloc[perm[:n_a]]\n            bb = pool.iloc[perm[n_a:]]\n            diffs.append(balanced_acc(aa[\"y_true\"], aa[\"y_pred\"]) - balanced_acc(bb[\"y_true\"], bb[\"y_pred\"]))\n        diffs = np.array(diffs)\n        p = (np.abs(diffs) >= abs(obs)).mean()\n        return obs, p\n\n    diffs = []\n    for _ in range(B):\n        parts_a = []\n        parts_b = []\n        for m in models:\n            am = a[a[\"model\"] == m]\n            bm = b[b[\"model\"] == m]\n            pool = pd.concat([am, bm], ignore_index=True)\n            perm = rng.permutation(len(pool))\n            aa = pool.iloc[perm[:len(am)]]\n            bb = pool.iloc[perm[len(am):]]\n            parts_a.append(aa)\n            parts_b.append(bb)\n        aa = pd.concat(parts_a, ignore_index=True)\n        bb = pd.concat(parts_b, ignore_index=True)\n        diffs.append(task_stat(aa) - task_stat(bb))\n    diffs = np.array(diffs)\n    p = (np.abs(diffs) >= abs(obs)).mean()\n    return obs, p\n\ndef holm_bonferroni(pvals):\n    pvals = np.array(pvals, dtype=float)\n    m = len(pvals)\n    order = np.argsort(pvals)\n    adj = np.empty(m, dtype=float)\n    for i, idx in enumerate(order):\n        adj[idx] = min(1.0, (m - i) * pvals[idx])\n    for i in range(1, m):\n        adj[order[i]] = max(adj[order[i]], adj[order[i-1]])\n    return adj\n\ntasks = sorted(d[\"task\"].unique().tolist())\nrows = []\nfor t in tasks:\n    point, (lo, hi), n = bootstrap_ci_task(d, t, B=3000, seed=123)\n    rows.append({\"task\": t, \"n\": n, \"balanced_acc\": point, \"ci95_lo\": lo, \"ci95_hi\": hi})\nci_table = pd.DataFrame(rows).sort_values(\"balanced_acc\", ascending=False)\n\nprint(\"\\n=== Balanced Accuracy + 95% Bootstrap CI (stratified by model) ===\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 20, \"display.width\", 160):\n    print(ci_table.to_string(index=False))\n\nbaseline1 = \"gsm8k_binary\"\nbaseline2 = \"tm_generic_halt\"\n\ncomparisons = []\nfor base in [baseline1, baseline2]:\n    if base not in tasks:\n        continue\n    for t in tasks:\n        if t == base:\n            continue\n        obs, p = perm_test_diff(d, t, base, B=20000, seed=123)\n        comparisons.append({\"contrast\": f\"{t} - {base}\", \"diff_balacc\": obs, \"p_perm\": p})\n\ncomp_df = pd.DataFrame(comparisons)\nif len(comp_df) > 0:\n    comp_df[\"p_holm\"] = holm_bonferroni(comp_df[\"p_perm\"].values)\n    comp_df = comp_df.sort_values([\"p_perm\", \"contrast\"])\n\n    print(\"\\n=== Permutation test (stratified by model): diff in balanced_acc ===\")\n    with pd.option_context(\"display.max_rows\", 5000, \"display.max_columns\", 20, \"display.width\", 200):\n        print(comp_df.to_string(index=False))\nelse:\n    print(\"\\n[WARN] No comparisons run (baseline task missing).\")",
      "metadata": {
        "id": "CSlufGOleqd7"
      },
      "id": "CSlufGOleqd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\ndf = raw_df_mixed_357.copy()\n\nd = df[df[\"solver_correct\"].notna()].copy()\nd[\"y_pred\"] = d[\"solver_pred\"].astype(str).str.upper().map({\"YES\": 1, \"NO\": 0})\nd = d[d[\"y_pred\"].notna()].copy()\nd[\"y_pred\"] = d[\"y_pred\"].astype(int)\nd[\"correct\"] = d[\"solver_correct\"].astype(int)\nd[\"y_true\"] = np.where(d[\"correct\"] == 1, d[\"y_pred\"], 1 - d[\"y_pred\"]).astype(int)\n\ndef balanced_acc(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=int)\n    y_pred = np.asarray(y_pred, dtype=int)\n    pos = (y_true == 1)\n    neg = (y_true == 0)\n    tpr = ((y_pred == 1) & pos).sum() / max(1, pos.sum())\n    tnr = ((y_pred == 0) & neg).sum() / max(1, neg.sum())\n    return 0.5 * (tpr + tnr)\n\ndef map_group_for_table(row):\n    if row[\"task\"] == \"tm_generic_halt\":\n        return \"tm_generic_halt\"\n    if row[\"group\"] == \"sgu\":\n        return \"sgu_other\"\n    return row[\"group\"]\n\nd[\"group_for_table\"] = d.apply(map_group_for_table, axis=1)\n\ndef agg(g):\n    return pd.Series({\n        \"n\": len(g),\n        \"balanced_acc\": balanced_acc(g[\"y_true\"].values, g[\"y_pred\"].values),\n        \"acc_on_fmt\": g[\"correct\"].mean(),\n        \"pos_rate\": g[\"y_true\"].mean(),\n    })\n\nstats = (\n    d.groupby([\"model\", \"group_for_table\"])\n     .apply(agg)\n     .reset_index()\n)\n\norder_cols = [\"gsm8k\", \"aqua\", \"p_time\", \"np_time\", \"sgu_other\", \"tm_generic_halt\"]\n\nbal_wide = stats.pivot(index=\"model\", columns=\"group_for_table\", values=\"balanced_acc\")\nn_wide   = stats.pivot(index=\"model\", columns=\"group_for_table\", values=\"n\")\nacc_wide = stats.pivot(index=\"model\", columns=\"group_for_table\", values=\"acc_on_fmt\")\npos_wide = stats.pivot(index=\"model\", columns=\"group_for_table\", values=\"pos_rate\")\n\nfor w in [bal_wide, n_wide, acc_wide, pos_wide]:\n    for c in order_cols:\n        if c not in w.columns:\n            w[c] = np.nan\n    w = w[order_cols]\n\nbal_wide = bal_wide[order_cols]\nn_wide   = n_wide[order_cols]\nacc_wide = acc_wide[order_cols]\npos_wide = pos_wide[order_cols]\n\nprint(\"\\n===== MODEL x GROUP/TASK (Balanced Accuracy) =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 200):\n    print(bal_wide.to_string())\n\nprint(\"\\n===== MODEL x GROUP/TASK (N used) =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 200):\n    print(n_wide.to_string())\n\nprint(\"\\n===== (OPTIONAL) MODEL x GROUP/TASK (acc_on_fmt) =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 200):\n    print(acc_wide.to_string())\n\nprint(\"\\n===== (OPTIONAL) MODEL x GROUP/TASK (pos_rate) =====\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 50, \"display.width\", 200):\n    print(pos_wide.to_string())\n",
      "metadata": {
        "id": "yKxBJXKTgrJb"
      },
      "id": "yKxBJXKTgrJb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport math\n\ndf = raw_df_mixed_357.copy()\n\nd = df[df[\"solver_correct\"].notna()].copy()\nd[\"correct\"] = d[\"solver_correct\"].astype(int)\n\ndef map_group_for_table(row):\n    if row[\"task\"] == \"tm_generic_halt\":\n        return \"tm_generic_halt\"\n    if row[\"group\"] == \"sgu\":\n        return \"sgu_other\"\n    return row[\"group\"]\n\nd[\"group_for_table\"] = d.apply(map_group_for_table, axis=1)\n\nd[\"item_idx\"] = d.groupby([\"model\", \"group_for_table\", \"task\"]).cumcount()\nd[\"pair_id\"] = d[\"group_for_table\"].astype(str) + \"|\" + d[\"task\"].astype(str) + \"|\" + d[\"item_idx\"].astype(str)\n\ndef cochran_q(mat):\n    \"\"\"\n    mat: numpy array shape (n_items, k_models) with 0/1\n    returns (Q, p)\n    \"\"\"\n    n, k = mat.shape\n    Tj = mat.sum(axis=0)          # col sums\n    Ri = mat.sum(axis=1)          # row sums\n    T = Tj.sum()\n    denom = (k * T - (Ri**2).sum())\n    if denom == 0:\n        return np.nan, np.nan\n    Q = (k - 1) * (k * (Tj**2).sum() - T**2) / denom\n    # chi-square df = k-1\n    try:\n        from scipy.stats import chi2\n        p = chi2.sf(Q, df=k-1)\n    except Exception:\n        p = np.nan\n    return float(Q), float(p)\n\ndef binom_cdf(k, n, p=0.5):\n    s = 0.0\n    for i in range(k + 1):\n        s += math.comb(n, i) * (p**i) * ((1-p)**(n-i))\n    return s\n\ndef mcnemar_exact(a, b):\n    \"\"\"\n    a,b: 0/1 arrays of same length\n    return (b_count, c_count, p_exact)\n    b = a=1,b=0 ; c = a=0,b=1\n    \"\"\"\n    a = np.asarray(a, dtype=int)\n    b2 = np.asarray(b, dtype=int)\n    b01 = ((a == 1) & (b2 == 0)).sum()\n    c10 = ((a == 0) & (b2 == 1)).sum()\n    n = b01 + c10\n    if n == 0:\n        return int(b01), int(c10), 1.0\n    k = min(b01, c10)\n    p = 2.0 * binom_cdf(k, n, 0.5)\n    p = min(1.0, p)\n    return int(b01), int(c10), float(p)\n\ndef holm(pvals):\n    pvals = np.asarray(pvals, dtype=float)\n    m = len(pvals)\n    order = np.argsort(pvals)\n    adj = np.empty(m, dtype=float)\n    for i, idx in enumerate(order):\n        adj[idx] = min(1.0, (m - i) * pvals[idx])\n    for i in range(1, m):\n        adj[order[i]] = max(adj[order[i]], adj[order[i-1]])\n    return adj\n\ntargets = [\"gsm8k\", \"aqua\", \"p_time\", \"np_time\", \"sgu_other\", \"tm_generic_halt\"]\n\nall_overall = []\nall_pairwise = []\n\nfor gname in targets:\n    sub = d[d[\"group_for_table\"] == gname].copy()\n    if sub.empty:\n        continue\n\n    mat = sub.pivot_table(index=\"pair_id\", columns=\"model\", values=\"correct\", aggfunc=\"first\")\n\n    mat = mat.dropna(axis=0, how=\"any\")\n    if mat.shape[0] == 0 or mat.shape[1] < 2:\n        continue\n\n    models = list(mat.columns)\n    X = mat.values.astype(int)\n\n    Q, pQ = cochran_q(X)\n    all_overall.append({\n        \"group_for_table\": gname,\n        \"n_items_paired\": mat.shape[0],\n        \"n_models\": mat.shape[1],\n        \"cochran_Q\": Q,\n        \"p_cochranQ\": pQ\n    })\n\n    for i in range(len(models)):\n        for j in range(i+1, len(models)):\n            m1, m2 = models[i], models[j]\n            b01, c10, p = mcnemar_exact(mat[m1].values, mat[m2].values)\n            all_pairwise.append({\n                \"group_for_table\": gname,\n                \"model_a\": m1,\n                \"model_b\": m2,\n                \"n_items_paired\": mat.shape[0],\n                \"b(a=1,b=0)\": b01,\n                \"c(a=0,b=1)\": c10,\n                \"p_mcnemar\": p\n            })\n\noverall_df = pd.DataFrame(all_overall).sort_values(\"p_cochranQ\", na_position=\"last\")\npair_df = pd.DataFrame(all_pairwise)\n\npair_df[\"p_holm_within_group\"] = np.nan\nfor gname in pair_df[\"group_for_table\"].unique():\n    mask = pair_df[\"group_for_table\"] == gname\n    pair_df.loc[mask, \"p_holm_within_group\"] = holm(pair_df.loc[mask, \"p_mcnemar\"].values)\n\npair_df = pair_df.sort_values([\"group_for_table\", \"p_holm_within_group\", \"p_mcnemar\"])\n\nprint(\"\\n=== OVERALL: Cochran's Q (is there ANY model difference?) ===\")\nwith pd.option_context(\"display.max_rows\", 200, \"display.max_columns\", 20, \"display.width\", 140):\n    print(overall_df.to_string(index=False))\n\nprint(\"\\n=== PAIRWISE: McNemar exact (paired model differences), Holm-corrected within group ===\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 20, \"display.width\", 200):\n    print(pair_df.to_string(index=False))\nimport numpy as np\nimport pandas as pd\nimport math\n\ndf = raw_df_mixed_357.copy()\n\nd = df[df[\"solver_correct\"].notna()].copy()\nd[\"correct\"] = d[\"solver_correct\"].astype(int)\n\nd[\"item_idx\"] = d.groupby([\"model\", \"task\"]).cumcount()\nd[\"pair_id\"] = d[\"task\"].astype(str) + \"|\" + d[\"item_idx\"].astype(str)\n\n# --- 3) Cochran’s Q ---\ndef cochran_q(mat):\n    n, k = mat.shape\n    Tj = mat.sum(axis=0)          # col sums\n    Ri = mat.sum(axis=1)          # row sums\n    T = Tj.sum()\n    denom = (k * T - (Ri**2).sum())\n    if denom == 0:\n        return np.nan, np.nan\n    Q = (k - 1) * (k * (Tj**2).sum() - T**2) / denom\n    try:\n        from scipy.stats import chi2\n        p = chi2.sf(Q, df=k-1)\n    except Exception:\n        p = np.nan\n    return float(Q), float(p)\n\n# --- 4) McNemar exact + Holm ---\ndef binom_cdf(k, n, p=0.5):\n    s = 0.0\n    for i in range(k + 1):\n        s += math.comb(n, i) * (p**i) * ((1-p)**(n-i))\n    return s\n\ndef mcnemar_exact(a, b):\n    a = np.asarray(a, dtype=int)\n    b2 = np.asarray(b, dtype=int)\n    b01 = ((a == 1) & (b2 == 0)).sum()  # a correct, b wrong\n    c10 = ((a == 0) & (b2 == 1)).sum()  # a wrong, b correct\n    n = b01 + c10\n    if n == 0:\n        return int(b01), int(c10), 1.0\n    k = min(b01, c10)\n    p = 2.0 * binom_cdf(k, n, 0.5)\n    return int(b01), int(c10), float(min(1.0, p))\n\ndef holm(pvals):\n    pvals = np.asarray(pvals, dtype=float)\n    m = len(pvals)\n    order = np.argsort(pvals)\n    adj = np.empty(m, dtype=float)\n    for i, idx in enumerate(order):\n        adj[idx] = min(1.0, (m - i) * pvals[idx])\n    for i in range(1, m):\n        adj[order[i]] = max(adj[order[i]], adj[order[i-1]])\n    return adj\n\nmat = d.pivot_table(index=\"pair_id\", columns=\"model\", values=\"correct\", aggfunc=\"first\")\nmat = mat.dropna(axis=0, how=\"any\")\nX = mat.values.astype(int)\n\nprint(\"\\n=== OVERALL (ALL TASKS): PAIRED ITEMS ===\")\nprint(\"n_items_paired =\", X.shape[0], \" | n_models =\", X.shape[1])\n\nQ, pQ = cochran_q(X)\nprint(\"\\n=== OVERALL: Cochran's Q ===\")\nprint(\"Q =\", Q, \"p =\", pQ)\n\nmodels = list(mat.columns)\npairs = []\npvals = []\nfor i in range(len(models)):\n    for j in range(i+1, len(models)):\n        m1, m2 = models[i], models[j]\n        b01, c10, p = mcnemar_exact(mat[m1].values, mat[m2].values)\n        pairs.append({\n            \"model_a\": m1,\n            \"model_b\": m2,\n            \"n_items_paired\": X.shape[0],\n            \"b(a=1,b=0)\": b01,\n            \"c(a=0,b=1)\": c10,\n            \"p_mcnemar\": p,\n        })\n        pvals.append(p)\n\npair_df = pd.DataFrame(pairs)\npair_df[\"p_holm\"] = holm(np.array(pvals))\npair_df = pair_df.sort_values([\"p_holm\", \"p_mcnemar\"])\n\nprint(\"\\n=== OVERALL: Pairwise McNemar exact (Holm corrected) ===\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 20, \"display.width\", 200):\n    print(pair_df.to_string(index=False))\n",
      "metadata": {
        "id": "dpxe1D4hh3oT"
      },
      "id": "dpxe1D4hh3oT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\nimport numpy as np\nimport pandas as pd\n\ndf = raw_df_mixed_357.copy()\n\nd = df[df[\"contam\"].isin([\"3\",\"5\",\"7\"])].copy()\n\ndef map_group_for_table(row):\n    if row[\"task\"] == \"tm_generic_halt\":\n        return \"tm_generic_halt\"\n    if row[\"group\"] == \"sgu\":\n        return \"sgu_other\"\n    return row[\"group\"]\n\nd[\"group_for_table\"] = d.apply(map_group_for_table, axis=1)\n\nCATS = [\"3\",\"5\",\"7\"]\n\ndef chi2_test_2x3(counts_a, counts_b):\n    \"\"\"\n    counts_a/b: length-3 array for categories 3/5/7\n    return chi2, p, dof\n    \"\"\"\n    obs = np.vstack([counts_a, counts_b]).astype(float)\n    rsum = obs.sum(axis=1, keepdims=True)\n    csum = obs.sum(axis=0, keepdims=True)\n    total = obs.sum()\n    if total == 0:\n        return np.nan, np.nan, 2\n    exp = (rsum @ csum) / total\n    mask = exp > 0\n    chi2 = ((obs[mask] - exp[mask])**2 / exp[mask]).sum()\n    dof = (obs.shape[0]-1)*(obs.shape[1]-1)  # (2-1)*(3-1)=2\n    try:\n        from scipy.stats import chi2 as chi2dist\n        p = chi2dist.sf(chi2, df=dof)\n    except Exception:\n        p = np.nan\n    return float(chi2), float(p), int(dof)\n\ndef holm(pvals):\n    pvals = np.asarray(pvals, dtype=float)\n    m = len(pvals)\n    order = np.argsort(pvals)\n    adj = np.empty(m, dtype=float)\n    for i, idx in enumerate(order):\n        adj[idx] = min(1.0, (m - i) * pvals[idx])\n    for i in range(1, m):\n        adj[order[i]] = max(adj[order[i]], adj[order[i-1]])\n    return adj\n\ndef counts_for(subdf):\n    vc = subdf[\"contam\"].value_counts()\n    return np.array([int(vc.get(c, 0)) for c in CATS], dtype=int)\n\n# ============================\n# ============================\nrows = []\nfor model, dm in d.groupby(\"model\"):\n    a = dm[dm[\"group_for_table\"] == \"gsm8k\"]\n    b = dm[dm[\"group_for_table\"] != \"gsm8k\"]\n    ca = counts_for(a)\n    cb = counts_for(b)\n    chi2, p, dof = chi2_test_2x3(ca, cb)\n    rows.append({\n        \"model\": model,\n        \"contrast\": \"gsm8k vs all_others\",\n        \"n_gsm8k\": int(ca.sum()),\n        \"n_others\": int(cb.sum()),\n        \"gsm8k_3/5/7\": f\"{ca[0]}/{ca[1]}/{ca[2]}\",\n        \"others_3/5/7\": f\"{cb[0]}/{cb[1]}/{cb[2]}\",\n        \"chi2\": chi2,\n        \"dof\": dof,\n        \"p\": p,\n    })\n\noverall_cmp = pd.DataFrame(rows).sort_values(\"p\")\noverall_cmp[\"p_holm_within_modelset\"] = holm(overall_cmp[\"p\"].values)\n\nprint(\"\\n=== GSM8K vs ALL OTHERS (per model) ===\")\nwith pd.option_context(\"display.max_rows\", 500, \"display.max_columns\", 50, \"display.width\", 220):\n    print(overall_cmp.to_string(index=False))\n\n# ============================\n# ============================\ngroups_to_compare = [\"aqua\", \"p_time\", \"np_time\", \"sgu_other\", \"tm_generic_halt\"]\nrows = []\nfor model, dm in d.groupby(\"model\"):\n    gsm = dm[dm[\"group_for_table\"] == \"gsm8k\"]\n    c_gsm = counts_for(gsm)\n    for g in groups_to_compare:\n        other = dm[dm[\"group_for_table\"] == g]\n        c_o = counts_for(other)\n        chi2, p, dof = chi2_test_2x3(c_gsm, c_o)\n        rows.append({\n            \"model\": model,\n            \"contrast\": f\"gsm8k vs {g}\",\n            \"n_gsm8k\": int(c_gsm.sum()),\n            f\"n_{g}\": int(c_o.sum()),\n            \"gsm8k_3/5/7\": f\"{c_gsm[0]}/{c_gsm[1]}/{c_gsm[2]}\",\n            f\"{g}_3/5/7\": f\"{c_o[0]}/{c_o[1]}/{c_o[2]}\",\n            \"chi2\": chi2,\n            \"dof\": dof,\n            \"p\": p,\n        })\n\npair_cmp = pd.DataFrame(rows)\n\npair_cmp[\"p_holm_within_model\"] = np.nan\nfor model in pair_cmp[\"model\"].unique():\n    mask = pair_cmp[\"model\"] == model\n    pair_cmp.loc[mask, \"p_holm_within_model\"] = holm(pair_cmp.loc[mask, \"p\"].values)\n\npair_cmp = pair_cmp.sort_values([\"model\", \"p_holm_within_model\", \"p\"])\n\nprint(\"\\n=== GSM8K vs EACH OTHER GROUP (per model, Holm corrected within model) ===\")\nwith pd.option_context(\"display.max_rows\", 2000, \"display.max_columns\", 60, \"display.width\", 260):\n    print(pair_cmp.to_string(index=False))\n",
      "metadata": {
        "id": "AbDsAeplmLUs"
      },
      "id": "AbDsAeplmLUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# =========================\n# FIX: build `datasets` from harmonized JSONL (standalone)\n# =========================\nimport os, json, re\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nBASE_DIR = os.environ.get(\"BAP_BASE_DIR\", \"/content/drive/MyDrive/complexity_data6\")\nHARMONIZED_PATH = os.environ.get(\"BAP_HARMONIZED_JSONL\", os.path.join(BASE_DIR, \"all_tasks_harmonized.jsonl\"))\n\nSGU_SUFFIX = \"__sgu\"\nSGU_COMPLEXITY_FAMILY_PATTERNS = [\n    \"strongly_generically_undecidable\",\n    \"strongly generically undecidable\",\n    \"sgu\",\n    \"undecidable\",\n]\n\n@dataclass\nclass TaskInstance:\n    instance_id: str\n    task_type: str          # here we use \"binary\"\n    prompt: str\n    label01: Optional[int] = None\n    ground_truth: Optional[str] = None\n    meta: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass TaskDataset:\n    name: str\n    instances: List[TaskInstance]\n\ndef load_harmonized_jsonl_df(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Missing harmonized JSONL: {path}\")\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                rows.append(json.loads(line))\n    df = pd.DataFrame(rows)\n\n    def _to01(x):\n        if isinstance(x, (bool, np.bool_)): return int(x)\n        if isinstance(x, (int, np.integer)): return int(x != 0)\n        s = str(x).strip().lower()\n        if s in [\"1\",\"true\",\"yes\",\"y\",\"t\"]: return 1\n        if s in [\"0\",\"false\",\"no\",\"n\",\"f\"]: return 0\n        m = re.search(r\"[01]\", s)\n        return int(m.group(0)) if m else 0\n\n    for req in [\"input\", \"label\", \"task\"]:\n        if req not in df.columns:\n            raise ValueError(f\"harmonized JSONL missing required column: {req}\")\n\n    df[\"label\"] = df[\"label\"].apply(_to01).astype(int)\n\n    if \"complexity_family\" not in df.columns:\n        df[\"complexity_family\"] = np.nan\n\n    return df\n\ndef build_harmonized_datasets(df: pd.DataFrame, include_sgu_slice: bool = True) -> Dict[str, TaskDataset]:\n    out: Dict[str, TaskDataset] = {}\n\n    def _make(inst_df: pd.DataFrame, name: str):\n        insts = []\n        for i, r in inst_df.reset_index(drop=True).iterrows():\n            meta = {k: r[k] for k in inst_df.columns if k not in [\"input\", \"label\", \"task\"]}\n            insts.append(TaskInstance(\n                instance_id=f\"{name}-{i}\",\n                task_type=\"binary\",\n                prompt=str(r[\"input\"]),\n                label01=int(r[\"label\"]),\n                meta=meta\n            ))\n        out[name] = TaskDataset(name=name, instances=insts)\n\n    tasks = sorted(df[\"task\"].dropna().astype(str).unique().tolist())\n    for t in tasks:\n        df_t = df[df[\"task\"] == t].copy()\n        if len(df_t) == 0:\n            continue\n        _make(df_t, t)\n\n        if include_sgu_slice:\n            cf = df_t[\"complexity_family\"].fillna(\"\").astype(str).str.lower()\n            mask = np.zeros(len(df_t), dtype=bool)\n            for pat in SGU_COMPLEXITY_FAMILY_PATTERNS:\n                mask |= cf.str.contains(pat.lower(), na=False).to_numpy()\n            if mask.any():\n                _make(df_t.loc[mask].copy(), t + SGU_SUFFIX)\n\n    return out\n\n# ---- build datasets ----\ndf_h = load_harmonized_jsonl_df(HARMONIZED_PATH)\ndatasets = build_harmonized_datasets(df_h, include_sgu_slice=True)\n\nprint(\"Built datasets. num_tasks =\", len(datasets))\nprint(\"Example keys:\", list(datasets.keys())[:20])",
      "metadata": {
        "id": "oJXV5n1BPg5m"
      },
      "id": "oJXV5n1BPg5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ============================================\n# FULL SCRIPT (LEAK EXPERIMENT + RAW PER-ITEM SAVES)\n# - contamination self-report as {3,5,7}\n# - Groups: gsm8k, aqua, p_time (2 subtasks), np_time (4 subtasks), sgu (10 subtasks)\n# - Leak levels: 0%, 50%, 100% (Train=300; Test=300)\n# - For each (group, leak, model): BEFORE eval -> LoRA SFT -> AFTER eval on SAME test\n# - Saves:\n#   1) splits: OUT_ROOT/splits/<group>/test_300.jsonl and train_leak{0,50,100}_300.jsonl\n#   2) adapters: OUT_ROOT/adapters/<group>/<leak>/<model>/\n#   3) per-item raw outputs: OUT_ROOT/raw_eval/<group>/<leak>/<model>/{before,after}.jsonl\n#   4) summary CSV: OUT_ROOT/results_before_after.csv\n#\n# PREREQS expected in your notebook:\n#   - datasets: Dict[str, TaskDataset], each TaskDataset.instances contains TaskInstance(prompt,label01)\n#   - Your environment has transformers + peft installed\n#   - GPU strongly recommended\n# ============================================\n\nimport os, json, random, gc, re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n    TrainingArguments, Trainer,\n    GenerationConfig, StoppingCriteria, StoppingCriteriaList\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\n# ----------------------------\n# 0) Your MODEL_SPECS style\n# ----------------------------\nMODEL_SPECS = [\n    {\"name\": \"qwen3-4b\",         \"model_id\": \"Qwen/Qwen3-4B\",                              \"adapter\": None},\n    {\"name\": \"deepseek-llama-8b\",\"model_id\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",   \"adapter\": None},\n    {\"name\": \"deepseek-math-7b\", \"model_id\": \"deepseek-ai/deepseek-math-7b-instruct\",      \"adapter\": None},\n    {\"name\": \"acemath-7b\",       \"model_id\": \"nvidia/AceMath-7B-Instruct\",                 \"adapter\": None},\n    {\"name\": \"Mathstral-7B\",     \"model_id\": \"mistralai/Mathstral-7B-v0.1\",                \"adapter\": None},\n]\n\n# ----------------------------\n# 1) Config\n# ----------------------------\nSEED = 123\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nOUT_ROOT = os.environ.get(\"EVAL_OUT_DIR\", \"/content/drive/MyDrive/complexity7/eval_outputs_leak\")\nos.makedirs(OUT_ROOT, exist_ok=True)\n\nUSE_4BIT = bool(int(os.environ.get(\"BAP_4BIT\", \"0\")))  # reuse your env var if set\n\nN_TEST = 300\nN_TRAIN = 300\nLEAK_LEVELS = {\"leak0\": 0.0, \"leak50\": 0.5, \"leak100\": 1.0}\n\n# LoRA hyperparams\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\nTARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # adjust if a model complains\n\n# Train hyperparams (start small; scale up after sanity check)\nLR = 2e-4\nEPOCHS = 4\nBATCH = 2\nGRAD_ACCUM = 8\nMAX_LEN = 512\n\n# ----------------------------\n# 2) Groups (include SGU + NP/P multi-subtasks)\n# ----------------------------\nSGU_TASKS = [\n    \"sgu_collatz_aligned\", \"sgu_collatz_aligned__sgu\",\n    \"sgu_index_empty_language\", \"sgu_index_empty_language__sgu\",\n    \"sgu_index_total_halt\", \"sgu_index_total_halt__sgu\",\n    \"sgu_semigroup_wp_amp\", \"sgu_semigroup_wp_amp__sgu\",\n    \"tm_generic_halt\", \"tm_hard_halt\",\n]\n\nGROUP_DEFS = {\n    \"gsm8k\": [\"gsm8k_binary\"],\n    \"aqua\": [\"aqua_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n    \"sgu\": SGU_TASKS,\n}\n\n# ----------------------------\n# 3) Model meta (family/size)\n# ----------------------------\ndef model_family(model_id: str) -> str:\n    s = model_id.lower()\n    if \"qwen\" in s: return \"qwen\"\n    if \"deepseek\" in s: return \"deepseek\"\n    if \"mistral\" in s or \"mathstral\" in s: return \"mistral\"\n    if \"nvidia\" in s or \"acemath\" in s: return \"nvidia\"\n    return \"other\"\n\ndef model_size_tag(s: str) -> str:\n    s = s.lower()\n    m = re.search(r\"(\\d+)\\s*b\", s)\n    if m: return f\"{m.group(1)}B\"\n    m = re.search(r\"-(\\d+)b\", s)\n    if m: return f\"{m.group(1)}B\"\n    return \"\"\n\n# ----------------------------\n# 4) Decoding + prompts/parsers (YES/NO + 3/5/7)\n# ----------------------------\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_strings):\n        self.stop_token_ids = []\n        for s in stop_strings:\n            ids = tokenizer.encode(s, add_special_tokens=False)\n            if ids:\n                self.stop_token_ids.append(ids[-1])\n    def __call__(self, input_ids, scores, **kwargs):\n        if input_ids.shape[1] == 0: return False\n        return input_ids[0, -1].item() in self.stop_token_ids\n\ndef generate_strict(model, tok, prompt: str, max_tokens: int = 10):\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    stopper = StopOnTokens(tok, stop_strings=[\"\\n\", \"\\r\", \"Question:\", \"Options:\"])\n    gen_config = GenerationConfig(\n        do_sample=False, temperature=0.0, max_new_tokens=max_tokens,\n        pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id,\n    )\n    with torch.no_grad():\n        out = model.generate(**inputs, generation_config=gen_config,\n                             stopping_criteria=StoppingCriteriaList([stopper]))\n    full = tok.decode(out[0], skip_special_tokens=True)\n    gen = full[len(prompt):] if full.startswith(prompt) else full\n    return gen.split(\"\\n\")[0].strip()\n\ndef build_solver_prompt(question: str) -> str:\n    instruction = (\n        \"You are a calculator. Output strictly ONLY the final numerical answer or Yes/No.\\n\"\n        \"Do NOT explain. Do NOT show work.\\n\"\n    )\n    return f\"{instruction}\\nQuestion: {question.strip()}\\nAnswer:\"\n\ndef build_contamination_prompt_357(question: str) -> str:\n    instruction = (\n        \"You are an AI assistant. Analyze the following problem.\\n\"\n        \"Tell me if you have seen this problem during your training data.\\n\"\n        \"Reply with strictly ONLY a single digit from {3, 5, 7}:\\n\"\n        \"3 = SEEN the same question before\\n\"\n        \"5 = Seen very similar questions before but not the same question\\n\"\n        \"7 = Not seen similar question before\\n\"\n        \"Ignore any YES/NO question in the content even if it asks.\\n\"\n    )\n    return f\"{instruction}\\nProblem Content: {question.strip()}\\n\\nReply (3, 5, or 7):\"\n\ndef parse_yesno(s: str):\n    if not s: return None\n    t = s.strip().upper()\n    if \"YES\" in t: return \"YES\"\n    if \"NO\" in t: return \"NO\"\n    return None\n\ndef parse_357(s: str):\n    if not s: return None\n    m = re.search(r\"\\b([357])\\b\", s.strip())\n    return m.group(1) if m else None\n\n# ----------------------------\n# 5) JSONL helpers\n# ----------------------------\ndef write_jsonl(path, rows):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\ndef load_jsonl(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            rows.append(json.loads(line))\n    return rows\n\n# ----------------------------\n# 6) Build & save splits (per group: test_300 + train_300 with leak 0/50/100)\n# ----------------------------\ndef group_pool(group_name: str):\n    pool = []\n    for tk in GROUP_DEFS[group_name]:\n        if tk not in datasets:\n            continue\n        for i, inst in enumerate(datasets[tk].instances):\n            if inst.label01 is None:\n                continue\n            pool.append({\n                \"group\": group_name,\n                \"task\": tk,\n                \"local_idx\": i,\n                \"prompt\": inst.prompt,\n                \"label01\": int(inst.label01),\n            })\n    return pool\n\ndef build_and_save_splits():\n    \"\"\"\n    Auto-adjust per group so that we can ALWAYS form:\n      test_n + train_n <= pool_size, and train can be (1-leak)*new + leak*test\n    Strategy:\n      - Try (TARGET_TEST=300, TARGET_TRAIN=300)\n      - If pool too small, set test_n = min(300, pool_size//2), train_n = test_n\n        (so aqua with 300 -> test=150, train=150)\n    \"\"\"\n    manifest = {}\n\n    TARGET_TEST = N_TEST   # your global (e.g. 300)\n    TARGET_TRAIN = N_TRAIN # your global (e.g. 300)\n\n    for g in GROUP_DEFS:\n        pool = group_pool(g)\n        pool_size = len(pool)\n        if pool_size < 2:\n            raise RuntimeError(f\"[{g}] pool too small: {pool_size}\")\n\n        # choose per-group sizes\n        if pool_size >= (TARGET_TEST + TARGET_TRAIN):\n            test_n = TARGET_TEST\n            train_n = TARGET_TRAIN\n        else:\n            # fallback: split roughly half/half\n            test_n = min(TARGET_TEST, pool_size // 2)\n            train_n = min(TARGET_TRAIN, pool_size - test_n)\n            # to keep leak definitions symmetric, make train_n == test_n when possible\n            train_n = min(train_n, test_n)\n\n        if test_n == 0 or train_n == 0:\n            raise RuntimeError(f\"[{g}] cannot form non-empty test/train with pool_size={pool_size}\")\n\n        rng = random.Random(SEED + (hash(g) % 10000))\n        rng.shuffle(pool)\n\n        test = pool[:test_n]\n        rest = pool[test_n:]\n\n        if len(rest) < train_n:\n            # if still insufficient (edge), shrink train_n\n            train_n = len(rest)\n            if train_n == 0:\n                raise RuntimeError(f\"[{g}] no remaining items for train after test selection.\")\n\n        fresh = rest[:train_n]\n\n        # save test\n        test_path = os.path.join(OUT_ROOT, \"splits\", g, f\"test_{test_n}.jsonl\")\n        write_jsonl(test_path, test)\n\n        # save trains for leak levels\n        for tag, leak in LEAK_LEVELS.items():\n            n_leak = int(round(train_n * leak))\n            n_new = train_n - n_leak\n\n            # leaked part always comes from test\n            leaked = test[:min(n_leak, len(test))]\n\n            # new part comes from fresh (rest)\n            newpart = fresh[:min(n_new, len(fresh))]\n\n            # if fresh not enough for required newpart (can happen when pool is tiny),\n            # we shrink newpart accordingly (still valid but deviates from exact ratio)\n            train = newpart + leaked\n            train_path = os.path.join(OUT_ROOT, \"splits\", g, f\"train_{tag}_{train_n}.jsonl\")\n            write_jsonl(train_path, train)\n\n            manifest[f\"{g}::{tag}\"] = {\n                \"group\": g,\n                \"leak_tag\": tag,\n                \"leak\": leak,\n                \"pool_size\": pool_size,\n                \"test_path\": test_path,\n                \"train_path\": train_path,\n                \"n_test\": len(test),\n                \"n_train\": len(train),\n                \"target_test\": test_n,\n                \"target_train\": train_n,\n                \"n_new\": len(newpart),\n                \"n_leak\": len(leaked),\n            }\n\n        print(f\"[SPLIT] {g}: pool={pool_size}, test={len(test)}, train_target={train_n}\")\n\n    man_path = os.path.join(OUT_ROOT, \"splits\", \"split_manifest.json\")\n    os.makedirs(os.path.dirname(man_path), exist_ok=True)\n    with open(man_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(manifest, f, ensure_ascii=False, indent=2)\n    print(\"Saved split manifest:\", man_path)\n    return manifest\n\n\n# ----------------------------\n# 7) SFT dataset (prompt -> YES/NO)\n# ----------------------------\ndef label_to_yesno(label01: int) -> str:\n    return \"YES\" if int(label01) == 1 else \"NO\"\n\ndef make_sft_items(rows, tok):\n    items = []\n    for r in rows:\n        prompt = build_solver_prompt(r[\"prompt\"])\n        ans = label_to_yesno(r[\"label01\"])\n        full = prompt + \" \" + ans\n\n        enc_full = tok(full, truncation=True, max_length=MAX_LEN)\n        enc_prompt = tok(prompt, truncation=True, max_length=MAX_LEN)\n\n        labels = np.array(enc_full[\"input_ids\"], dtype=np.int64)\n        labels[:len(enc_prompt[\"input_ids\"])] = -100\n\n        items.append({\n            \"input_ids\": enc_full[\"input_ids\"],\n            \"attention_mask\": enc_full[\"attention_mask\"],\n            \"labels\": labels.tolist(),\n        })\n    return items\n\n# class SimpleDataset(torch.utils.data.Dataset):\n#     def __init__(self, items): self.items = items\n#     def __len__(self): return len(self.items)\n#     def __getitem__(self, i):\n#         x = self.items[i]\n#         return {k: torch.tensor(v) for k, v in x.items()}\n\n# ----------------------------\n# 8) Load base + finetune LoRA\n# ----------------------------\ndef load_base(model_id, use_4bit=False):\n    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    kwargs = {}\n    if use_4bit:\n        try:\n            from transformers import BitsAndBytesConfig\n            kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n            kwargs[\"device_map\"] = \"auto\"\n        except Exception as e:\n            print(\"[WARN] 4bit requested but BitsAndBytes not available:\", e)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n        **kwargs\n    )\n    model.eval()\n    return tok, model\n\n# def finetune_lora(model_id, train_rows, out_dir, use_4bit=False):\n#     tok, base = load_base(model_id, use_4bit=use_4bit)\n\n#     lora_cfg = LoraConfig(\n#         r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n#         bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=TARGET_MODULES,\n#     )\n#     model = get_peft_model(base, lora_cfg)\n#     model.train()\n\n#     train_items = make_sft_items(train_rows, tok)\n#     train_ds = SimpleDataset(train_items)\n\n#     args = TrainingArguments(\n#         output_dir=out_dir,\n#         per_device_train_batch_size=BATCH,\n#         gradient_accumulation_steps=GRAD_ACCUM,\n#         num_train_epochs=EPOCHS,\n#         learning_rate=LR,\n#         logging_steps=20,\n#         save_strategy=\"no\",\n#         report_to=\"none\",\n#         fp16=torch.cuda.is_available(),\n#     )\n#     Trainer(model=model, args=args, train_dataset=train_ds).train()\n\n#     os.makedirs(out_dir, exist_ok=True)\n#     model.save_pretrained(out_dir)\n#     tok.save_pretrained(out_dir)\n#     return out_dir\n# import torch\n# from torch.nn.utils.rnn import pad_sequence\n\nclass SimpleDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Return python lists; let collator pad dynamically.\n    \"\"\"\n    def __init__(self, items):\n        self.items = items\n    def __len__(self):\n        return len(self.items)\n    def __getitem__(self, i):\n        return self.items[i]\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass DataCollatorForCausalLMWithLabelPadding:\n    \"\"\"\n    Pads input_ids/attention_mask to max length in batch.\n    Pads labels to same length using -100.\n    \"\"\"\n    def __init__(self, pad_token_id: int, label_pad_token_id: int = -100):\n        self.pad_token_id = pad_token_id\n        self.label_pad_token_id = label_pad_token_id\n\n    def __call__(self, features):\n        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n        attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ndef finetune_lora(model_id, train_rows, out_dir, use_4bit=False):\n    tok, base = load_base(model_id, use_4bit=use_4bit)\n\n    lora_cfg = LoraConfig(\n        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n        bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=TARGET_MODULES,\n    )\n    model = get_peft_model(base, lora_cfg)\n    model.train()\n\n    train_items = make_sft_items(train_rows, tok)  # your function that returns list of dicts\n    train_ds = SimpleDataset(train_items)\n\n    collator = DataCollatorForCausalLMWithLabelPadding(\n        pad_token_id=tok.pad_token_id,\n        label_pad_token_id=-100\n    )\n\n    args = TrainingArguments(\n        output_dir=out_dir,\n        per_device_train_batch_size=BATCH,\n        gradient_accumulation_steps=GRAD_ACCUM,\n        num_train_epochs=EPOCHS,\n        learning_rate=LR,\n        logging_steps=20,\n        save_strategy=\"no\",\n        report_to=\"none\",\n        fp16=torch.cuda.is_available(),\n        remove_unused_columns=False,  # important when using custom collator/dataset dicts\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_ds,\n        data_collator=collator,\n    )\n    trainer.train()\n\n    import os\n    os.makedirs(out_dir, exist_ok=True)\n    model.save_pretrained(out_dir)\n    tok.save_pretrained(out_dir)\n    return out_dir\n# ----------------------------\n# 9) Eval on test (DETAILED per-item + summary)\n# ----------------------------\ndef eval_model_on_test_detailed(model, tok, test_rows):\n    total = 0\n    fmt = 0\n    correct_sum = 0\n    cc = {\"3\":0,\"5\":0,\"7\":0,\"other\":0,\"empty\":0}\n    detailed = []\n\n    for i, r in enumerate(test_rows):\n        total += 1\n        q = r[\"prompt\"]\n        y_true = int(r[\"label01\"])\n\n        solver_prompt = build_solver_prompt(q)\n        solver_raw = generate_strict(model, tok, solver_prompt, max_tokens=15)\n        solver_pred = parse_yesno(solver_raw)\n\n        solver_ok = None\n        if solver_pred is not None:\n            fmt += 1\n            y_pred = 1 if solver_pred == \"YES\" else 0\n            solver_ok = int(y_pred == y_true)\n            correct_sum += solver_ok\n\n        contam_prompt = build_contamination_prompt_357(q)\n        contam_raw = generate_strict(model, tok, contam_prompt, max_tokens=5)\n        contam_pred = parse_357(contam_raw)\n\n        if contam_raw is None or contam_raw == \"\":\n            cc[\"empty\"] += 1\n        elif contam_pred in [\"3\",\"5\",\"7\"]:\n            cc[contam_pred] += 1\n        else:\n            cc[\"other\"] += 1\n\n        detailed.append({\n            \"idx\": i,\n            \"task\": r.get(\"task\"),\n            \"local_idx\": r.get(\"local_idx\"),\n            \"label01\": y_true,\n            \"solver_pred\": solver_pred,\n            \"solver_correct\": solver_ok,\n            \"solver_raw\": solver_raw,\n            \"contam_pred\": contam_pred,\n            \"contam_raw\": contam_raw,\n        })\n\n    acc_on_fmt = correct_sum / fmt if fmt else float(\"nan\")\n    fmt_rate = fmt / total if total else 0.0\n    acc_overall = correct_sum / total if total else float(\"nan\")\n\n    summary = {\n        \"n_total\": total,\n        \"n_fmt\": fmt,\n        \"fmt_rate\": fmt_rate,\n        \"acc_on_fmt\": acc_on_fmt,\n        \"acc_overall\": acc_overall,\n        \"c3\": cc[\"3\"], \"c5\": cc[\"5\"], \"c7\": cc[\"7\"],\n        \"c_other\": cc[\"other\"], \"c_empty\": cc[\"empty\"],\n    }\n    return summary, detailed\n\n# ----------------------------\n# 10) Run experiment (before/after) + save summary + raw per-item\n# ----------------------------\n# SKIP_GROUPS = {\"aqua\"}  # <- add\n\n# def run_leak_experiment_with_raw():\n#     build_and_save_splits()\n#     results = []\n\n#     for g in GROUP_DEFS.keys():\n#         if g in SKIP_GROUPS:\n#             print(f\"[SKIP] group={g}\")\n#             continue\n\n#         test_path = os.path.join(OUT_ROOT, \"splits\", g, f\"test_{N_TEST}.jsonl\")\n#         test_rows = load_jsonl(test_path)\n\n#         for leak_tag in LEAK_LEVELS.keys():\n#             train_path = os.path.join(OUT_ROOT, \"splits\", g, f\"train_{leak_tag}_{N_TRAIN}.jsonl\")\n#             train_rows = load_jsonl(train_path)\n\n#             for spec in MODEL_SPECS:\n#                 name = spec[\"name\"]\n#                 model_id = spec[\"model_id\"]\n#                 fam = model_family(model_id)\n#                 size = model_size_tag(name) or model_size_tag(model_id)\n\n#                 print(\"\\n\" + \"=\"*90)\n#                 print(f\"[RUN] group={g} leak={leak_tag} model={name} ({model_id})\")\n\n#                 # -------- BEFORE --------\n#                 tok0, base0 = load_base(model_id, use_4bit=USE_4BIT)\n#                 before_summary, before_rows = eval_model_on_test_detailed(base0, tok0, test_rows)\n\n#                 before_raw_path = os.path.join(OUT_ROOT, \"raw_eval\", g, leak_tag, name, \"before.jsonl\")\n#                 for rr in before_rows:\n#                     rr.update({\"group\": g, \"leak\": leak_tag, \"model\": name, \"phase\": \"before\"})\n#                 write_jsonl(before_raw_path, before_rows)\n\n#                 del base0\n#                 gc.collect()\n#                 if torch.cuda.is_available():\n#                     torch.cuda.empty_cache()\n\n#                 # -------- TRAIN LoRA --------\n#                 adapter_dir = os.path.join(OUT_ROOT, \"adapters\", g, leak_tag, name)\n#                 finetune_lora(model_id, train_rows, out_dir=adapter_dir, use_4bit=USE_4BIT)\n\n#                 # -------- AFTER --------\n#                 tok1, base1 = load_base(model_id, use_4bit=USE_4BIT)\n#                 model1 = PeftModel.from_pretrained(base1, adapter_dir)\n#                 model1.eval()\n#                 after_summary, after_rows = eval_model_on_test_detailed(model1, tok1, test_rows)\n\n#                 after_raw_path = os.path.join(OUT_ROOT, \"raw_eval\", g, leak_tag, name, \"after.jsonl\")\n#                 for rr in after_rows:\n#                     rr.update({\"group\": g, \"leak\": leak_tag, \"model\": name, \"phase\": \"after\"})\n#                 write_jsonl(after_raw_path, after_rows)\n\n#                 del model1, base1\n#                 gc.collect()\n#                 if torch.cuda.is_available():\n#                     torch.cuda.empty_cache()\n\n#                 # -------- SUMMARY ROW --------\n#                 row = {\n#                     \"group\": g,\n#                     \"leak\": leak_tag,\n#                     \"model\": name,\n#                     \"model_id\": model_id,\n#                     \"model_family\": fam,\n#                     \"model_size\": size,\n#                     \"train_path\": train_path,\n#                     \"test_path\": test_path,\n#                     \"adapter_dir\": adapter_dir,\n#                     \"before_raw_path\": before_raw_path,\n#                     \"after_raw_path\": after_raw_path,\n#                     \"n_train\": len(train_rows),\n#                     \"n_test\": len(test_rows),\n#                 }\n#                 for k,v in before_summary.items(): row[f\"before_{k}\"] = v\n#                 for k,v in after_summary.items():  row[f\"after_{k}\"]  = v\n\n#                 row[\"delta_acc_on_fmt\"] = row[\"after_acc_on_fmt\"] - row[\"before_acc_on_fmt\"]\n#                 row[\"delta_acc_overall\"] = row[\"after_acc_overall\"] - row[\"before_acc_overall\"]\n#                 row[\"delta_fmt_rate\"] = row[\"after_fmt_rate\"] - row[\"before_fmt_rate\"]\n#                 results.append(row)\n\n#     res_df = pd.DataFrame(results)\n#     out_csv = os.path.join(OUT_ROOT, \"results_before_after.csv\")\n#     res_df.to_csv(out_csv, index=False)\n\n#     print(\"\\nSaved summary results:\", out_csv)\n#     print(\"Per-item raw outputs saved under:\", os.path.join(OUT_ROOT, \"raw_eval\"))\n#     return res_df\n\n# # =========================\n# # RUN\n# # =========================\n# results_df = run_leak_experiment_with_raw()\n# results_df.head()\n\n\n# ----------------------------\n# 10) Run experiment (before/after) + save summary + raw per-item\n#     (manifest-aware: auto adapts to actual test/train sizes per group)\n# ----------------------------\ndef run_leak_experiment_with_raw_manifest_aware(skip_groups=None):\n    \"\"\"\n    Reads split_manifest.json produced by build_and_save_splits() and uses the\n    actual generated test/train paths and sizes per (group, leak_tag).\n\n    This avoids hard-coding test_{N_TEST}.jsonl / train_{leak_tag}_{N_TRAIN}.jsonl\n    when a group's pool is too small and the splitter auto-shrinks sizes.\n    \"\"\"\n    skip_groups = set(skip_groups or [])\n\n    # build splits + manifest\n    manifest = build_and_save_splits()\n\n    results = []\n\n    # iterate by group then leak_tag for reproducible order\n    for g in GROUP_DEFS.keys():\n        if g in skip_groups:\n            print(f\"[SKIP] group={g}\")\n            continue\n\n        for leak_tag in LEAK_LEVELS.keys():\n            key = f\"{g}::{leak_tag}\"\n            if key not in manifest:\n                print(f\"[WARN] manifest missing key={key}, skipping\")\n                continue\n\n            m = manifest[key]\n            test_path = m[\"test_path\"]\n            train_path = m[\"train_path\"]\n\n            # load the actual files that were created\n            if not os.path.exists(test_path):\n                print(f\"[WARN] missing test file: {test_path}, skipping {key}\")\n                continue\n            if not os.path.exists(train_path):\n                print(f\"[WARN] missing train file: {train_path}, skipping {key}\")\n                continue\n\n            test_rows = load_jsonl(test_path)\n            train_rows = load_jsonl(train_path)\n\n            # sanity print\n            print(\"\\n\" + \"-\" * 90)\n            print(f\"[SPLIT-USE] group={g} leak={leak_tag} \"\n                  f\"test={len(test_rows)} train={len(train_rows)} \"\n                  f\"(target_test={m.get('target_test')} target_train={m.get('target_train')} \"\n                  f\"new={m.get('n_new')} leak_items={m.get('n_leak')})\")\n\n            for spec in MODEL_SPECS:\n                name = spec[\"name\"]\n                model_id = spec[\"model_id\"]\n                fam = model_family(model_id)\n                size = model_size_tag(name) or model_size_tag(model_id)\n\n                print(\"\\n\" + \"=\" * 90)\n                print(f\"[RUN] group={g} leak={leak_tag} model={name} ({model_id})\")\n\n                # -------- BEFORE --------\n                tok0, base0 = load_base(model_id, use_4bit=USE_4BIT)\n                before_summary, before_rows = eval_model_on_test_detailed(base0, tok0, test_rows)\n\n                before_raw_path = os.path.join(OUT_ROOT, \"raw_eval\", g, leak_tag, name, \"before.jsonl\")\n                for rr in before_rows:\n                    rr.update({\"group\": g, \"leak\": leak_tag, \"model\": name, \"phase\": \"before\"})\n                write_jsonl(before_raw_path, before_rows)\n\n                del base0\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n                # -------- TRAIN LoRA --------\n                adapter_dir = os.path.join(OUT_ROOT, \"adapters\", g, leak_tag, name)\n                finetune_lora(model_id, train_rows, out_dir=adapter_dir, use_4bit=USE_4BIT)\n\n                # -------- AFTER --------\n                tok1, base1 = load_base(model_id, use_4bit=USE_4BIT)\n                model1 = PeftModel.from_pretrained(base1, adapter_dir)\n                model1.eval()\n                after_summary, after_rows = eval_model_on_test_detailed(model1, tok1, test_rows)\n\n                after_raw_path = os.path.join(OUT_ROOT, \"raw_eval\", g, leak_tag, name, \"after.jsonl\")\n                for rr in after_rows:\n                    rr.update({\"group\": g, \"leak\": leak_tag, \"model\": name, \"phase\": \"after\"})\n                write_jsonl(after_raw_path, after_rows)\n\n                del model1, base1\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n                # -------- SUMMARY ROW --------\n                row = {\n                    \"group\": g,\n                    \"leak\": leak_tag,\n                    \"model\": name,\n                    \"model_id\": model_id,\n                    \"model_family\": fam,\n                    \"model_size\": size,\n\n                    # from manifest\n                    \"pool_size\": m.get(\"pool_size\"),\n                    \"target_test\": m.get(\"target_test\"),\n                    \"target_train\": m.get(\"target_train\"),\n                    \"n_new\": m.get(\"n_new\"),\n                    \"n_leak\": m.get(\"n_leak\"),\n\n                    \"train_path\": train_path,\n                    \"test_path\": test_path,\n                    \"adapter_dir\": adapter_dir,\n                    \"before_raw_path\": before_raw_path,\n                    \"after_raw_path\": after_raw_path,\n                    \"n_train\": len(train_rows),\n                    \"n_test\": len(test_rows),\n                }\n                for k, v in before_summary.items():\n                    row[f\"before_{k}\"] = v\n                for k, v in after_summary.items():\n                    row[f\"after_{k}\"] = v\n\n                row[\"delta_acc_on_fmt\"] = row[\"after_acc_on_fmt\"] - row[\"before_acc_on_fmt\"]\n                row[\"delta_acc_overall\"] = row[\"after_acc_overall\"] - row[\"before_acc_overall\"]\n                row[\"delta_fmt_rate\"] = row[\"after_fmt_rate\"] - row[\"before_fmt_rate\"]\n                results.append(row)\n\n    res_df = pd.DataFrame(results)\n    out_csv = os.path.join(OUT_ROOT, \"results_before_after.csv\")\n    res_df.to_csv(out_csv, index=False)\n\n    print(\"\\nSaved summary results:\", out_csv)\n    print(\"Per-item raw outputs saved under:\", os.path.join(OUT_ROOT, \"raw_eval\"))\n    return res_df\n\n\n# =========================\n# RUN (manifest-aware)\n# =========================\n# 1) run everything (including aqua if split exists)\nresults_df = run_leak_experiment_with_raw_manifest_aware()\nresults_df.head()",
      "metadata": {
        "id": "dGeeQxI365Pu"
      },
      "id": "dGeeQxI365Pu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import os, json, glob, hashlib\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nman_path = os.path.join(OUT_ROOT, \"splits\", \"split_manifest.json\")\n\ndef read_jsonl(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            rows.append(json.loads(line))\n    return rows\n\ndef stable_hash(s: str) -> str:\n    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n\nmanifest = json.load(open(man_path, \"r\", encoding=\"utf-8\"))\n\ndef item_key(r):\n    t = r.get(\"task\", None)\n    li = r.get(\"local_idx\", None)\n    if t is not None and li is not None:\n        return (\"task_local\", t, int(li))\n    # fallback: prompt hash\n    p = r.get(\"prompt\") or r.get(\"question\") or \"\"\n    return (\"prompt_hash\", stable_hash(p.strip()))\n\nleaked_keys = {}  # (group, leak_tag) -> set(keys)\nfor key, m in manifest.items():\n    g = m[\"group\"]\n    leak_tag = m[\"leak_tag\"]\n    test = read_jsonl(m[\"test_path\"])\n    train = read_jsonl(m[\"train_path\"])\n    test_set = set(item_key(r) for r in test)\n    train_set = set(item_key(r) for r in train)\n    leaked = test_set & train_set\n    leaked_keys[(g, leak_tag)] = leaked\n\nraw_paths = glob.glob(os.path.join(OUT_ROOT, \"raw_eval\", \"*\", \"*\", \"*\", \"*.jsonl\"))\nall_rows = []\nfor p in raw_paths:\n    all_rows.extend(read_jsonl(p))\n\ndf = pd.DataFrame(all_rows)\n\ndf[\"solver_correct\"] = pd.to_numeric(df.get(\"solver_correct\"), errors=\"coerce\")\ndf_fmt = df[df[\"solver_correct\"].notna()].copy()\n\ndf_fmt[\"phase\"] = df_fmt[\"phase\"].astype(str)\n\ndef compute_item_key_row(row):\n    t = row.get(\"task\", None)\n    li = row.get(\"local_idx\", None)\n    if pd.notna(t) and pd.notna(li):\n        return (\"task_local\", str(t), int(li))\n    p = row.get(\"prompt\", None)\n    if isinstance(p, str) and p.strip():\n        return (\"prompt_hash\", stable_hash(p.strip()))\n    # last resort:\n    return (\"fallback\", str(row.get(\"task\", \"\")), int(row.get(\"idx\", -1)))\n\ndf_fmt[\"item_key\"] = df_fmt.apply(compute_item_key_row, axis=1)\n\nrun = (\n    df_fmt\n    .groupby([\"group\",\"leak\",\"model\",\"phase\"], as_index=False)\n    .agg(n=(\"solver_correct\",\"size\"), acc=(\"solver_correct\",\"mean\"))\n)\n\npivot = run.pivot_table(index=[\"group\",\"leak\",\"model\"], columns=\"phase\", values=\"acc\").reset_index()\npivot[\"delta\"] = pivot[\"after\"] - pivot[\"before\"]\n\nbase = pivot[pivot[\"leak\"]==\"leak0\"][[\"group\",\"model\",\"delta\"]].rename(columns={\"delta\":\"delta_leak0\"})\ndid = pivot.merge(base, on=[\"group\",\"model\"], how=\"left\")\ndid[\"leak_effect\"] = did[\"delta\"] - did[\"delta_leak0\"]\n\nkey_cols = [\"group\",\"leak\",\"model\",\"item_key\"]\n\nbefore = df_fmt[df_fmt[\"phase\"]==\"before\"][key_cols + [\"solver_correct\",\"contam_pred\"]].rename(columns={\"solver_correct\":\"y_before\"})\nafter  = df_fmt[df_fmt[\"phase\"]==\"after\"][ key_cols + [\"solver_correct\",\"contam_pred\"]].rename(columns={\"solver_correct\":\"y_after\"})\n\nmerged = before.merge(after, on=key_cols, how=\"inner\", suffixes=(\"_b\",\"_a\"))\nmerged[\"delta_item\"] = merged[\"y_after\"] - merged[\"y_before\"]\n\ndef leaked_indicator(row):\n    g, leak = row[\"group\"], row[\"leak\"]\n    lk = leaked_keys.get((g, leak), set())\n    return int(row[\"item_key\"] in lk)\n\nmerged[\"is_leaked_item\"] = merged.apply(leaked_indicator, axis=1)\n\nitem_te = (\n    merged\n    .groupby([\"group\",\"leak\",\"model\",\"is_leaked_item\"], as_index=False)\n    .agg(n=(\"delta_item\",\"size\"), delta_mean=(\"delta_item\",\"mean\"))\n)\n\nleaked_delta = item_te[item_te[\"is_leaked_item\"]==1][[\"group\",\"leak\",\"model\",\"delta_mean\"]].rename(columns={\"delta_mean\":\"delta_leaked\"})\nnon_delta    = item_te[item_te[\"is_leaked_item\"]==0][[\"group\",\"leak\",\"model\",\"delta_mean\"]].rename(columns={\"delta_mean\":\"delta_nonleaked\"})\n\nitem_contrast = leaked_delta.merge(non_delta, on=[\"group\",\"leak\",\"model\"], how=\"inner\")\nitem_contrast[\"delta_leaked_minus_nonleaked\"] = item_contrast[\"delta_leaked\"] - item_contrast[\"delta_nonleaked\"]\n\ndef chi2_test(sub):\n    tab = pd.crosstab(sub[\"contam_pred\"], sub[\"solver_correct\"])\n    if tab.shape[0] < 2 or tab.shape[1] < 2:\n        return pd.Series({\"chi2\":np.nan,\"pval\":np.nan,\"dof\":np.nan})\n    chi2, p, dof, _ = chi2_contingency(tab.values)\n    return pd.Series({\"chi2\":chi2,\"pval\":p,\"dof\":dof})\n\ndet = (\n    df_fmt[df_fmt[\"contam_pred\"].isin([\"3\",\"5\",\"7\"])]\n    .groupby([\"group\",\"leak\",\"model\",\"phase\"])\n    .apply(chi2_test)\n    .reset_index()\n)\n\n\nleak_order = [\"leak0\", \"leak50\", \"leak100\"]\nfor _df in [did, item_contrast, det]:\n    if \"leak\" in _df.columns:\n        _df[\"leak\"] = pd.Categorical(_df[\"leak\"], categories=leak_order, ordered=True)\n\ndid_sorted = did.sort_values([\"group\",\"model\",\"leak\"])\nitem_sorted = item_contrast.sort_values([\"group\",\"model\",\"leak\"])\ndet_sorted = det.sort_values([\"group\",\"model\",\"leak\",\"phase\"])\n\nprint(\"\\n=== Diff-in-Diff (ALL leaks) ===\")\nprint(did_sorted[[\"group\",\"model\",\"leak\",\"before\",\"after\",\"delta\",\"delta_leak0\",\"leak_effect\"]].to_string(index=False))\n\nprint(\"\\n=== Diff-in-Diff (leak100 only) ===\")\nprint(did_sorted[did_sorted[\"leak\"]==\"leak100\"][[\"group\",\"model\",\"leak\",\"before\",\"after\",\"delta\",\"delta_leak0\",\"leak_effect\"]].to_string(index=False))\n\nprint(\"\\n=== Item-level contrast (ALL leaks) ===\")\nprint(item_sorted[[\"group\",\"model\",\"leak\",\"delta_leaked\",\"delta_nonleaked\",\"delta_leaked_minus_nonleaked\"]].to_string(index=False))\n\nprint(\"\\n=== Item-level contrast (leak100 only) ===\")\nprint(item_sorted[item_sorted[\"leak\"]==\"leak100\"][[\"group\",\"model\",\"leak\",\"delta_leaked\",\"delta_nonleaked\",\"delta_leaked_minus_nonleaked\"]].to_string(index=False))\n\nprint(\"\\n=== Detection (chi-square p-values) ===\")\nprint(det_sorted[[\"group\",\"model\",\"leak\",\"phase\",\"chi2\",\"pval\"]].to_string(index=False))\n\nprint(\"\\n=== Detection (leak100 only) ===\")\nprint(det_sorted[det_sorted[\"leak\"]==\"leak100\"][[\"group\",\"model\",\"leak\",\"phase\",\"chi2\",\"pval\"]].to_string(index=False))\n\ndid.to_csv(os.path.join(OUT_ROOT, \"analysis_did.csv\"), index=False)\nitem_contrast.to_csv(os.path.join(OUT_ROOT, \"analysis_item_contrast.csv\"), index=False)\ndet.to_csv(os.path.join(OUT_ROOT, \"analysis_detection_chi2.csv\"), index=False)\n\nprint(\"\\nSaved: analysis_did.csv, analysis_item_contrast.csv, analysis_detection_chi2.csv\")\n\nif not os.path.exists(MANIFEST_PATH):\n    raise FileNotFoundError(f\"missing {MANIFEST_PATH}\")\n\nmanifest = json.load(open(MANIFEST_PATH, \"r\", encoding=\"utf-8\"))\n\nindex_rows = []\n\nfor key, m in manifest.items():\n    g = m[\"group\"]\n    leak_tag = m[\"leak_tag\"]\n    test_path = m[\"test_path\"]\n    train_path = m[\"train_path\"]\n\n    if not (os.path.exists(test_path) and os.path.exists(train_path)):\n        print(f\"[SKIP] missing files for {key}\")\n        continue\n\n    test = load_jsonl(test_path)\n    train = load_jsonl(train_path)\n\n    test_keys = [stable_key(r) for r in test]\n    train_keys = set(stable_key(r) for r in train)\n    leaked_set = set(k for k in test_keys if k in train_keys)\n\n    test_out = []\n    for r in test:\n        k = stable_key(r)\n        test_out.append({\n            \"group\": g,\n            \"leak_tag\": leak_tag,\n            \"item_key_type\": k[0],\n            \"item_key\": \"::\".join(map(str, k[1:])),\n            \"is_leaked_into_train\": int(k in leaked_set),\n\n            \"task\": r.get(\"task\"),\n            \"local_idx\": r.get(\"local_idx\"),\n            \"label01\": r.get(\"label01\"),\n            \"prompt\": r.get(\"prompt\"),\n        })\n\n    out_dir = os.path.join(OUT_AUDIT, g, leak_tag)\n    os.makedirs(out_dir, exist_ok=True)\n\n    test_audit_path = os.path.join(out_dir, \"test_items_with_leak_flag.jsonl\")\n    write_jsonl(test_audit_path, test_out)\n\n    leaked_only = [r for r in test_out if r[\"is_leaked_into_train\"] == 1]\n    leaked_path = os.path.join(out_dir, \"leaked_test_items_only.jsonl\")\n    write_jsonl(leaked_path, leaked_only)\n\n    nonleaked_only = [r for r in test_out if r[\"is_leaked_into_train\"] == 0]\n    nonleaked_path = os.path.join(out_dir, \"nonleaked_test_items_only.jsonl\")\n    write_jsonl(nonleaked_path, nonleaked_only)\n\n    index_rows.append({\n        \"group\": g,\n        \"leak_tag\": leak_tag,\n        \"test_path\": test_path,\n        \"train_path\": train_path,\n        \"audit_test_path\": test_audit_path,\n        \"audit_leaked_only_path\": leaked_path,\n        \"audit_nonleaked_only_path\": nonleaked_path,\n        \"n_test\": len(test),\n        \"n_train\": len(train),\n        \"n_leaked_test_items\": len(leaked_only),\n        \"n_nonleaked_test_items\": len(nonleaked_only),\n        \"pool_size\": m.get(\"pool_size\"),\n        \"target_test\": m.get(\"target_test\"),\n        \"target_train\": m.get(\"target_train\"),\n        \"n_new\": m.get(\"n_new\"),\n        \"n_leak\": m.get(\"n_leak\"),\n    })\n\nidx_df = pd.DataFrame(index_rows).sort_values([\"group\",\"leak_tag\"])\nidx_csv = os.path.join(OUT_AUDIT, \"audit_index.csv\")\nidx_df.to_csv(idx_csv, index=False)\n\nprint(\"[DONE] Saved audit files under:\", OUT_AUDIT)\nprint(\"[DONE] Index CSV:\", idx_csv)\n# ========== Leak100 item-level alternative: use leak50 nonleaked as generalization baseline ==========\n# goal: for each (group, model):\n#   gen_baseline = delta_nonleaked at leak50 (if exists)\n#   mem_lift_100_est = delta(leak100) - gen_baseline\n\ngen50 = (\n    item_te[(item_te[\"leak\"]==\"leak50\") & (item_te[\"is_leaked_item\"]==0)]\n    .rename(columns={\"delta_mean\":\"delta_nonleaked_leak50\"})\n    [[\"group\",\"model\",\"delta_nonleaked_leak50\"]]\n)\n\ndelta100 = did[did[\"leak\"]==\"leak100\"][[\"group\",\"model\",\"delta\",\"leak_effect\"]].rename(columns={\"delta\":\"delta_leak100\"})\n\nalt = delta100.merge(gen50, on=[\"group\",\"model\"], how=\"left\")\nalt[\"mem_lift100_minus_gen50\"] = alt[\"delta_leak100\"] - alt[\"delta_nonleaked_leak50\"]\n\nprint(\"\\n=== Leak100 alternative item-level attribution ===\")\nprint(alt.sort_values([\"group\",\"model\"])[\n    [\"group\",\"model\",\"delta_leak100\",\"leak_effect\",\"delta_nonleaked_leak50\",\"mem_lift100_minus_gen50\"]\n].to_string(index=False))\n\nalt.to_csv(os.path.join(OUT_ROOT, \"analysis_leak100_alt_itemlevel.csv\"), index=False)\nprint(\"\\nSaved: analysis_leak100_alt_itemlevel.csv\")\n\n\n\nimport os, json, glob\nimport pandas as pd\nimport numpy as np\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nRES_CSV  = os.path.join(OUT_ROOT, \"results_before_after.csv\")\nMAN_JSON = os.path.join(OUT_ROOT, \"splits\", \"split_manifest.json\")\nOUT_CSV  = os.path.join(OUT_ROOT, \"master_results_with_trainloss.csv\")\n\n# -------------------------\n# 0) Load baseline results\n# -------------------------\nif not os.path.exists(RES_CSV):\n    raise FileNotFoundError(f\"Missing {RES_CSV}. Run your experiment first.\")\n\nres = pd.read_csv(RES_CSV)\n\n# Normalize column names if needed\nfor col in [\"group\",\"leak\",\"model\"]:\n    if col not in res.columns:\n        raise KeyError(f\"results_before_after.csv missing required column: {col}\")\n\n# -------------------------\n# 1) Load split manifest -> table\n# -------------------------\nif not os.path.exists(MAN_JSON):\n    raise FileNotFoundError(f\"Missing {MAN_JSON}. Expected splits manifest.\")\n\nmanifest = json.load(open(MAN_JSON, \"r\", encoding=\"utf-8\"))\nman_rows = []\nfor k, m in manifest.items():\n    man_rows.append({\n        \"group\": m[\"group\"],\n        \"leak\": m[\"leak_tag\"],   # align with res[\"leak\"]\n        \"pool_size\": m.get(\"pool_size\"),\n        \"target_test\": m.get(\"target_test\"),\n        \"target_train\": m.get(\"target_train\"),\n        \"n_test_manifest\": m.get(\"n_test\"),\n        \"n_train_manifest\": m.get(\"n_train\"),\n        \"n_new_manifest\": m.get(\"n_new\"),\n        \"n_leak_manifest\": m.get(\"n_leak\"),\n        \"test_path_manifest\": m.get(\"test_path\"),\n        \"train_path_manifest\": m.get(\"train_path\"),\n    })\nman_df = pd.DataFrame(man_rows)\n\n# Merge manifest info\ndf = res.merge(man_df, on=[\"group\",\"leak\"], how=\"left\")\n\n# -------------------------\n# 2) Add DiD leak_effect using leak0 baseline\n# -------------------------\n# We compute per (group, model) baseline delta at leak0\nif \"delta_acc_on_fmt\" in df.columns:\n    df[\"delta_acc\"] = df[\"delta_acc_on_fmt\"]\nelif \"delta_acc_overall\" in df.columns:\n    df[\"delta_acc\"] = df[\"delta_acc_overall\"]\nelse:\n    # fallback: compute from before/after if present\n    if \"before_acc_on_fmt\" in df.columns and \"after_acc_on_fmt\" in df.columns:\n        df[\"delta_acc\"] = df[\"after_acc_on_fmt\"] - df[\"before_acc_on_fmt\"]\n    elif \"before_acc_overall\" in df.columns and \"after_acc_overall\" in df.columns:\n        df[\"delta_acc\"] = df[\"after_acc_overall\"] - df[\"before_acc_overall\"]\n    else:\n        df[\"delta_acc\"] = np.nan\n\nbase = (\n    df[df[\"leak\"]==\"leak0\"][[\"group\",\"model\",\"delta_acc\"]]\n    .rename(columns={\"delta_acc\":\"delta_acc_leak0\"})\n)\ndf = df.merge(base, on=[\"group\",\"model\"], how=\"left\")\ndf[\"did_leak_effect_acc\"] = df[\"delta_acc\"] - df[\"delta_acc_leak0\"]\n\n# -------------------------\n# 3) (Optional) Attach training loss summaries if train_loss.csv exists\n# Expected path: OUT_ROOT/train_logs/<group>/<leak>/<model>/train_loss.csv\n# -------------------------\nloss_paths = glob.glob(os.path.join(OUT_ROOT, \"train_logs\", \"*\", \"*\", \"*\", \"train_loss.csv\"))\n\nloss_rows = []\nfor p in loss_paths:\n    # parse path components\n    # .../train_logs/<group>/<leak>/<model>/train_loss.csv\n    parts = p.split(os.sep)\n    try:\n        i = parts.index(\"train_logs\")\n        g = parts[i+1]\n        leak = parts[i+2]\n        model = parts[i+3]\n    except Exception:\n        continue\n\n    try:\n        tdf = pd.read_csv(p)\n    except Exception:\n        continue\n\n    if \"loss\" not in tdf.columns or len(tdf) == 0:\n        continue\n\n    tdf = tdf.sort_values(\"global_step\") if \"global_step\" in tdf.columns else tdf\n\n    loss_min = float(tdf[\"loss\"].min())\n    loss_last = float(tdf[\"loss\"].iloc[-1])\n    loss_first = float(tdf[\"loss\"].iloc[0])\n    loss_drop = loss_first - loss_last\n\n    # optional: epoch_last\n    epoch_last = float(tdf[\"epoch\"].iloc[-1]) if \"epoch\" in tdf.columns and pd.notna(tdf[\"epoch\"].iloc[-1]) else np.nan\n    steps = int(tdf[\"global_step\"].max()) if \"global_step\" in tdf.columns else np.nan\n\n    loss_rows.append({\n        \"group\": g,\n        \"leak\": leak,\n        \"model\": model,\n        \"train_loss_path\": p,\n        \"train_loss_first\": loss_first,\n        \"train_loss_last\": loss_last,\n        \"train_loss_min\": loss_min,\n        \"train_loss_drop\": loss_drop,\n        \"train_epoch_last\": epoch_last,\n        \"train_global_step_max\": steps,\n        \"train_log_points\": int(len(tdf)),\n    })\n\nloss_df = pd.DataFrame(loss_rows)\n\nif len(loss_df) > 0:\n    df = df.merge(loss_df, on=[\"group\",\"leak\",\"model\"], how=\"left\")\nelse:\n    # create empty columns for consistent schema\n    for c in [\"train_loss_path\",\"train_loss_first\",\"train_loss_last\",\"train_loss_min\",\"train_loss_drop\",\n              \"train_epoch_last\",\"train_global_step_max\",\"train_log_points\"]:\n        df[c] = np.nan\n\n# -------------------------\n# 4) Final ordering + save\n# -------------------------\n# Make leak ordering stable\nleak_order = [\"leak0\",\"leak50\",\"leak100\"]\ndf[\"leak\"] = pd.Categorical(df[\"leak\"], categories=leak_order, ordered=True)\n\n# Put key columns first\nfront = [\n    \"group\",\"leak\",\"model\",\"model_id\",\n    \"n_test\",\"n_train\",\n    \"n_test_manifest\",\"n_train_manifest\",\"n_new_manifest\",\"n_leak_manifest\",\n    \"before_acc_on_fmt\",\"after_acc_on_fmt\",\"delta_acc_on_fmt\",\n    \"before_acc_overall\",\"after_acc_overall\",\"delta_acc_overall\",\n    \"delta_acc\",\"delta_acc_leak0\",\"did_leak_effect_acc\",\n    \"train_loss_first\",\"train_loss_last\",\"train_loss_min\",\"train_loss_drop\",\"train_log_points\",\n    \"train_path\",\"test_path\",\"train_path_manifest\",\"test_path_manifest\",\n    \"adapter_dir\",\"before_raw_path\",\"after_raw_path\",\"train_loss_path\"\n]\nfront = [c for c in front if c in df.columns]\nrest = [c for c in df.columns if c not in front]\ndf = df[front + rest].sort_values([\"group\",\"model\",\"leak\"])\n\ndf.to_csv(OUT_CSV, index=False)\nprint(\"Saved master table:\", OUT_CSV)\nprint(\"Rows:\", len(df))\ndf.head(10)\n",
      "metadata": {
        "id": "SnhiM57XMvs0"
      },
      "id": "SnhiM57XMvs0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import os, json, glob, re, hashlib\nimport numpy as np\nimport pandas as pd\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nMAN_PATH = os.path.join(OUT_ROOT, \"splits\", \"split_manifest.json\")\nRAW_GLOB = os.path.join(OUT_ROOT, \"raw_eval\", \"*\", \"*\", \"*\", \"*.jsonl\")\nRES_CSV  = os.path.join(OUT_ROOT, \"results_before_after.csv\")\n\nOUT_ITEM_CSV   = os.path.join(OUT_ROOT, \"baseline_item_features.csv\")\nOUT_METRIC_CSV = os.path.join(OUT_ROOT, \"baseline_detector_metrics.csv\")\nOUT_RUN_CSV    = os.path.join(OUT_ROOT, \"baseline_runlevel_compare.csv\")\n\n# --------------------------\n# IO helpers\n# --------------------------\ndef read_jsonl(path):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            rows.append(json.loads(line))\n    return rows\n\ndef normalize_text(s: str) -> str:\n    s = (s or \"\").strip().lower()\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\ndef prompt_md5(s: str) -> str:\n    return hashlib.md5(normalize_text(s).encode(\"utf-8\")).hexdigest()\n\ndef item_key_from_splitrow(r):\n    # Prefer task+local_idx; fallback to prompt hash\n    t = r.get(\"task\", None)\n    li = r.get(\"local_idx\", None)\n    if t is not None and li is not None:\n        return (\"task_local\", str(t), int(li))\n    return (\"prompt_md5\", prompt_md5(r.get(\"prompt\",\"\")))\n\ndef item_key_from_rawrow(row, test_idx_map=None):\n    # Prefer task+local_idx; fallback to (task, idx) mapping; fallback to idx only\n    t = row.get(\"task\", None)\n    li = row.get(\"local_idx\", None)\n    if pd.notna(t) and pd.notna(li):\n        return (\"task_local\", str(t), int(li))\n    idx = int(row.get(\"idx\", -1))\n    if test_idx_map is not None and idx in test_idx_map:\n        # map idx -> task_local if possible, else prompt_md5\n        return test_idx_map[idx]\n    return (\"fallback_idx\", idx)\n\n# --------------------------\n# Basic AUC (no sklearn)\n# --------------------------\ndef roc_auc_score(y_true, y_score):\n    \"\"\"\n    Mann-Whitney U / rank-based AUROC.\n    Returns NaN if only one class present.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int)\n    y_score = np.asarray(y_score).astype(float)\n    mask = np.isfinite(y_score)\n    y_true = y_true[mask]\n    y_score = y_score[mask]\n    if len(y_true) == 0:\n        return np.nan\n    n_pos = np.sum(y_true == 1)\n    n_neg = np.sum(y_true == 0)\n    if n_pos == 0 or n_neg == 0:\n        return np.nan\n    # rank scores (average ranks for ties)\n    order = np.argsort(y_score)\n    ranks = np.empty_like(order, dtype=float)\n    ranks[order] = np.arange(1, len(y_score) + 1)\n\n    # tie handling: average ranks for equal scores\n    # group equal scores\n    sorted_scores = y_score[order]\n    i = 0\n    while i < len(sorted_scores):\n        j = i\n        while j + 1 < len(sorted_scores) and sorted_scores[j + 1] == sorted_scores[i]:\n            j += 1\n        if j > i:\n            avg_rank = (i + 1 + j + 1) / 2.0\n            ranks[order[i:j+1]] = avg_rank\n        i = j + 1\n\n    sum_ranks_pos = np.sum(ranks[y_true == 1])\n    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\n# --------------------------\n# N-gram overlap\n# --------------------------\n_word_re = re.compile(r\"[a-z0-9]+\")\n\ndef tokenize_words(s: str):\n    return _word_re.findall((s or \"\").lower())\n\ndef ngram_set(tokens, n):\n    if len(tokens) < n:\n        return set()\n    return set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n\ndef jaccard(a: set, b: set):\n    if not a and not b:\n        return 1.0\n    if not a or not b:\n        return 0.0\n    inter = len(a & b)\n    union = len(a) + len(b) - inter\n    return inter / union if union else 0.0\n\ndef max_jaccard_against_train(test_ng, train_ng_list):\n    # return max Jaccard over all train items\n    best = 0.0\n    for ng in train_ng_list:\n        score = jaccard(test_ng, ng)\n        if score > best:\n            best = score\n            if best >= 0.999999:\n                break\n    return best\n\n# --------------------------\n# 1) Load manifest, build (group, leak) -> test/train rows and leaked set\n# --------------------------\nif not os.path.exists(MAN_PATH):\n    raise FileNotFoundError(f\"Missing {MAN_PATH}\")\n\nmanifest = json.load(open(MAN_PATH, \"r\", encoding=\"utf-8\"))\n\n# For each group, store test rows + idx->key map (to align raw_eval rows)\ngroup_test_rows = {}\ngroup_test_idx_map = {}  # group -> {idx: item_key}\n# For each (group, leak): leaked set + train prompts\nleaked_sets = {}\ntrain_prompts = {}       # (group, leak) -> list[str]\ntest_prompts  = {}       # (group, leak) -> list[str] (same as group test, but keep aligned)\ntest_keys     = {}       # (group, leak) -> list[item_key] aligned to test_rows order\n\nfor k, m in manifest.items():\n    g = m[\"group\"]\n    leak = m[\"leak_tag\"]\n    test = read_jsonl(m[\"test_path\"])\n    train = read_jsonl(m[\"train_path\"])\n\n    # cache group test (same across leaks)\n    if g not in group_test_rows:\n        group_test_rows[g] = test\n        idx_map = {}\n        for i, r in enumerate(test):\n            idx_map[i] = item_key_from_splitrow(r)\n        group_test_idx_map[g] = idx_map\n\n    # compute leaked = test ∩ train\n    test_set = set(item_key_from_splitrow(r) for r in test)\n    train_set = set(item_key_from_splitrow(r) for r in train)\n    leaked = test_set & train_set\n    leaked_sets[(g, leak)] = leaked\n\n    # store prompts aligned to test\n    test_prompts[(g, leak)] = [r.get(\"prompt\",\"\") for r in test]\n    test_keys[(g, leak)] = [item_key_from_splitrow(r) for r in test]\n\n    # store train prompts\n    train_prompts[(g, leak)] = [r.get(\"prompt\",\"\") for r in train]\n\nprint(\"[INFO] Loaded manifest. Groups:\", sorted(group_test_rows.keys()))\n\n# --------------------------\n# 2) Load raw_eval and enrich each row with: prompt, is_leaked_item, label01 (from test split)\n# --------------------------\nraw_paths = glob.glob(RAW_GLOB)\nif not raw_paths:\n    raise FileNotFoundError(f\"No raw_eval found under {RAW_GLOB}\")\n\nraw_rows = []\nfor p in raw_paths:\n    raw_rows.extend(read_jsonl(p))\n\ndf = pd.DataFrame(raw_rows)\n\nrequired_cols = [\"group\",\"leak\",\"model\",\"phase\"]\nfor c in required_cols:\n    if c not in df.columns:\n        raise KeyError(f\"raw_eval missing required column: {c}\")\n\n# numeric correctness fields\ndf[\"solver_correct\"] = pd.to_numeric(df.get(\"solver_correct\"), errors=\"coerce\")\ndf[\"logp_correct\"]   = pd.to_numeric(df.get(\"logp_correct\"), errors=\"coerce\") if \"logp_correct\" in df.columns else np.nan\n\n# attach prompt/label from test split\ndef attach_from_test(row):\n    g = row[\"group\"]\n    leak = row[\"leak\"]\n    idx = int(row.get(\"idx\", -1))\n    # locate test row by idx (most stable in your pipeline)\n    tr = None\n    if g in group_test_rows and 0 <= idx < len(group_test_rows[g]):\n        tr = group_test_rows[g][idx]\n    prompt = tr.get(\"prompt\",\"\") if tr else \"\"\n    label01 = tr.get(\"label01\", None) if tr else None\n\n    # is_leaked depends on leak tag\n    key = item_key_from_rawrow(row, test_idx_map=group_test_idx_map.get(g))\n    leaked = leaked_sets.get((g, leak), set())\n    is_leaked = int(key in leaked) if leaked else 0\n\n    return pd.Series({\n        \"prompt\": prompt,\n        \"label01_from_split\": label01,\n        \"item_key_type\": key[0],\n        \"item_key\": \"::\".join(map(str, key[1:])),\n        \"is_leaked_item\": is_leaked,\n        \"prompt_md5\": prompt_md5(prompt),\n    })\n\nenriched = df.apply(attach_from_test, axis=1)\ndf = pd.concat([df, enriched], axis=1)\n\n# self-report score\n# 3=SEEN, 5=SIMILAR, 7=NOT SEEN -> map to higher = \"more seen\"\nmap_seen = {\"3\": 2.0, \"5\": 1.0, \"7\": 0.0}\ndf[\"seen_score\"] = df.get(\"contam_pred\").map(map_seen).astype(float)\n\n# keep only rows where we have solver_correct (since that's your main evaluation)\ndf_eval = df[df[\"solver_correct\"].notna()].copy()\n\nprint(\"[INFO] raw_eval rows:\", len(df), \"eval rows with solver_correct:\", len(df_eval))\n\n# --------------------------\n# 3) Build n-gram overlap scores per (group, leak) on the TEST split\n#    Score is computed per test item, then merged to df_eval via (group, leak, idx)\n# --------------------------\nNGRAM_N = 5  # you can try 3/4/5\noverlap_rows = []\nfor (g, leak), tr_prompts in train_prompts.items():\n    te_prompts = test_prompts[(g, leak)]\n    te_keys = test_keys[(g, leak)]\n    # precompute train ngram sets\n    train_ng = []\n    train_md5 = set()\n    for p in tr_prompts:\n        norm = normalize_text(p)\n        train_md5.add(prompt_md5(norm))\n        toks = tokenize_words(norm)\n        train_ng.append(ngram_set(toks, NGRAM_N))\n\n    # compute per test item\n    for idx, p in enumerate(te_prompts):\n        norm = normalize_text(p)\n        md5 = prompt_md5(norm)\n        toks = tokenize_words(norm)\n        te_ng = ngram_set(toks, NGRAM_N)\n        max_j = max_jaccard_against_train(te_ng, train_ng) if train_ng else 0.0\n        exact = int(md5 in train_md5)\n        # true leaked label via key intersection (your controlled overlap)\n        key = te_keys[idx]\n        is_leaked = int(key in leaked_sets[(g, leak)])\n        overlap_rows.append({\n            \"group\": g,\n            \"leak\": leak,\n            \"idx\": idx,\n            \"item_key_type\": key[0],\n            \"item_key\": \"::\".join(map(str, key[1:])),\n            \"is_leaked_item\": is_leaked,\n            \"ngram_n\": NGRAM_N,\n            \"ngram_max_jaccard\": float(max_j),\n            \"exact_prompt_match\": exact,\n        })\n\nov = pd.DataFrame(overlap_rows)\nprint(\"[INFO] Computed ngram overlap rows:\", len(ov))\n\n# merge overlap features into df_eval\ndf_eval = df_eval.merge(\n    ov[[\"group\",\"leak\",\"idx\",\"ngram_max_jaccard\",\"exact_prompt_match\"]],\n    on=[\"group\",\"leak\",\"idx\"],\n    how=\"left\"\n)\n\n# --------------------------\n# 4) Evaluate detectors on leak50 only (since leak0 has no positives, leak100 has no negatives)\n#    We compute AUROC for predicting is_leaked_item.\n#    Detectors:\n#      - ngram_max_jaccard\n#      - exact_prompt_match\n#      - seen_score (3/5/7)\n#      - (optional) logp_margin if present\n# --------------------------\ndf50 = df_eval[df_eval[\"leak\"]==\"leak50\"].copy()\n\n# If you saved logp_margin in raw_eval, include it\nhas_logp_margin = \"logp_margin\" in df50.columns\nif has_logp_margin:\n    df50[\"logp_margin\"] = pd.to_numeric(df50[\"logp_margin\"], errors=\"coerce\")\n\nmetric_rows = []\nfor (g, model, phase), sub in df50.groupby([\"group\",\"model\",\"phase\"]):\n    y = sub[\"is_leaked_item\"].astype(int).values\n\n    # ngram detector\n    auc_ng = roc_auc_score(y, sub[\"ngram_max_jaccard\"].values)\n\n    # exact match detector (binary; AUROC defined but may tie-heavy)\n    auc_ex = roc_auc_score(y, sub[\"exact_prompt_match\"].values)\n\n    # self report\n    auc_seen = roc_auc_score(y, sub[\"seen_score\"].values) if sub[\"seen_score\"].notna().any() else np.nan\n\n    row = {\n        \"group\": g,\n        \"model\": model,\n        \"phase\": phase,\n        \"n_items\": int(len(sub)),\n        \"pos_leaked\": int(np.sum(y==1)),\n        \"neg_nonleaked\": int(np.sum(y==0)),\n        \"auc_ngram_max_jaccard\": auc_ng,\n        \"auc_exact_prompt_match\": auc_ex,\n        \"auc_seen_score_357\": auc_seen,\n    }\n\n    if has_logp_margin:\n        row[\"auc_logp_margin\"] = roc_auc_score(y, sub[\"logp_margin\"].values)\n    metric_rows.append(row)\n\nmetrics = pd.DataFrame(metric_rows).sort_values([\"group\",\"model\",\"phase\"])\nmetrics.to_csv(OUT_METRIC_CSV, index=False)\n\n# --------------------------\n# 5) Run-level compare: DiD leak_effect (from results_before_after.csv) vs detectors\n#    - load results_before_after.csv\n#    - compute leak_effect on accuracy (delta - delta(leak0))\n#    - attach detector AUROC (leak50 only) for before/after, for context\n# --------------------------\nif not os.path.exists(RES_CSV):\n    print(\"[WARN] results_before_after.csv not found; run-level DiD compare will be skipped.\")\n    run_cmp = pd.DataFrame()\nelse:\n    res = pd.read_csv(RES_CSV)\n    # choose an accuracy column\n    if \"delta_acc_on_fmt\" in res.columns:\n        res[\"delta_acc\"] = res[\"delta_acc_on_fmt\"]\n    elif \"delta_acc_overall\" in res.columns:\n        res[\"delta_acc\"] = res[\"delta_acc_overall\"]\n    else:\n        # fallback\n        if \"before_acc_on_fmt\" in res.columns and \"after_acc_on_fmt\" in res.columns:\n            res[\"delta_acc\"] = res[\"after_acc_on_fmt\"] - res[\"before_acc_on_fmt\"]\n        elif \"before_acc_overall\" in res.columns and \"after_acc_overall\" in res.columns:\n            res[\"delta_acc\"] = res[\"after_acc_overall\"] - res[\"before_acc_overall\"]\n        else:\n            res[\"delta_acc\"] = np.nan\n\n    base = res[res[\"leak\"]==\"leak0\"][[\"group\",\"model\",\"delta_acc\"]].rename(columns={\"delta_acc\":\"delta_acc_leak0\"})\n    res = res.merge(base, on=[\"group\",\"model\"], how=\"left\")\n    res[\"did_leak_effect_acc\"] = res[\"delta_acc\"] - res[\"delta_acc_leak0\"]\n\n    # attach detector metrics (use leak50 only; for each phase)\n    # merge by (group, model, phase)\n    # (res is run-level without phase; we attach both before/after AUC columns)\n    m_before = metrics[metrics[\"phase\"]==\"before\"].copy()\n    m_after  = metrics[metrics[\"phase\"]==\"after\"].copy()\n\n    def rename_auc(dfm, suffix):\n        cols = [c for c in dfm.columns if c.startswith(\"auc_\")]\n        ren = {c: f\"{c}_{suffix}\" for c in cols}\n        return dfm.rename(columns=ren)[[\"group\",\"model\"] + list(ren.values())]\n\n    res = res.merge(rename_auc(m_before, \"leak50_before\"), on=[\"group\",\"model\"], how=\"left\")\n    res = res.merge(rename_auc(m_after,  \"leak50_after\"),  on=[\"group\",\"model\"], how=\"left\")\n\n    run_cmp = res.sort_values([\"group\",\"model\",\"leak\"])\n    run_cmp.to_csv(OUT_RUN_CSV, index=False)\n\n# --------------------------\n# 6) Save per-item features table (for paper plots / ablations)\n# --------------------------\n# Keep one row per item per run (group, leak, model, phase, idx)\nkeep_cols = [\n    \"group\",\"leak\",\"model\",\"phase\",\"idx\",\n    \"task\",\"local_idx\",\n    \"item_key_type\",\"item_key\",\n    \"is_leaked_item\",\n    \"solver_correct\",\n    \"seen_score\",\n    \"ngram_max_jaccard\",\"exact_prompt_match\",\n    \"prompt_md5\",\"prompt\"\n]\n# include optional logp fields if present\nfor c in [\"logp_margin\",\"logp_pred01\",\"logp_correct\",\"logp_yes\",\"logp_no\"]:\n    if c in df_eval.columns:\n        keep_cols.append(c)\n\nkeep_cols = [c for c in keep_cols if c in df_eval.columns]\nitem_out = df_eval[keep_cols].copy()\nitem_out.to_csv(OUT_ITEM_CSV, index=False)\n\nprint(\"\\n[DONE] Saved:\")\nprint(\"  Per-item features:\", OUT_ITEM_CSV)\nprint(\"  Detector metrics (leak50 AUROC):\", OUT_METRIC_CSV)\nif len(run_cmp) > 0:\n    print(\"  Run-level compare (DiD + detector AUC):\", OUT_RUN_CSV)\nelse:\n    print(\"  Run-level compare skipped (missing results_before_after.csv).\")\n\n# --------------------------\n# 7) Quick textual summary in console\n# --------------------------\nprint(\"\\n=== QUICK SUMMARY: leak50 detector AUROC (higher is better for detecting leaked items) ===\")\nsummary = metrics.groupby([\"group\",\"phase\"], as_index=False).agg(\n    auc_ngram=(\"auc_ngram_max_jaccard\",\"mean\"),\n    auc_exact=(\"auc_exact_prompt_match\",\"mean\"),\n    auc_seen=(\"auc_seen_score_357\",\"mean\"),\n    auc_logp=(\"auc_logp_margin\",\"mean\") if \"auc_logp_margin\" in metrics.columns else (\"auc_seen_score_357\",\"mean\"),\n)\nprint(summary.to_string(index=False))\n\nprint(\"\\nNOTE:\")\nprint(\"- AUROC is computed only on leak50 because leak0 has no positives and leak100 has no negatives.\")\nprint(\"- If logp_margin columns are absent in raw_eval, logp-based detector is skipped automatically.\")\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nRUN_CSV = os.path.join(OUT_ROOT, \"baseline_runlevel_compare.csv\")\n\ndf = pd.read_csv(RUN_CSV)\n\n# pick one row per (group, model): use leak100 row as representative for DiD (effect size)\ndf[\"leak\"] = pd.Categorical(df[\"leak\"], categories=[\"leak100\",\"leak50\",\"leak0\"], ordered=True)\none = (\n    df.sort_values([\"group\",\"model\",\"leak\"])\n      .groupby([\"group\",\"model\"], as_index=False)\n      .first()\n)\n\n# choose AUROC column (ngram after) if exists\nauc_col = \"auc_ngram_max_jaccard_leak50_after\"\nif auc_col not in one.columns:\n    raise KeyError(f\"{auc_col} not found in baseline_runlevel_compare.csv\")\n\nx = pd.to_numeric(one[\"did_leak_effect_acc\"], errors=\"coerce\")\ny = pd.to_numeric(one[auc_col], errors=\"coerce\")\nmask = x.notna() & y.notna()\nx = x[mask].values\ny = y[mask].values\n\ndef safe_corr(x, y):\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    if len(x) < 3:\n        return (np.nan, np.nan, \"too_few_points\")\n    if np.std(x) == 0 or np.std(y) == 0:\n        return (np.nan, np.nan, \"zero_variance\")\n    pearson = float(np.corrcoef(x, y)[0,1])\n\n    # Spearman via ranks (average ranks)\n    rx = pd.Series(x).rank(method=\"average\").values\n    ry = pd.Series(y).rank(method=\"average\").values\n    if np.std(rx) == 0 or np.std(ry) == 0:\n        spearman = np.nan\n    else:\n        spearman = float(np.corrcoef(rx, ry)[0,1])\n    return (pearson, spearman, \"ok\")\n\npearson_all, spearman_all, status_all = safe_corr(x, y)\n\nprint(\"\\n=== Overall correlation (DiD contribution vs AUROC detector) ===\")\nprint(f\"col={auc_col}\")\nprint(f\"n={len(x)}, status={status_all}, pearson={pearson_all}, spearman={spearman_all}\")\n\n# Per-group correlations (skip groups where AUROC constant or too few)\nprint(\"\\n=== Per-group correlations ===\")\nrows = []\nfor g in sorted(one[\"group\"].unique()):\n    sub = one[one[\"group\"]==g]\n    xx = pd.to_numeric(sub[\"did_leak_effect_acc\"], errors=\"coerce\")\n    yy = pd.to_numeric(sub[auc_col], errors=\"coerce\")\n    m = xx.notna() & yy.notna()\n    xx = xx[m].values\n    yy = yy[m].values\n    p, s, st = safe_corr(xx, yy)\n    rows.append({\"group\": g, \"n\": len(xx), \"status\": st, \"pearson\": p, \"spearman\": s,\n                 \"auc_min\": float(np.nanmin(yy)) if len(yy) else np.nan,\n                 \"auc_max\": float(np.nanmax(yy)) if len(yy) else np.nan,\n                 \"did_min\": float(np.nanmin(xx)) if len(xx) else np.nan,\n                 \"did_max\": float(np.nanmax(xx)) if len(xx) else np.nan})\n\nrep = pd.DataFrame(rows)\nprint(rep.to_string(index=False))\n\n# \"oracle detection but zero contribution\" cases + fraction\nthr_auc = 0.99\nthr_did = 0.01\noracle_zero = one[(pd.to_numeric(one[auc_col], errors=\"coerce\") >= thr_auc) &\n                  (pd.to_numeric(one[\"did_leak_effect_acc\"], errors=\"coerce\").abs() <= thr_did)].copy()\n\nprint(f\"\\n=== Oracle-detection but ~zero contribution cases (AUROC>={thr_auc}, |DiD|<={thr_did}) ===\")\nprint(f\"count={len(oracle_zero)} out of total group-model pairs={len(one)} \"\n      f\"({len(oracle_zero)/max(1,len(one))*100:.1f}%)\")\nprint(oracle_zero.sort_values([\"group\",\"model\"])[[\"group\",\"model\",auc_col,\"did_leak_effect_acc\"]].to_string(index=False))\n\n# Save the report\nout_path = os.path.join(OUT_ROOT, \"corr_detection_vs_contribution_clean.csv\")\nrep.to_csv(out_path, index=False)\nprint(\"\\nSaved per-group corr report:\", out_path)\nimport os\nimport numpy as np\nimport pandas as pd\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nRUN_CSV = os.path.join(OUT_ROOT, \"baseline_runlevel_compare.csv\")\n\ndf = pd.read_csv(RUN_CSV)\n\n# One row per (group, model): use leak100 row (effect size). AUROC columns are leak50-based but repeated.\ndf[\"leak\"] = pd.Categorical(df[\"leak\"], categories=[\"leak100\",\"leak50\",\"leak0\"], ordered=True)\none = (\n    df.sort_values([\"group\",\"model\",\"leak\"])\n      .groupby([\"group\",\"model\"], as_index=False)\n      .first()\n)\n\nauc_col = \"auc_ngram_max_jaccard_leak50_after\"\nif auc_col not in one.columns:\n    raise KeyError(f\"{auc_col} not found in baseline_runlevel_compare.csv\")\n\n# Clean numeric\none[\"did_leak_effect_acc\"] = pd.to_numeric(one[\"did_leak_effect_acc\"], errors=\"coerce\")\none[auc_col] = pd.to_numeric(one[auc_col], errors=\"coerce\")\n\n# Oracle-but-zero definition\nTHR_AUC = 0.99\nTHR_DID = 0.01\none[\"oracle_detect\"] = (one[auc_col] >= THR_AUC).astype(int)\none[\"zero_contrib\"] = (one[\"did_leak_effect_acc\"].abs() <= THR_DID).astype(int)\none[\"oracle_but_zero\"] = ((one[\"oracle_detect\"] == 1) & (one[\"zero_contrib\"] == 1)).astype(int)\n\n# Group-level summary: AUROC mean/min/max; DiD min/max; count oracle-but-zero; N models\ngrp = (\n    one.groupby(\"group\", as_index=False)\n       .agg(\n           n_models=(\"model\",\"nunique\"),\n           auc_mean=(auc_col,\"mean\"),\n           auc_min=(auc_col,\"min\"),\n           auc_max=(auc_col,\"max\"),\n           did_min=(\"did_leak_effect_acc\",\"min\"),\n           did_max=(\"did_leak_effect_acc\",\"max\"),\n           oracle_but_zero=(\"oracle_but_zero\",\"sum\"),\n       )\n)\n\n# Optional: show overall row\noverall = pd.DataFrame([{\n    \"group\": \"ALL\",\n    \"n_models\": int(one[\"model\"].nunique() if one[\"group\"].nunique()==1 else len(one)),\n    \"auc_mean\": one[auc_col].mean(),\n    \"auc_min\": one[auc_col].min(),\n    \"auc_max\": one[auc_col].max(),\n    \"did_min\": one[\"did_leak_effect_acc\"].min(),\n    \"did_max\": one[\"did_leak_effect_acc\"].max(),\n    \"oracle_but_zero\": int(one[\"oracle_but_zero\"].sum()),\n}])\n\n# For ALL row, better report number of (group,model) pairs\noverall.loc[0, \"n_models\"] = len(one)\n\ntable = pd.concat([grp, overall], ignore_index=True)\n\n# Formatting helpers\ndef f3(x):\n    if pd.isna(x): return \"--\"\n    return f\"{x:.3f}\"\n\ndef did_range(a, b):\n    if pd.isna(a) or pd.isna(b): return \"--\"\n    return f\"[{a:.3f}, {b:.3f}]\"\n\ndef auc_triplet(mean_, mn, mx):\n    if pd.isna(mean_) or pd.isna(mn) or pd.isna(mx): return \"--\"\n    # show mean (min–max)\n    return f\"{mean_:.3f} ({mn:.3f}--{mx:.3f})\"\n\n# Build LaTeX rows\nrows = []\nfor _, r in table.iterrows():\n    group = r[\"group\"]\n    n = int(r[\"n_models\"])\n    auc_str = auc_triplet(r[\"auc_mean\"], r[\"auc_min\"], r[\"auc_max\"])\n    did_str = did_range(r[\"did_min\"], r[\"did_max\"])\n    obz = int(r[\"oracle_but_zero\"])\n    rows.append((group, n, auc_str, did_str, obz))\n\nlatex_lines = []\nlatex_lines.append(r\"\\begin{table}[t]\")\nlatex_lines.append(r\"\\centering\")\nlatex_lines.append(r\"\\small\")\nlatex_lines.append(r\"\\begin{tabular}{lrrrr}\")\nlatex_lines.append(r\"\\toprule\")\nlatex_lines.append(r\"Stratum & \\#Pairs & AUROC$_{\\text{ngram}}$ (mean [min--max]) & DiD Range & Oracle$\\wedge$Zero \\\\\")\nlatex_lines.append(r\"\\midrule\")\n\nfor group, n, auc_str, did_str, obz in rows:\n    # escape underscores if any\n    g = str(group).replace(\"_\", r\"\\_\")\n    latex_lines.append(f\"{g} & {n} & {auc_str} & {did_str} & {obz} \\\\\\\\\")\nlatex_lines.append(r\"\\bottomrule\")\nlatex_lines.append(r\"\\end{tabular}\")\nlatex_lines.append(r\"\\caption{Detection vs. attribution. AUROC measures how well lexical overlap detects leaked items (computed on the 50\\% overlap setting). DiD range reports the min/max causal leakage effect on accuracy across models (computed using 100\\% vs 0\\% overlap). Oracle$\\wedge$Zero counts cases with AUROC$\\geq$0.99 but $|$DiD$|\\leq$0.01, illustrating detection does not imply contribution.}\")\nlatex_lines.append(r\"\\label{tab:detection_vs_attribution}\")\nlatex_lines.append(r\"\\end{table}\")\n\nlatex = \"\\n\".join(latex_lines)\n\nout_tex = os.path.join(OUT_ROOT, \"latex_detection_vs_attribution_table.tex\")\nwith open(out_tex, \"w\", encoding=\"utf-8\") as f:\n    f.write(latex)\n\nprint(latex)\nprint(\"\\nSaved LaTeX table to:\", out_tex)\n\n# ============================\n# Anchor Certificate (Mainline-2 Only)\n# - No interventions, no injection experiments\n# - Compute p(4) on anchor stratum (default: group=\"sgu\")\n# - Certify leakage lower bound LB_E(u)=max(0,p-u) and FracLeakLB=LB_E/p\n# - Sensitivity over u in [U_MIN, U_MAX]\n# - Optional stratification by self-report S in {3,5,7} (contam_pred)\n# ============================\n\n",
      "metadata": {
        "id": "vJpOP4PNJOtP"
      },
      "id": "vJpOP4PNJOtP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nimport pandas as pd\n\nOUT_ROOT = \"/content/drive/MyDrive/complexity7/eval_outputs_leak\"\nRES_CSV  = os.path.join(OUT_ROOT, \"results_before_after.csv\")\n\nANCHOR_GROUP = \"sgu\"\nLEAK_USE  = \"leak0\"\nPHASE_USE = \"before\"\n\nU_MIN = 0.50\nU_MAX = 0.85\nU_STAR = 0.68\n\nOUT_CERT_CSV = os.path.join(OUT_ROOT, \"anchor_certificates_simple.csv\")\nOUT_CERT_TXT = os.path.join(OUT_ROOT, \"anchor_certificates_simple.txt\")\n\nif not os.path.exists(RES_CSV):\n    raise FileNotFoundError(f\"Missing {RES_CSV}\")\n\ndf = pd.read_csv(RES_CSV)\n\ndef pick_p_col(phase):\n    for c in [f\"{phase}_acc_overall\", f\"{phase}_acc_on_fmt\"]:\n        if c in df.columns:\n            return c\n    raise KeyError(f\"Neither {phase}_acc_overall nor {phase}_acc_on_fmt found.\")\n\np_col = pick_p_col(PHASE_USE)\n\nsub = df[(df[\"group\"] == ANCHOR_GROUP) & (df[\"leak\"] == LEAK_USE)].copy()\nif sub.empty:\n    raise ValueError(f\"No rows for group={ANCHOR_GROUP}, leak={LEAK_USE}\")\n\nsub[p_col] = pd.to_numeric(sub[p_col], errors=\"coerce\")\nsub = sub.dropna(subset=[p_col])\n\ndef lb_e(p, u):\n    return max(0.0, float(p) - float(u))\n\nrows = []\nfor _, r in sub.iterrows():\n    model = r[\"model\"]\n    p = float(r[p_col])\n    rows.append({\n        \"group\": ANCHOR_GROUP,\n        \"leak\": LEAK_USE,\n        \"phase\": PHASE_USE,\n        \"model\": model,\n        \"p4\": p,\n        \"u_min\": U_MIN,\n        \"u_max\": U_MAX,\n        \"LB_E_at_u_min\": lb_e(p, U_MIN),\n        \"LB_E_at_u_max\": lb_e(p, U_MAX),\n        \"FracLeakLB_at_u_min\": (lb_e(p, U_MIN)/p) if p>0 else np.nan,\n        \"FracLeakLB_at_u_max\": (lb_e(p, U_MAX)/p) if p>0 else np.nan,\n        \"u_cert_max\": p\n    })\n\ncert = pd.DataFrame(rows).sort_values(\"model\").reset_index(drop=True)\n\n# mean row\np_bar = float(cert[\"p4\"].mean())\nmean_row = {\n    \"group\": ANCHOR_GROUP,\n    \"leak\": LEAK_USE,\n    \"phase\": PHASE_USE,\n    \"model\": \"__MEAN__\",\n    \"p4\": p_bar,\n    \"u_min\": U_MIN,\n    \"u_max\": U_MAX,\n    \"LB_E_at_u_min\": lb_e(p_bar, U_MIN),\n    \"LB_E_at_u_max\": lb_e(p_bar, U_MAX),\n    \"FracLeakLB_at_u_min\": (lb_e(p_bar, U_MIN)/p_bar) if p_bar>0 else np.nan,\n    \"FracLeakLB_at_u_max\": (lb_e(p_bar, U_MAX)/p_bar) if p_bar>0 else np.nan,\n    \"u_cert_max\": p_bar\n}\ncert = pd.concat([cert, pd.DataFrame([mean_row])], ignore_index=True)\n\n# save CSV\ncert.to_csv(OUT_CERT_CSV, index=False)\n\n# save a plain-text report\nlines = []\nlines.append(f\"ANCHOR CERTIFICATE (no-JS mode)\")\nlines.append(f\"anchor_group={ANCHOR_GROUP}, leak={LEAK_USE}, phase={PHASE_USE}\")\nlines.append(f\"p_col={p_col}\")\nlines.append(f\"u_range=[{U_MIN},{U_MAX}], u_star={U_STAR}\")\nlines.append(\"\")\nlines.append(\"TABLE:\")\nlines.append(cert.to_string(index=False))\nlines.append(\"\")\nlines.append(\"CERTIFICATE STATEMENTS:\")\nfor _, r in cert.iterrows():\n    if r[\"model\"] == \"__MEAN__\":\n        continue\n    p = float(r[\"p4\"])\n    lb = lb_e(p, U_STAR)\n    frac = (lb/p) if p>0 else np.nan\n    lines.append(\n        f\"[CERT] model={r['model']}: p(4)={p:.3f}. \"\n        f\"For u <= {U_STAR:.2f}, certify Pr(E=1|C=4) >= {lb:.3f} \"\n        f\"(FracLeakLB >= {frac:.3f}).\"\n    )\nlb_mean = lb_e(p_bar, U_STAR)\nfrac_mean = (lb_mean/p_bar) if p_bar>0 else np.nan\nlines.append(\n    f\"[CERT] __MEAN__: p(4)={p_bar:.3f}. For u <= {U_STAR:.2f}, \"\n    f\"certify Pr(E=1|C=4) >= {lb_mean:.3f} (FracLeakLB >= {frac_mean:.3f}).\"\n)\n\nos.makedirs(OUT_ROOT, exist_ok=True)\nwith open(OUT_CERT_TXT, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines))\n\nprint(\"Saved CSV :\", OUT_CERT_CSV)\nprint(\"Saved TXT :\", OUT_CERT_TXT)\nprint(\"\\n(If Colab output is broken, open the TXT in Drive to view the table.)\")\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nU_GRID = np.linspace(U_MIN, U_MAX, 71)\n\ndef certify_curve(p):\n    lb = np.maximum(0.0, p - U_GRID)\n    frac = (lb / p) if p > 0 else np.full_like(lb, np.nan)\n    return lb, frac\n\nplt.figure()\nfor _, r in cert.iterrows():\n    if r[\"model\"] == \"__MEAN__\":\n        continue\n    lb, _ = certify_curve(float(r[\"p4\"]))\n    plt.plot(U_GRID, lb, label=r[\"model\"])\nplt.xlabel(\"u\"); plt.ylabel(\"LB_E(u)\")\nplt.title(f\"Anchor LB_E(u) on {ANCHOR_GROUP} ({LEAK_USE}/{PHASE_USE})\")\nplt.legend(fontsize=7)\n\npng1 = os.path.join(OUT_ROOT, \"anchor_LB_E_curve.png\")\nplt.savefig(png1, dpi=200, bbox_inches=\"tight\")\nplt.close()\n\nplt.figure()\nfor _, r in cert.iterrows():\n    if r[\"model\"] == \"__MEAN__\":\n        continue\n    _, frac = certify_curve(float(r[\"p4\"]))\n    plt.plot(U_GRID, frac, label=r[\"model\"])\nplt.xlabel(\"u\"); plt.ylabel(\"FracLeakLB(u)\")\nplt.title(f\"Anchor FracLeakLB(u) on {ANCHOR_GROUP} ({LEAK_USE}/{PHASE_USE})\")\nplt.legend(fontsize=7)\n\npng2 = os.path.join(OUT_ROOT, \"anchor_FracLeakLB_curve.png\")\nplt.savefig(png2, dpi=200, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"Saved PNG:\", png1)\nprint(\"Saved PNG:\", png2)",
      "metadata": {
        "id": "IEmYuKwPZEth"
      },
      "id": "IEmYuKwPZEth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dab49312",
      "metadata": {
        "id": "dab49312"
      },
      "source": [
        "\n",
        "## 1) Global Config: paths, models, datasets\n",
        "This cell is the only one you should edit first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e059656e",
      "metadata": {
        "id": "e059656e"
      },
      "outputs": [],
      "source": "\n# -----------------------------\n# Paths: support your Drive layout + local fallback\n# -----------------------------\nBASE_DIR = os.environ.get(\"BAP_BASE_DIR\", \"/content/drive/MyDrive/complexity_data6\")\nHARMONIZED_PATH = os.environ.get(\"BAP_HARMONIZED_JSONL\", os.path.join(BASE_DIR, \"all_tasks_harmonized.jsonl\"))\nRESULTS_DIR = os.environ.get(\"BAP_RESULTS_DIR\", os.path.join(BASE_DIR, \"bap_runs_full_coverage\"))\n\nos.makedirs(RESULTS_DIR, exist_ok=True)\nprint(\"HARMONIZED_PATH:\", HARMONIZED_PATH)\nprint(\"RESULTS_DIR:\", RESULTS_DIR)\n\n# -----------------------------\n# Models: all models you used (from your v10/v12)\n# -----------------------------\nHF_MODELS = [\n    \"Qwen/Qwen3-4B\",\n    \"Qwen/Qwen2.5-Math-7B-Instruct\",\n    \"deepseek-ai/deepseek-math-7b-instruct\",\n    \"mistralai/Mathstral-7B-v0.1\",\n    \"nvidia/AceMath-7B-Instruct\",\n]\n\n# -----------------------------\n# Datasets: all public ones you used + your JSONL\n# -----------------------------\nRUN_HARMONIZED_JSONL = True\nRUN_GSM8K_ANSWER = True\nRUN_AQUA_MC = True\nRUN_ARC_CHALLENGE_MC = True\n\n# For contamination baselines (optional)\nRUN_BASELINE_CORPORA = True\n\n# -----------------------------\n# Evaluation budget and seeds\n# -----------------------------\nN_EVAL = int(os.environ.get(\"BAP_N_EVAL\", \"400\"))     # per (dataset, model, run)\nWITH_REPLACEMENT = True\nEVAL_SEEDS = [101, 202, 303]  # replicates\n\n# Public randomness beacon for binding seeds (anti-cherry-pick)\nPUBLIC_BEACON = os.environ.get(\"BAP_PUBLIC_BEACON\", \"CHANGE_ME_TO_PUBLIC_RANDOMNESS_BEACON\")\n\n# Generation configs (kept in config digest)\nGEN_CFG_BINARY = dict(max_new_tokens=1, do_sample=False, temperature=0.0)\nGEN_CFG_ANSWER = dict(max_new_tokens=256, do_sample=False, temperature=0.0)\nGEN_CFG_MC = dict(max_new_tokens=8, do_sample=False, temperature=0.0)\n\n\n# For the 3-question protocol (asked in separate fresh sessions)\nGEN_CFG_SEEN = dict(max_new_tokens=4, do_sample=False, temperature=0.0)\nGEN_CFG_LABEL = dict(max_new_tokens=16, do_sample=False, temperature=0.0)\n# -----------------------------\n# Harmonized task selection\n# -----------------------------\n# If TASKS=None => run ALL tasks found in JSONL.\nTASKS: Optional[List[str]] = None\n\n# Optional SGU slice (from your v12 notebook)\nINCLUDE_SGU_SLICE = True\nSGU_SUFFIX = \"__sgu\"\nSGU_COMPLEXITY_FAMILY_PATTERNS = [\n    \"strongly_generically_undecidable\",\n    \"strongly generically undecidable\",\n    \"sgu\",\n    \"undecidable\",\n]\n\nprint(\"HF_MODELS:\", HF_MODELS)\n"
    },
    {
      "cell_type": "markdown",
      "id": "50893fc7",
      "metadata": {
        "id": "50893fc7"
      },
      "source": [
        "\n",
        "## 2) Utilities: hashing, deterministic seed derivation, sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51413f0",
      "metadata": {
        "id": "a51413f0"
      },
      "outputs": [],
      "source": "\ndef sha256_bytes(x: bytes) -> str:\n    return hashlib.sha256(x).hexdigest()\n\ndef sha256_json(obj: Any) -> str:\n    return sha256_bytes(json.dumps(obj, sort_keys=True, default=str).encode(\"utf-8\"))\n\ndef derive_seed(beacon: str, dataset_digest: str, model_digest: str, config_digest: str, run_nonce: str) -> int:\n    msg = f\"{beacon}|{dataset_digest}|{model_digest}|{config_digest}|{run_nonce}\".encode(\"utf-8\")\n    return int(sha256_bytes(msg)[:16], 16)  # 64-bit from prefix\n\ndef sample_indices(seed: int, N: int, n: int, with_replacement: bool = True) -> np.ndarray:\n    rng = np.random.default_rng(seed)\n    if with_replacement:\n        return rng.integers(0, N, size=n, endpoint=False)\n    n = min(n, N)\n    return rng.choice(N, size=n, replace=False)\n\ndef indices_commitment(indices: np.ndarray) -> str:\n    return sha256_bytes(indices.tobytes())\n\ndef safe_name(s: str) -> str:\n    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s)\n"
    },
    {
      "cell_type": "markdown",
      "id": "d76bafab",
      "metadata": {
        "id": "d76bafab"
      },
      "source": [
        "\n",
        "## 3) Task Dataset Format (unified)\n",
        "We unify:\n",
        "- harmonized JSONL **binary** tasks,\n",
        "- GSM8K **answer** tasks,\n",
        "- AQUA / ARC **multi-choice** tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d8e897",
      "metadata": {
        "id": "d4d8e897"
      },
      "outputs": [],
      "source": "\n@dataclass(frozen=True)\nclass TaskInstance:\n    instance_id: str\n    task_type: str      # \"binary\" | \"answer\" | \"mc\"\n    prompt: str\n    # labels:\n    label01: Optional[int] = None\n    ground_truth: Optional[str] = None   # answer string (GSM8K)\n    mc_answer: Optional[str] = None      # option key like \"A\"/\"B\"/\"C\"/\"D\" or \"E\"\n    mc_choices: Optional[List[str]] = None\n    meta: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass TaskDataset:\n    name: str\n    instances: List[TaskInstance]\n"
    },
    {
      "cell_type": "markdown",
      "id": "1f0c06f9",
      "metadata": {
        "id": "1f0c06f9"
      },
      "source": [
        "\n",
        "## 4) Load Your Harmonized JSONL (ALL tasks)\n",
        "This loader matches your v10/v12 schema:\n",
        "- required: `input`, `label`, `task`\n",
        "- optional: `tier`, `complexity_family`, `case_type`, `enum_regime`, `n_enum`, `group_id`, `within`, `is_base`, …\n",
        "\n",
        "We also support an optional `__sgu` slice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0554ad",
      "metadata": {
        "id": "cd0554ad"
      },
      "outputs": [],
      "source": "\ndef load_harmonized_jsonl_df(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Missing harmonized JSONL: {path}\")\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                rows.append(json.loads(line))\n    df = pd.DataFrame(rows)\n\n    # normalize label to int {0,1}\n    def _to01(x):\n        if isinstance(x, (bool, np.bool_)):\n            return int(x)\n        if isinstance(x, (int, np.integer, float, np.floating)):\n            return int(x)\n        s = str(x).strip().lower()\n        if s in [\"1\",\"true\",\"yes\",\"y\",\"t\"]:\n            return 1\n        if s in [\"0\",\"false\",\"no\",\"n\",\"f\"]:\n            return 0\n        m = re.search(r\"[01]\", s)\n        return int(m.group(0)) if m else 0\n\n    if \"label\" in df.columns:\n        df[\"label\"] = df[\"label\"].apply(_to01).astype(int)\n\n    # fill optional cols\n    for col in [\"complexity_family\", \"case_type\", \"enum_regime\", \"n_enum\", \"tier\", \"group_id\", \"within\", \"is_base\", \"source\"]:\n        if col not in df.columns:\n            df[col] = np.nan\n\n    # required cols check\n    for req in [\"input\", \"label\", \"task\"]:\n        if req not in df.columns:\n            raise ValueError(f\"Harmonized JSONL missing required field: {req}\")\n\n    return df\n\ndef build_taskdatasets_from_harmonized(df: pd.DataFrame,\n                                      tasks: Optional[List[str]] = None,\n                                      include_sgu_slice: bool = True) -> Dict[str, TaskDataset]:\n    if tasks is None:\n        tasks = sorted(df[\"task\"].dropna().unique().tolist())\n\n    out: Dict[str, TaskDataset] = {}\n\n    def _make(inst_df: pd.DataFrame, name: str):\n        instances = []\n        for i, r in inst_df.reset_index(drop=True).iterrows():\n            meta = {k: r[k] for k in inst_df.columns if k not in [\"input\",\"label\",\"task\"]}\n            instances.append(TaskInstance(\n                instance_id=f\"{name}-{i}\",\n                task_type=\"binary\",\n                prompt=str(r[\"input\"]),\n                label01=int(r[\"label\"]),\n                meta=meta\n            ))\n        out[name] = TaskDataset(name=name, instances=instances)\n\n    for t in tasks:\n        df_t = df[df[\"task\"] == t].copy()\n        if len(df_t) == 0:\n            continue\n        _make(df_t, t)\n\n        if include_sgu_slice and \"complexity_family\" in df_t.columns:\n            cf = df_t[\"complexity_family\"].fillna(\"\").astype(str).str.lower()\n            mask = np.zeros(len(df_t), dtype=bool)\n            for pat in SGU_COMPLEXITY_FAMILY_PATTERNS:\n                mask = mask | cf.str.contains(pat.lower(), na=False).to_numpy()\n            if mask.any():\n                _make(df_t.loc[mask].copy(), t + SGU_SUFFIX)\n\n    return out\n\nharmonized_tasks: Dict[str, TaskDataset] = {}\nif RUN_HARMONIZED_JSONL:\n    df_all = load_harmonized_jsonl_df(HARMONIZED_PATH)\n    harmonized_tasks = build_taskdatasets_from_harmonized(df_all, tasks=TASKS, include_sgu_slice=INCLUDE_SGU_SLICE)\n    print(\"Harmonized tasks loaded:\", len(harmonized_tasks))\n    print(\"Example tasks:\", list(harmonized_tasks.keys())[:10])\n"
    },
    {
      "cell_type": "markdown",
      "id": "ba461283",
      "metadata": {
        "id": "ba461283"
      },
      "source": [
        "\n",
        "## 5) Load Public Benchmarks (HF)\n",
        "We load:\n",
        "- GSM8K (answer-mode)\n",
        "- AQUA-RAT (multi-choice)\n",
        "- ARC-Challenge (multi-choice)\n",
        "\n",
        "These are *in addition to* your harmonized binary versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6783021d",
      "metadata": {
        "id": "6783021d"
      },
      "outputs": [],
      "source": "\nfrom typing import Iterable\n\ndef load_gsm8k_answer(split: str = \"test\") -> TaskDataset:\n    from datasets import load_dataset\n    ds = load_dataset(\"gsm8k\", \"main\", split=split)\n    inst = []\n    for i, row in enumerate(ds):\n        inst.append(TaskInstance(\n            instance_id=f\"gsm8k-{split}-{i}\",\n            task_type=\"answer\",\n            prompt=row[\"question\"].strip(),\n            ground_truth=row[\"answer\"],\n            meta={\"source\": \"gsm8k\", \"split\": split}\n        ))\n    return TaskDataset(name=f\"gsm8k_answer_{split}\", instances=inst)\n\ndef load_aqua_mc(split: str = \"test\") -> TaskDataset:\n    from datasets import load_dataset\n    ds = load_dataset(\"aqua_rat\", split=split)\n    inst = []\n    for i, row in enumerate(ds):\n        q = row.get(\"question\", \"\").strip()\n        options = row.get(\"options\", [])\n        correct = row.get(\"correct\", None)  # typically \"A\"/\"B\"/\"C\"/\"D\"/\"E\"\n        prompt = \"Choose the correct option (A/B/C/D/E). Return only the letter.\\n\\n\"\n        prompt += f\"Question:\\n{q}\\n\\nOptions:\\n\"\n        for j, opt in enumerate(options):\n            letter = chr(ord(\"A\") + j)\n            prompt += f\"{letter}) {opt}\\n\"\n        prompt += \"\\nAnswer:\"\n        inst.append(TaskInstance(\n            instance_id=f\"aqua-{split}-{i}\",\n            task_type=\"mc\",\n            prompt=prompt,\n            mc_answer=str(correct).strip() if correct is not None else None,\n            mc_choices=options,\n            meta={\"source\": \"aqua_rat\", \"split\": split}\n        ))\n    return TaskDataset(name=f\"aqua_mc_{split}\", instances=inst)\n\ndef load_arc_challenge_mc(split: str = \"test\") -> TaskDataset:\n    from datasets import load_dataset\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=split)\n    inst = []\n    for i, row in enumerate(ds):\n        q = row[\"question\"]\n        stem = q.get(\"stem\",\"\").strip()\n        choices = q.get(\"choices\", [])\n        # ARC uses answerKey like \"A\"/\"B\"/\"C\"/\"D\"\n        correct = row.get(\"answerKey\", None)\n        prompt = \"Choose the correct option (A/B/C/D). Return only the letter.\\n\\n\"\n        prompt += f\"Question:\\n{stem}\\n\\nOptions:\\n\"\n        for ch in choices:\n            prompt += f\"{ch.get('label')}) {ch.get('text')}\\n\"\n        prompt += \"\\nAnswer:\"\n        inst.append(TaskInstance(\n            instance_id=f\"arc-challenge-{split}-{i}\",\n            task_type=\"mc\",\n            prompt=prompt,\n            mc_answer=str(correct).strip() if correct is not None else None,\n            mc_choices=[c.get(\"text\") for c in choices],\n            meta={\"source\": \"ai2_arc\", \"subset\": \"ARC-Challenge\", \"split\": split}\n        ))\n    return TaskDataset(name=f\"arc_challenge_mc_{split}\", instances=inst)\n\npublic_datasets: Dict[str, TaskDataset] = {}\n\ntry:\n    if RUN_GSM8K_ANSWER:\n        public_datasets[\"gsm8k_answer_test\"] = load_gsm8k_answer(\"test\")\n        print(\"Loaded gsm8k_answer_test:\", len(public_datasets[\"gsm8k_answer_test\"].instances))\nexcept Exception as e:\n    print(\"Failed to load GSM8K (need internet in runtime):\", e)\n\ntry:\n    if RUN_AQUA_MC:\n        public_datasets[\"aqua_mc_test\"] = load_aqua_mc(\"test\")\n        print(\"Loaded aqua_mc_test:\", len(public_datasets[\"aqua_mc_test\"].instances))\nexcept Exception as e:\n    print(\"Failed to load AQUA-RAT:\", e)\n\ntry:\n    if RUN_ARC_CHALLENGE_MC:\n        public_datasets[\"arc_challenge_mc_test\"] = load_arc_challenge_mc(\"test\")\n        print(\"Loaded arc_challenge_mc_test:\", len(public_datasets[\"arc_challenge_mc_test\"].instances))\nexcept Exception as e:\n    print(\"Failed to load ARC-Challenge:\", e)\n"
    },
    {
      "cell_type": "markdown",
      "id": "b19aea46",
      "metadata": {
        "id": "b19aea46"
      },
      "source": [
        "\n",
        "## 6) Verifiers (bit + verifier-cost complexity)\n",
        "Complexity is defined as **verifier computation cost** (step-count proxy).\n",
        "We bin costs coarsely before releasing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3c467c",
      "metadata": {
        "id": "4c3c467c"
      },
      "outputs": [],
      "source": "\n# Cost bins: coarse disclosure to reduce leakage via timing/covert channels\nCOST_BINS = [0, 40, 80, 140, 220, 320, 500, 800, 1200, 2000]\n\ndef cost_bin(steps: int) -> int:\n    return int(np.digitize([steps], COST_BINS, right=False)[0] - 1)\n\n_FINAL_ANS_RE = re.compile(r\"####\\s*([-+]?[\\d\\.,]+)\")\n_LAST_NUM_RE = re.compile(r\"([-+]?[\\d\\.,]+)\\s*$\")\n\ndef normalize_number_string(s: str) -> str:\n    return s.strip().replace(\",\", \"\")\n\ndef extract_gsm8k_final(text: str) -> Optional[str]:\n    m = _FINAL_ANS_RE.search(text)\n    if m:\n        return normalize_number_string(m.group(1))\n    m2 = _LAST_NUM_RE.search(text.strip())\n    if m2:\n        return normalize_number_string(m2.group(1))\n    return None\n\ndef extract_model_final(text: str) -> Optional[str]:\n    m = _FINAL_ANS_RE.search(text)\n    if m:\n        return normalize_number_string(m.group(1))\n    nums = re.findall(r\"[-+]?\\d[\\d,]*\\.?\\d*\", text.replace(\",\", \"\"))\n    if nums:\n        return normalize_number_string(nums[-1])\n    return None\n\ndef score_and_cost(inst: TaskInstance, model_out: str) -> Tuple[int, int]:\n    # deterministic scoring + cost proxy\n    out = (model_out or \"\").strip()\n    steps = 0\n\n    if inst.task_type == \"binary\":\n        steps += 5 + len(out)//4\n        tok = None\n        for ch in out:\n            if ch in [\"0\",\"1\"]:\n                tok = int(ch); break\n        steps += 10\n        if tok is None:\n            return 0, steps + 10\n        return int(tok == int(inst.label01)), steps\n\n    if inst.task_type == \"answer\":\n        gt = extract_gsm8k_final(inst.ground_truth or \"\")\n        steps += 20 + len(inst.ground_truth or \"\")//5\n        pred = extract_model_final(out)\n        steps += 30 + len(out)//5\n        if gt is None or pred is None:\n            return 0, steps + 20\n        return int(gt == pred), steps\n\n    if inst.task_type == \"mc\":\n        # parse first A-E\n        steps += 8 + len(out)//4\n        m = re.search(r\"[A-E]\", out.upper())\n        steps += 12\n        if m is None or inst.mc_answer is None:\n            return 0, steps + 10\n        pred = m.group(0)\n        return int(pred == inst.mc_answer.upper().strip()), steps\n\n    raise ValueError(f\"Unknown task_type: {inst.task_type}\")\n"
    },
    {
      "cell_type": "markdown",
      "id": "e9bf19f1",
      "metadata": {
        "id": "e9bf19f1"
      },
      "source": [
        "\n",
        "## 7) Real HF Models: wrapper + lazy loading\n",
        "We keep memory manageable by allowing **lazy load per model**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45c643a0",
      "metadata": {
        "id": "45c643a0"
      },
      "outputs": [],
      "source": "\nDRY_RUN = bool(int(os.environ.get(\"BAP_DRY_RUN\", \"0\")))  # set 1 to test pipeline without model downloads\nUSE_4BIT = bool(int(os.environ.get(\"BAP_4BIT\", \"0\")))    # 4-bit loading if supported\n\nDEVICE = \"cuda\" if (os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\") != \"\" or os.environ.get(\"BAP_FORCE_CUDA\",\"0\")==\"1\") else \"cpu\"\n\nprint(\"DRY_RUN:\", DRY_RUN, \"| USE_4BIT:\", USE_4BIT, \"| DEVICE:\", DEVICE)\n\n# Label set for dataset identification question (Q2)\nDATASET_LABELS = [\n    \"gsm8k\",\n    \"sgu\",\n    \"np_hard\",\n    \"easy\",\n    \"aqua_mc_test\",\n    \"arc_challenge_mc_test\",\n    \"unknown\",\n]\n\n@dataclass\nclass HFModelWrapper:\n    model_id: str\n    tokenizer: Any\n    model: Any\n\n    @staticmethod\n    def load(model_id: str, adapter_path: Optional[str] = None, use_4bit: bool = False):\n        import torch\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n\n        tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n        if tok.pad_token is None:\n            tok.pad_token = tok.eos_token\n\n        kwargs = {}\n        if use_4bit:\n            kwargs.update(dict(load_in_4bit=True, device_map=\"auto\"))\n        else:\n            kwargs.update(dict(device_map=\"auto\"))\n        model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n        model.eval()\n        if adapter_path is not None:\n            from peft import PeftModel\n            model = PeftModel.from_pretrained(model, adapter_path)\n            model.eval()\n        return HFModelWrapper(model_id=model_id, tokenizer=tok, model=model)\n\n    def _generate(self, prompt: str, gen_cfg: Dict[str, Any]) -> str:\n        import torch\n        from transformers import GenerationConfig\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        gcfg = GenerationConfig(**gen_cfg)\n        with torch.no_grad():\n            out = self.model.generate(**inputs, generation_config=gcfg)\n        text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n        if text.startswith(prompt):\n            return text[len(prompt):].strip()\n        return text.strip()\n    def ask_seen_level(self, question: str) -> str:\n        \"\"\"Q1: Ask whether the model has seen the problem (0/1/2). One-shot prompt; no chat history.\"\"\"\n        prompt = (\n            \"You will see a problem. Output ONLY a single digit: 0, 1, or 2.\\n\\n\"\n            \"2 = I have seen this exact problem before.\\n\"\n            \"1 = I have not seen this exact problem, but I have seen very similar problems.\\n\"\n            \"0 = I have never seen this problem and have not seen similar ones.\\n\\n\"\n            \"Do NOT output any other text.\\n\\n\"\n            f\"Problem:\\n{question.strip()}\\n\"\n        )\n        return self._generate(prompt, GEN_CFG_SEEN)\n\n    def ask_dataset_label(self, question: str) -> str:\n        \"\"\"Q2: Predict which dataset/complexity bucket the problem belongs to. One-shot prompt.\"\"\"\n        labels = \"\\n\".join(DATASET_LABELS)\n        prompt = (\n            \"You will see a problem. Output ONLY one label from the list below, EXACTLY as written:\\n\\n\"\n            f\"{labels}\\n\\n\"\n            \"Do NOT output any other text.\\n\\n\"\n            f\"Problem:\\n{question.strip()}\\n\"\n        )\n        return self._generate(prompt, GEN_CFG_LABEL)\n    def generate(self, inst: TaskInstance) -> str:\n        if inst.task_type == \"binary\":\n            prompt = (\n                \"Answer the following decision problem.\\n\"\n                \"Return ONLY a single token:\\n\"\n                \"1 if YES, 0 if NO.\\n\\n\"\n                f\"Problem:\\n{inst.prompt}\\n\\nReturn only a single token (0 or 1):\"\n            )\n            return self._generate(prompt, GEN_CFG_BINARY)\n        if inst.task_type == \"answer\":\n            prompt = (\n                \"Solve the following math problem. Return ONLY the final numeric answer.\\n\\n\"\n                f\"Problem:\\n{inst.prompt}\\n\\nFinal answer:\"\n            )\n            return self._generate(prompt, GEN_CFG_ANSWER)\n        if inst.task_type == \"mc\":\n            # prompt already contains instruction + choices\n            return self._generate(inst.prompt, GEN_CFG_MC)\n        raise ValueError(inst.task_type)\n\ndef digest_model_public(model_id: str) -> str:\n    return sha256_json({\"model_id\": model_id})\n\n\ndef _sha256_file(path: str) -> str:\n    h = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\ndef digest_model_state(model_id: str, adapter_path: Optional[str] = None) -> str:\n    \"\"\"Digest for base model + optional LoRA adapter directory (if provided).\"\"\"\n    if adapter_path is None:\n        return digest_model_public(model_id)\n    adapter_path = os.path.abspath(adapter_path)\n    cfg = os.path.join(adapter_path, \"adapter_config.json\")\n    # peft saves weights as adapter_model.safetensors or adapter_model.bin\n    w1 = os.path.join(adapter_path, \"adapter_model.safetensors\")\n    w2 = os.path.join(adapter_path, \"adapter_model.bin\")\n    weights = w1 if os.path.exists(w1) else (w2 if os.path.exists(w2) else None)\n    payload = {\"model_id\": model_id, \"adapter_path\": adapter_path}\n    if os.path.exists(cfg):\n        payload[\"adapter_config_sha256\"] = _sha256_file(cfg)\n    if weights is not None and os.path.exists(weights):\n        payload[\"adapter_weights_sha256\"] = _sha256_file(weights)\n    return sha256_json(payload)\n\n# Lazy cache (can set to 0 to unload after each run)\nMAX_CACHED_MODELS = int(os.environ.get(\"BAP_MAX_CACHED_MODELS\", \"1\"))\nMODEL_CACHE: Dict[str, HFModelWrapper] = {}  # key: f\"{model_id}||{adapter_path or 'base'}\"\n\n\ndef get_model(model_id: str, adapter_path: Optional[str] = None) -> HFModelWrapper:\n    \"\"\"Lazy-load base model, and optionally attach a LoRA adapter (PEFT). Cached per (model_id, adapter_path).\"\"\"\n    if DRY_RUN:\n        return None  # type: ignore\n    key = f\"{model_id}||{os.path.abspath(adapter_path) if adapter_path is not None else 'base'}\"\n    if key in MODEL_CACHE:\n        return MODEL_CACHE[key]\n    print(\"[LOAD MODEL]\", model_id, (\"+LoRA\" if adapter_path else \"\"))\n    mw = HFModelWrapper.load(model_id, adapter_path=adapter_path, use_4bit=USE_4BIT)\n    MODEL_CACHE[key] = mw\n    # evict if needed\n    if len(MODEL_CACHE) > MAX_CACHED_MODELS:\n        k0 = next(iter(MODEL_CACHE.keys()))\n        if k0 != key:\n            del MODEL_CACHE[k0]\n    return mw\n"
    },
    {
      "cell_type": "markdown",
      "id": "7be56068",
      "metadata": {
        "id": "7be56068"
      },
      "source": [
        "\n",
        "## 8) Evidence Package + Mock Attestation\n",
        "We bind:\n",
        "- code digest\n",
        "- dataset digest\n",
        "- model digest\n",
        "- config digest\n",
        "- derived seed + index commitment\n",
        "- output hash over `(bits, cost_bins)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce36998a",
      "metadata": {
        "id": "ce36998a"
      },
      "outputs": [],
      "source": "\nCODE_ID = \"eval_v_full\"\n\ndef digest_code(code_id: str) -> str:\n    return sha256_json({\"code_id\": code_id})\n\ndef digest_config(task_type: str, n: int, with_replacement: bool, gen_cfg: Dict[str, Any]) -> str:\n    return sha256_json({\"task_type\": task_type, \"n\": n, \"with_replacement\": with_replacement, \"gen_cfg\": gen_cfg})\n\ndef digest_dataset(dataset: TaskDataset, include_text: bool = False) -> str:\n    if include_text:\n        payload = [(x.instance_id, x.task_type, x.prompt, x.label01, x.ground_truth, x.mc_answer, x.meta) for x in dataset.instances]\n    else:\n        payload = []\n        for x in dataset.instances:\n            gt_hash = sha256_json({\"gt\": x.ground_truth}) if x.ground_truth is not None else None\n            prompt_hash = sha256_json({\"prompt\": x.prompt})  # optional; you can remove to reduce leakage\n            payload.append((x.instance_id, x.task_type, x.label01, gt_hash, x.mc_answer, prompt_hash, x.meta))\n    return sha256_json({\"name\": dataset.name, \"payload\": payload})\n\ndef hash_outputs(bits: np.ndarray, cost_bins: np.ndarray) -> str:\n    return sha256_json({\"bits\": bits.astype(int).tolist(), \"cost_bins\": cost_bins.astype(int).tolist()})\n\n# Mock attestation via HMAC\nATT_KEY = os.environ.get(\"BAP_ATT_KEY\", \"mock-attestation-key-change-me\").encode(\"utf-8\")\n\ndef sign_hmac(message: Dict[str, Any]) -> str:\n    msg = json.dumps(message, sort_keys=True).encode(\"utf-8\")\n    return hmac.new(ATT_KEY, msg, hashlib.sha256).hexdigest()\n\ndef verify_hmac(message: Dict[str, Any], sig: str) -> bool:\n    return hmac.compare_digest(sign_hmac(message), sig)\n"
    },
    {
      "cell_type": "markdown",
      "id": "859ebfc4",
      "metadata": {
        "id": "859ebfc4"
      },
      "source": [
        "\n",
        "## 9) BAP Server + Verifier Client (bit-only interface)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf867899",
      "metadata": {
        "id": "cf867899"
      },
      "outputs": [],
      "source": "\n@dataclass\nclass EvalPackage:\n    bits: np.ndarray\n    cost_bins: np.ndarray\n    evidence: Dict[str, Any]\n    signature: str\n\n@dataclass\nclass BAPServer:\n    code_id: str\n\n    def run(self, dataset: TaskDataset, model_id: str, run_nonce: str,\n            adapter_path: Optional[str] = None, seed_model_id: Optional[str] = None,\n            n: int = N_EVAL, with_replacement: bool = WITH_REPLACEMENT) -> EvalPackage:\n\n        # resolve gen cfg by task_type (dataset is single-type expected)\n        task_type = dataset.instances[0].task_type\n        gen_cfg = GEN_CFG_BINARY if task_type==\"binary\" else (GEN_CFG_ANSWER if task_type==\"answer\" else GEN_CFG_MC)\n\n        cfg_digest = digest_config(task_type, n, with_replacement, gen_cfg)\n        ds_digest = digest_dataset(dataset, include_text=False)\n        m_digest = digest_model_state(model_id, adapter_path)\n        m_digest_seed = digest_model_public(seed_model_id or model_id)\n\n        seed = derive_seed(PUBLIC_BEACON, ds_digest, m_digest_seed, cfg_digest, run_nonce)\n        idx = sample_indices(seed, len(dataset.instances), n, with_replacement)\n        idx_commit = indices_commitment(idx)\n\n        bits = np.zeros(n, dtype=int)\n        cb = np.zeros(n, dtype=int)\n\n        seen_levels = np.full(n, -1, dtype=int)\n        dataset_label_preds = [\"unknown\"] * n\n        mw = get_model(model_id, adapter_path=adapter_path)\n\n        for t, j in enumerate(tqdm(\n              idx,\n              desc=f\"{dataset.name} | {model_id}\",\n              unit=\"q\",\n              dynamic_ncols=True,\n              leave=False,\n              disable=not TQDM_ENABLED\n          )):\n            inst = dataset.instances[int(j)]\n            if DRY_RUN:\n                out = \"0\"\n                seen_out = \"0\"\n                label_out = \"unknown\"\n            else:\n                # Q1/Q2 are asked in separate one-shot prompts (fresh context each call)\n                seen_out = mw.ask_seen_level(inst.prompt)\n                label_out = mw.ask_dataset_label(inst.prompt)\n                out = mw.generate(inst)\n            # Parse and record Q1/Q2 outputs (robust to occasional extra tokens)\n            m_seen = re.search(r\"[0-2]\", str(seen_out))\n            seen_levels[t] = int(m_seen.group(0)) if m_seen else -1\n\n            lo = str(label_out).strip().lower()\n            m_lab = None\n            for lab in DATASET_LABELS:\n                if re.search(rf\"\\b{re.escape(lab)}\\b\", lo):\n                    m_lab = lab\n                    break\n            dataset_label_preds[t] = m_lab if m_lab is not None else \"unknown\"\n\n            b, steps = score_and_cost(inst, out)\n            bits[t] = b\n            cb[t] = cost_bin(int(steps))\n\n        evidence = {\n            \"code_digest\": digest_code(self.code_id),\n            \"dataset_digest\": ds_digest,\n            \"model_digest\": m_digest,\n            \"seed_model_digest\": m_digest_seed,\n            \"adapter_path\": os.path.abspath(adapter_path) if adapter_path is not None else None,\n            \"config_digest\": cfg_digest,\n            \"public_beacon\": PUBLIC_BEACON,\n            \"run_nonce\": run_nonce,\n            \"seed\": int(seed),\n            \"index_commitment\": idx_commit,\n            \"output_hash\": hash_outputs(bits, cb),\n            \"seen_levels\": seen_levels.astype(int).tolist(),\n            \"dataset_label_preds\": dataset_label_preds,\n        }\n        sig = sign_hmac(evidence)\n        return EvalPackage(bits=bits, cost_bins=cb, evidence=evidence, signature=sig)\n\n@dataclass\nclass VerifierClient:\n    trusted_code_digests: set\n    trusted_dataset_digests: set\n    trusted_model_digests: set\n\n    def verify(self, pkg: EvalPackage) -> bool:\n        if not verify_hmac(pkg.evidence, pkg.signature):\n            return False\n        if pkg.evidence[\"code_digest\"] not in self.trusted_code_digests:\n            return False\n        if pkg.evidence[\"dataset_digest\"] not in self.trusted_dataset_digests:\n            return False\n        if pkg.evidence[\"model_digest\"] not in self.trusted_model_digests:\n            return False\n        if pkg.evidence[\"output_hash\"] != hash_outputs(pkg.bits, pkg.cost_bins):\n            return False\n        return True\n\nserver = BAPServer(code_id=CODE_ID)\n\n# Build dataset registry: (harmonized tasks) + (public HF datasets)\nDATASETS: Dict[str, TaskDataset] = {}\nDATASETS.update(harmonized_tasks)\nDATASETS.update(public_datasets)\n\ntrusted_code = {digest_code(CODE_ID)}\ntrusted_models = {digest_model_public(m) for m in HF_MODELS}\ntrusted_datasets = {digest_dataset(ds, include_text=False) for ds in DATASETS.values()}\n\nverifier = VerifierClient(trusted_code, trusted_datasets, trusted_models)\n\nprint(\"Datasets registered:\", len(DATASETS))\nprint(\"Trusted datasets:\", len(trusted_datasets), \"| Trusted models:\", len(trusted_models))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eYCiWBgcR_I",
      "metadata": {
        "id": "0eYCiWBgcR_I"
      },
      "outputs": [],
      "source": "# ==========================\n# 9b) Build grouped datasets (6 groups) with fixed 300/300 train/eval and save to disk\n# ==========================\n# Groups (as requested):\n#   aqua           = aqua_binary + aqua_mc_test\n#   arc_challenge  = arc_challenge_binary\n#   gsm8k          = gsm8k_answer_test + gsm8k_binary\n#   p_time         = p_graph_connectivity + ptime_arith  (+ group_word_generic_easy, if present)\n#   np_time        = random_np_rbh_word_avg + np_subset_sum_avg + np_subset_sum_worst + np_rbh_word_worst_3sat\n#   undecidable    = remaining halting-related tasks (from the universe list below)\n#\n# Output keys are:\n#   <group>_train  (300 samples)\n#   <group>_test   (300 samples)\n# These are persisted as JSONL, and registered back into DATASETS for later LoRA/post-train.\n\nimport json, os\nimport numpy as np\nfrom typing import Dict, List, Tuple\n\nGROUPED_DIR = os.environ.get(\"BAP_GROUPED_DIR\", os.path.join(BASE_DIR, \"bap_grouped_splits_v1\"))\nos.makedirs(GROUPED_DIR, exist_ok=True)\n\nN_GROUP_TRAIN = int(os.environ.get(\"BAP_GROUP_TRAIN_N\", \"300\"))\nN_GROUP_TEST  = int(os.environ.get(\"BAP_GROUP_TEST_N\", \"300\"))\nGROUP_BASE_SEED = int(os.environ.get(\"BAP_GROUP_BASE_SEED\", \"20260112\"))\n\n# Restrict grouping to the canonical set used in your full-coverage runs.\n# This prevents accidentally pulling in extra tasks from the harmonized JSONL.\nUNIVERSE_KEYS = [\n    # AQuA / ARC / GSM8K\n    \"aqua_binary\", \"aqua_mc_test\",\n    \"arc_challenge_binary\",\n    \"gsm8k_answer_test\", \"gsm8k_binary\",\n    # P-time\n    \"p_graph_connectivity\", \"ptime_arith\",\n    # (optional easy P-time-ish task from harmonized JSONL)\n    \"group_word_generic_easy\",\n    # NP-time (NP-hard-ish)\n    \"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\",\n    # Halting/undecidable family (SGU/TM keys from your runs)\n    \"sgu_collatz_aligned\", \"sgu_collatz_aligned__sgu\",\n    \"sgu_index_empty_language\", \"sgu_index_empty_language__sgu\",\n    \"sgu_index_total_halt\", \"sgu_index_total_halt__sgu\",\n    \"sgu_semigroup_wp_amp\", \"sgu_semigroup_wp_amp__sgu\",\n    \"tm_generic_halt\", \"tm_hard_halt\",\n]\n\n# Keep only keys that exist in current DATASETS registry.\nUNIVERSE_KEYS = [k for k in UNIVERSE_KEYS if k in DATASETS]\nprint(\"Grouping universe keys:\", len(UNIVERSE_KEYS))\n\nfixed_groups: Dict[str, List[str]] = {\n    \"aqua\": [\"aqua_binary\", \"aqua_mc_test\"],\n    \"arc_challenge\": [\"arc_challenge_binary\"],\n    \"gsm8k\": [\"gsm8k_answer_test\", \"gsm8k_binary\"],\n    \"p_time\": [\"p_graph_connectivity\", \"ptime_arith\"],\n    \"np_time\": [\"random_np_rbh_word_avg\", \"np_subset_sum_avg\", \"np_subset_sum_worst\", \"np_rbh_word_worst_3sat\"],\n}\n\n# Put group_word_generic_easy into p_time if present (keeps 6 groups total and avoids dropping it)\nif \"group_word_generic_easy\" in UNIVERSE_KEYS and \"group_word_generic_easy\" in DATASETS:\n    if \"group_word_generic_easy\" not in fixed_groups[\"p_time\"]:\n        fixed_groups[\"p_time\"].append(\"group_word_generic_easy\")\n\nused = set(k for ks in fixed_groups.values() for k in ks if k in UNIVERSE_KEYS)\nundecidable_keys = [k for k in UNIVERSE_KEYS if k not in used]\nfixed_groups[\"undecidable\"] = undecidable_keys\n\nprint(\"=== Group plan ===\")\nfor g, ks in fixed_groups.items():\n    ks2 = [k for k in ks if k in DATASETS]\n    print(f\"[{g}] {len(ks2)} sources:\", ks2)\n\ndef _flatten_instances(dataset_keys: List[str]) -> List[TaskInstance]:\n    inst: List[TaskInstance] = []\n    for k in dataset_keys:\n        if k not in DATASETS:\n            continue\n        inst.extend(DATASETS[k].instances)\n    return inst\n\ndef _sample_disjoint(instances: List[TaskInstance], n_train: int, n_eval: int, seed: int) -> Tuple[List[TaskInstance], List[TaskInstance]]:\n    rng = np.random.default_rng(seed)\n    N = len(instances)\n    if N == 0:\n        raise ValueError(\"No instances to sample from.\")\n    need = n_train + n_eval\n    perm = rng.permutation(N)\n    if N >= need:\n        tr_idx = perm[:n_train]\n        ev_idx = perm[n_train:need]\n        train = [instances[int(i)] for i in tr_idx]\n        evals = [instances[int(i)] for i in ev_idx]\n        return train, evals\n\n    print(f\"[WARN] Only {N} instances available < {need}. Using replacement to reach {n_train}/{n_eval}.\")\n    # train first (no replacement if possible)\n    tr_take = min(n_train, N)\n    train = [instances[int(i)] for i in perm[:tr_take]]\n    # remaining for eval\n    remaining = [instances[int(i)] for i in perm[tr_take:]]\n    evals = remaining[:min(len(remaining), n_eval)]\n    if len(evals) < n_eval:\n        extra = rng.choice(instances, size=(n_eval - len(evals)), replace=True).tolist()\n        evals = evals + extra\n    if len(train) < n_train:\n        extra = rng.choice(instances, size=(n_train - len(train)), replace=True).tolist()\n        train = train + extra\n    return train, evals\n\ndef _inst_to_dict(x: TaskInstance) -> dict:\n    return {\n        \"instance_id\": x.instance_id,\n        \"task_type\": x.task_type,\n        \"prompt\": x.prompt,\n        \"label01\": x.label01,\n        \"ground_truth\": x.ground_truth,\n        \"mc_answer\": x.mc_answer,\n        \"mc_choices\": x.mc_choices,\n        \"meta\": x.meta,\n    }\n\ndef _save_jsonl(path: str, instances: List[TaskInstance]):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for inst in instances:\n            f.write(json.dumps(_inst_to_dict(inst), ensure_ascii=False) + \"\\n\")\n\ndef _make_dataset(name: str, instances: List[TaskInstance]) -> TaskDataset:\n    return TaskDataset(name=name, instances=instances)\n\nmanifest = {\n    \"version\": \"bap_grouped_splits_v1\",\n    \"n_train\": N_GROUP_TRAIN,\n    \"n_test\": N_GROUP_TEST,\n    \"base_seed\": GROUP_BASE_SEED,\n    \"universe_keys\": UNIVERSE_KEYS,\n    \"groups\": {}\n}\n\nfor gi, (gname, keys) in enumerate(fixed_groups.items()):\n    keys = [k for k in keys if k in DATASETS]\n    merged = _flatten_instances(keys)\n    seed = GROUP_BASE_SEED + gi * 1000\n    train_inst, test_inst = _sample_disjoint(merged, N_GROUP_TRAIN, N_GROUP_TEST, seed=seed)\n\n    train_key = f\"{gname}_train\"\n    test_key  = f\"{gname}_test\"  # important: _make_train_eval_300() detects *_train/*_test\n    train_path = os.path.join(GROUPED_DIR, f\"{train_key}.jsonl\")\n    test_path  = os.path.join(GROUPED_DIR, f\"{test_key}.jsonl\")\n    _save_jsonl(train_path, train_inst)\n    _save_jsonl(test_path, test_inst)\n\n    DATASETS[train_key] = _make_dataset(train_key, train_inst)\n    DATASETS[test_key]  = _make_dataset(test_key, test_inst)\n\n    manifest[\"groups\"][gname] = {\n        \"sources\": keys,\n        \"n_merged\": len(merged),\n        \"seed\": seed,\n        \"train_key\": train_key,\n        \"test_key\": test_key,\n        \"train_path\": train_path,\n        \"test_path\": test_path,\n    }\n    print(f\"[SAVED] {gname}: merged={len(merged)} train={len(train_inst)} test={len(test_inst)}\")\n\nmanifest_path = os.path.join(GROUPED_DIR, \"manifest.json\")\nwith open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(manifest, f, indent=2, ensure_ascii=False)\n\nGROUP_TEST_KEYS = [f\"{g}_test\" for g in fixed_groups.keys()]\nGROUP_TRAIN_KEYS = [f\"{g}_train\" for g in fixed_groups.keys()]\nprint(\"Grouped dataset keys ready.\")\nprint(\"GROUP_TEST_KEYS:\", GROUP_TEST_KEYS)\nprint(\"manifest:\", manifest_path)\n"
    },
    {
      "cell_type": "markdown",
      "id": "1cc89b56",
      "metadata": {
        "id": "1cc89b56"
      },
      "source": [
        "\n",
        "## 10) Batch Runner: evaluate **all models × all datasets**\n",
        "This is the piece you asked for: everything connected.\n",
        "It runs and stores artifacts:\n",
        "- `bits`, `cost_bins`, `evidence`, `signature`\n",
        "\n",
        "We store each run as JSON in `RESULTS_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a2e617",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d2a2e617"
      },
      "outputs": [],
      "source": "\ndef save_package(pkg: EvalPackage, path: str):\n    out = {\n        \"bits\": pkg.bits.astype(int).tolist(),\n        \"cost_bins\": pkg.cost_bins.astype(int).tolist(),\n        \"evidence\": pkg.evidence,\n        \"signature\": pkg.signature,\n        \"signature_type\": \"hmac\",\n    }\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out, f, indent=2, sort_keys=True)\n\ndef run_all(models: List[str], datasets: Dict[str, TaskDataset], seeds: List[int]) -> pd.DataFrame:\n    \"\"\"Evaluate all (dataset × model × seed) combinations with caching + verification.\n    Adds a progress bar and avoids NaN/empty-bit pitfalls.\n    \"\"\"\n    rows = []\n    # Build a verifier that trusts exactly the dataset digests evaluated in this call.\n    trusted_datasets_local = set(digest_dataset(_ds, include_text=False) for _ds in datasets.values())\n    verifier_local = VerifierClient(trusted_code, trusted_datasets_local, set(trusted_models))\n\n    jobs = []\n    for dname, ds in datasets.items():\n        if len(getattr(ds, \"instances\", [])) == 0:\n            continue\n        for mid in models:\n            for s in seeds:\n                jobs.append((dname, ds, mid, int(s)))\n\n    pbar = tqdm(jobs, desc=\"Evaluating (dataset×model×seed)\", unit=\"run\",\n                dynamic_ncols=True, disable=not TQDM_ENABLED)\n\n    for dname, ds, mid, s in pbar:\n        pbar.set_postfix(dataset=dname, model=mid, seed=s)\n\n        run_nonce = f\"{dname}|{mid}|seed{s}\"\n        out_path = os.path.join(RESULTS_DIR, f\"{safe_name(dname)}__{safe_name(mid)}__{s}.json\")\n\n        if os.path.exists(out_path):\n            # cached\n            with open(out_path, \"r\", encoding=\"utf-8\") as f:\n                saved = json.load(f)\n            bits = np.array(saved.get(\"bits\", []), dtype=int)\n            acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n            rows.append({\"dataset\": dname, \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": True, \"path\": out_path})\n            continue\n\n        pkg = server.run(ds, mid, run_nonce=run_nonce, n=N_EVAL, with_replacement=WITH_REPLACEMENT)\n        ok = verifier_local.verify(pkg)\n        if not ok:\n            raise RuntimeError(f\"Verification failed for {dname} {mid} seed{s}\")\n        save_package(pkg, out_path)\n\n        acc = float(pkg.bits.mean()) if len(pkg.bits) > 0 else float(\"nan\")\n        rows.append({\"dataset\": dname, \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": False, \"path\": out_path})\n        print(\"[DONE]\", dname, mid, \"seed\", s, \"acc\", acc)\n\n    return pd.DataFrame(rows)\n\n\n# WARNING: This can be very expensive if DATASETS contains many harmonized tasks.\n# Start small by setting TASKS to a short list or by filtering DATASETS keys.\n# Example: DATASETS_TO_RUN = {k: DATASETS[k] for k in [\"gsm8k_binary\",\"aqua_binary\"] if k in DATASETS}\nGROUP_TEST_KEYS = [\n    \"aqua_test\",\n    \"arc_challenge_test\",\n    \"gsm8k_test\",\n    \"p_time_test\",\n    \"np_time_test\",\n    \"undecidable_test\",\n]\nDATASETS_TO_RUN = {k: DATASETS[k] for k in GROUP_TEST_KEYS if k in DATASETS}\n\n\ndf_runs = run_all(HF_MODELS, DATASETS_TO_RUN, EVAL_SEEDS)\ndisplay(df_runs.head())\nprint(\"Total runs:\", len(df_runs))\n\n\n\n# =======================\n# 10b) Before/After runner with per-dataset LoRA state switching\n# =======================\n\nLORA_PER_DATASET = bool(int(os.environ.get(\"BAP_LORA_PER_DATASET\", \"0\")))  # set 1 to train adapters\nLORA_TRAIN_N = int(os.environ.get(\"BAP_LORA_TRAIN_N\", \"300\"))\nLORA_EVAL_N = int(os.environ.get(\"BAP_LORA_EVAL_N\", \"300\"))\nLORA_SPLIT_SEED = int(os.environ.get(\"BAP_LORA_SPLIT_SEED\", \"123\"))\n\nLORA_MAX_STEPS_PER_DATASET = int(os.environ.get(\"BAP_LORA_MAX_STEPS\", \"200\"))\nLORA_LR_PER_DATASET = float(os.environ.get(\"BAP_LORA_LR\", \"2e-4\"))\nLORA_R_PER_DATASET = int(os.environ.get(\"BAP_LORA_R\", \"8\"))\nLORA_ALPHA_PER_DATASET = int(os.environ.get(\"BAP_LORA_ALPHA\", \"16\"))\nLORA_DROPOUT_PER_DATASET = float(os.environ.get(\"BAP_LORA_DROPOUT\", \"0.05\"))\nLORA_MAX_LEN_PER_DATASET = int(os.environ.get(\"BAP_LORA_MAX_LEN\", \"512\"))\n\nLORA_ADAPTERS_DIR = os.path.join(RESULTS_DIR, \"lora_per_dataset\")\nos.makedirs(LORA_ADAPTERS_DIR, exist_ok=True)\n\ndef _make_train_eval_300(ds_name: str, datasets: Dict[str, TaskDataset], train_n: int, eval_n: int, seed: int):\n    \"\"\"Return (train_ds, eval_ds). Prefer explicit *_train/*_test pairs, else split within ds.\"\"\"\n    # Explicit pair if available\n    if ds_name.endswith(\"_test\"):\n        cand_train = ds_name[:-5] + \"_train\"\n        if cand_train in datasets:\n            train_ds_full = datasets[cand_train]\n            eval_ds_full = datasets[ds_name]\n            # sample 300 each (no replacement)\n            rng = np.random.default_rng(seed)\n            tr_idx = rng.choice(len(train_ds_full.instances), size=min(train_n, len(train_ds_full.instances)), replace=False)\n            ev_idx = rng.choice(len(eval_ds_full.instances), size=min(eval_n, len(eval_ds_full.instances)), replace=False)\n            tr = [train_ds_full.instances[int(i)] for i in tr_idx]\n            ev = [eval_ds_full.instances[int(i)] for i in ev_idx]\n            return TaskDataset(name=f\"{cand_train}_n{len(tr)}\", instances=tr), TaskDataset(name=f\"{ds_name}_n{len(ev)}\", instances=ev)\n\n    # Fallback: split within ds\n    base = datasets[ds_name]\n    rng = np.random.default_rng(seed)\n    idx = rng.permutation(len(base.instances)).tolist()\n    tr = [base.instances[i] for i in idx[:min(train_n, len(idx))]]\n    ev = [base.instances[i] for i in idx[min(train_n, len(idx)):min(train_n+eval_n, len(idx))]]\n    return TaskDataset(name=f\"{ds_name}_train_n{len(tr)}\", instances=tr), TaskDataset(name=f\"{ds_name}_eval_n{len(ev)}\", instances=ev)\n\ndef _train_rows_for_inst(inst: TaskInstance) -> Optional[str]:\n    \"\"\"Build a single training text example aligned with HFModelWrapper.generate() prompts.\"\"\"\n    if inst.task_type == \"answer\":\n        gt_raw = inst.ground_truth or \"\"\n        gt = extract_gsm8k_final(gt_raw) or extract_model_final(gt_raw) or gt_raw.strip()\n        prompt = (\n            \"Solve the following math problem. Return ONLY the final numeric answer.\\n\\n\"\n            f\"Problem:\\n{inst.prompt.strip()}\\n\\nFinal answer:\"\n        )\n        return (prompt + \" \" + str(gt).strip()).strip()\n    if inst.task_type == \"mc\":\n        if inst.mc_answer is None:\n            return None\n        return (inst.prompt.strip() + \" \" + inst.mc_answer.strip()).strip()\n    if inst.task_type == \"binary\":\n        if inst.label01 is None:\n            return None\n        prompt = (\n            \"Read the following problem and decide if the statement is TRUE (1) or FALSE (0).\\n\\n\"\n            f\"Problem:\\n{inst.prompt.strip()}\\n\\nReturn only a single token (0 or 1):\"\n        )\n        return (prompt + \" \" + str(int(inst.label01))).strip()\n    return None\n\ndef build_lora_train_rows(train_ds: TaskDataset, n: int, seed: int) -> List[Dict[str, str]]:\n    rng = np.random.default_rng(seed)\n    m = min(n, len(train_ds.instances))\n    idx = rng.choice(len(train_ds.instances), size=m, replace=False)\n    rows = []\n    for i in idx:\n        txt = _train_rows_for_inst(train_ds.instances[int(i)])\n        if txt is not None:\n            rows.append({\"text\": txt})\n    return rows\n\ndef train_lora_adapter_simple(base_model_id: str, train_rows: List[Dict[str,str]], out_dir: str,\n                             max_steps: int, lr: float, r: int, alpha: int, dropout: float, max_len: int = 512):\n    \"\"\"Train and save a PEFT LoRA adapter to out_dir.\"\"\"\n    import torch\n    from datasets import Dataset as HFDataset\n    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n    from peft import LoraConfig, get_peft_model\n\n    os.makedirs(out_dir, exist_ok=True)\n\n    tok = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    kwargs = {\"device_map\": \"auto\"}\n    if USE_4BIT:\n        kwargs = {\"load_in_4bit\": True, \"device_map\": \"auto\"}\n\n    model = AutoModelForCausalLM.from_pretrained(base_model_id, **kwargs)\n    model.train()\n\n    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n    peft_cfg = LoraConfig(\n        r=r,\n        lora_alpha=alpha,\n        lora_dropout=dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=target_modules,\n    )\n    model = get_peft_model(model, peft_cfg)\n\n    hf = HFDataset.from_list(train_rows)\n\n    def tok_fn(ex):\n        x = tok(ex[\"text\"], truncation=True, max_length=max_len)\n        x[\"labels\"] = x[\"input_ids\"].copy()\n        return x\n\n    hf = hf.map(tok_fn, remove_columns=[\"text\"])\n    collator = DataCollatorForLanguageModeling(tok, mlm=False)\n\n    args = TrainingArguments(\n        output_dir=out_dir,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        learning_rate=lr,\n        max_steps=max_steps,\n        logging_steps=max(1, max_steps//10),\n        save_steps=max_steps,\n        save_total_limit=1,\n        report_to=[],\n        fp16=torch.cuda.is_available(),\n    )\n\n    trainer = Trainer(model=model, args=args, train_dataset=hf, data_collator=collator)\n    trainer.train()\n    model.save_pretrained(out_dir)\n    tok.save_pretrained(out_dir)\n    print(\"[LoRA saved]\", out_dir)\n\ndef ensure_lora_adapter(model_id: str, train_ds: TaskDataset, adapter_dir: str, seed: int) -> str:\n    \"\"\"Train adapter if not already present; return adapter path.\"\"\"\n    marker = os.path.join(adapter_dir, \"adapter_config.json\")\n    if os.path.exists(marker):\n        return adapter_dir\n    train_rows = build_lora_train_rows(train_ds, n=LORA_TRAIN_N, seed=seed)\n    if len(train_rows) == 0:\n        raise RuntimeError(f\"No train rows constructed for {train_ds.name}\")\n    train_lora_adapter_simple(\n        base_model_id=model_id,\n        train_rows=train_rows,\n        out_dir=adapter_dir,\n        max_steps=LORA_MAX_STEPS_PER_DATASET,\n        lr=LORA_LR_PER_DATASET,\n        r=LORA_R_PER_DATASET,\n        alpha=LORA_ALPHA_PER_DATASET,\n        dropout=LORA_DROPOUT_PER_DATASET,\n        max_len=LORA_MAX_LEN_PER_DATASET,\n    )\n    return adapter_dir\n\ndef _state_dir(phase: str, ds_base: str, split: str, model_id: str, state: str) -> str:\n    return os.path.join(RESULTS_DIR, \"before_after\", phase, safe_name(ds_base), split, safe_name(model_id), state)\n\ndef run_before_after(models: List[str], datasets: Dict[str, TaskDataset], dataset_keys: List[str], seeds: List[int]) -> pd.DataFrame:\n    \"\"\"Runs:\n      - BEFORE (base): train + eval\n      - AFTER  (lora): eval only, where LoRA is trained on train split for the same dataset key.\n\n    Adds caching + verification + progress bar. Also fixes:\n      - verifier_local is defined per dataset key (so digests match the split datasets)\n      - uses verifier_local (not a stray global verifier) when trusting adapter digests\n      - guards empty bit arrays when computing accuracy\n    \"\"\"\n    rows = []\n\n    # Pre-compute total units for a single global progress bar\n    present_keys = [k for k in dataset_keys if k in datasets]\n    total_units = len(present_keys) * len(models) * len(seeds) * 3  # before(train)+before(eval)+after(eval)\n    pbar = tqdm(total=total_units, desc=\"Before/After (base vs LoRA)\", unit=\"run\",\n                dynamic_ncols=True, disable=not TQDM_ENABLED)\n\n    try:\n        for ds_key in dataset_keys:\n            if ds_key not in datasets:\n                print(\"[SKIP missing dataset]\", ds_key)\n                continue\n\n            train_ds, eval_ds = _make_train_eval_300(ds_key, datasets, LORA_TRAIN_N, LORA_EVAL_N, LORA_SPLIT_SEED)\n\n            # Verifier must trust *these* split datasets (their digests differ from the original full dataset)\n            trusted_datasets_local = {\n                digest_dataset(train_ds, include_text=False),\n                digest_dataset(eval_ds, include_text=False),\n            }\n            verifier_local = VerifierClient(trusted_code, trusted_datasets_local, set(trusted_models))\n\n            for mid in models:\n                for s in seeds:\n                    s = int(s)\n\n                    # ---------- BEFORE: base on train + eval ----------\n                    for split_name, ds_obj in [(\"train\", train_ds), (\"eval\", eval_ds)]:\n                        pbar.set_postfix(dataset=ds_key, model=mid, seed=s, phase=f\"before/{split_name}\")\n\n                        run_nonce = f\"{ds_key}|{mid}|base|{split_name}|seed{s}\"\n                        out_dir = _state_dir(\"before\", ds_key, split_name, mid, \"base\")\n                        os.makedirs(out_dir, exist_ok=True)\n                        out_path = os.path.join(out_dir, f\"seed{s}.json\")\n\n                        if os.path.exists(out_path):\n                            with open(out_path, \"r\", encoding=\"utf-8\") as f:\n                                saved = json.load(f)\n                            bits = np.array(saved.get(\"bits\", []), dtype=int)\n                            acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n                            rows.append({\n                                \"dataset\": ds_key, \"split\": split_name, \"phase\": \"before\", \"state\": \"base\",\n                                \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": True, \"path\": out_path\n                            })\n                            pbar.update(1)\n                        else:\n                            pkg = server.run(\n                                ds_obj, mid, adapter_path=None, seed_model_id=mid,\n                                run_nonce=run_nonce, n=min(N_EVAL, len(ds_obj.instances)), with_replacement=False\n                            )\n                            ok = verifier_local.verify(pkg)\n                            if not ok:\n                                raise RuntimeError(f\"Verification failed for {ds_key} {mid} before {split_name} seed{s}\")\n                            save_package(pkg, out_path)\n\n                            acc = float(pkg.bits.mean()) if len(pkg.bits) > 0 else float(\"nan\")\n                            rows.append({\n                                \"dataset\": ds_key, \"split\": split_name, \"phase\": \"before\", \"state\": \"base\",\n                                \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": False, \"path\": out_path\n                            })\n                            print(\"[DONE]\", ds_key, mid, \"before\", split_name, \"seed\", s, \"acc\", acc)\n                            pbar.update(1)\n\n                    # ---------- AFTER: lora on eval only ----------\n                    pbar.set_postfix(dataset=ds_key, model=mid, seed=s, phase=\"after/eval\")\n\n                    if LORA_PER_DATASET:\n                        adapter_dir = os.path.join(LORA_ADAPTERS_DIR, safe_name(mid), safe_name(ds_key))\n                        adapter_path = ensure_lora_adapter(mid, train_ds, adapter_dir, seed=LORA_SPLIT_SEED)\n                        # Trust adapter digest for verifier (so verification passes for LoRA-modified model state)\n                        verifier_local.trusted_model_digests.add(digest_model_state(mid, adapter_path))\n                    else:\n                        adapter_path = None\n\n                    run_nonce = f\"{ds_key}|{mid}|lora|eval|seed{s}\"\n                    out_dir = _state_dir(\"after\", ds_key, \"eval\", mid, \"lora\" if adapter_path else \"no_lora\")\n                    os.makedirs(out_dir, exist_ok=True)\n                    out_path = os.path.join(out_dir, f\"seed{s}.json\")\n\n                    if os.path.exists(out_path):\n                        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n                            saved = json.load(f)\n                        bits = np.array(saved.get(\"bits\", []), dtype=int)\n                        acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n                        rows.append({\n                            \"dataset\": ds_key, \"split\": \"eval\", \"phase\": \"after\",\n                            \"state\": \"lora\" if adapter_path else \"no_lora\",\n                            \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": True, \"path\": out_path\n                        })\n                        pbar.update(1)\n                        continue\n\n                    pkg = server.run(\n                        eval_ds, mid, adapter_path=adapter_path, seed_model_id=mid,\n                        run_nonce=run_nonce, n=min(N_EVAL, len(eval_ds.instances)), with_replacement=False\n                    )\n                    ok = verifier_local.verify(pkg)\n                    if not ok:\n                        raise RuntimeError(f\"Verification failed for {ds_key} {mid} after eval seed{s}\")\n                    save_package(pkg, out_path)\n\n                    acc = float(pkg.bits.mean()) if len(pkg.bits) > 0 else float(\"nan\")\n                    rows.append({\n                        \"dataset\": ds_key, \"split\": \"eval\", \"phase\": \"after\",\n                        \"state\": \"lora\" if adapter_path else \"no_lora\",\n                        \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": False, \"path\": out_path\n                    })\n                    print(\"[DONE]\", ds_key, mid, \"after eval\", \"seed\", s, \"acc\", acc)\n                    pbar.update(1)\n\n        return pd.DataFrame(rows)\n    finally:\n        try:\n            pbar.close()\n        except Exception:\n            pass\n\n\n# Example usage:\n# DATASET_KEYS = [\"gsm8k_answer_test\", \"aqua_mc_test\", \"arc_challenge_mc_test\"]  # add more keys as desired\n# df_ba = run_before_after(HF_MODELS, DATASETS, DATASET_KEYS, EVAL_SEEDS)\n# display(df_ba.head())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8CHvvJzeS8E9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8CHvvJzeS8E9"
      },
      "outputs": [],
      "source": "# =========================\n# Minimal runner WITHOUT verifier\n# - drop VerifierClient / trusted_code / digests\n# - keep caching, saving JSON, and aggregation\n# =========================\n\nimport os, time, json\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Optional\n\n# Assumes these already exist in your notebook:\n# - RESULTS_DIR: str\n# - server: object with .run(ds, model_id, ..., run_nonce=..., n=..., with_replacement=...)\n# - TaskDataset / TaskInstance types (only need ds.name and ds.instances)\n# - safe_name(name: str) -> str\n# - N_EVAL: int\n# - WITH_REPLACEMENT: bool\n\ndef save_package_noverify(pkg, path: str, meta: Optional[Dict] = None):\n    \"\"\"\n    Save EvalPackage-like object to JSON without any verifier.\n    Expected pkg fields:\n      - bits: array-like of {0,1}\n      - cost_bins: array-like ints (optional)\n      - evidence: any JSON-serializable payload (optional)\n    \"\"\"\n    bits = np.asarray(getattr(pkg, \"bits\", []), dtype=int)\n    cost_bins = np.asarray(getattr(pkg, \"cost_bins\", []), dtype=int)\n\n    out = {\n        \"bits\": bits.tolist(),\n        \"cost_bins\": cost_bins.tolist(),\n        \"evidence\": getattr(pkg, \"evidence\", None),\n        # keep lightweight provenance for debugging/repro\n        \"signature\": {\n            \"meta\": meta or {},\n            \"saved_at\": time.time(),\n        },\n        \"signature_type\": \"meta_only\",\n    }\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out, f, indent=2, sort_keys=True)\n\ndef _load_cached_run(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        saved = json.load(f)\n    bits = np.asarray(saved.get(\"bits\", []), dtype=int)\n    cb   = np.asarray(saved.get(\"cost_bins\", []), dtype=int)\n    acc  = float(bits.mean()) if bits.size > 0 else float(\"nan\")\n    return bits, cb, acc, saved\n\ndef run_all_noverify(models: List[str], datasets: Dict[str, \"TaskDataset\"], seeds: List[int]) -> pd.DataFrame:\n    \"\"\"Evaluate each (dataset, model, seed), cache to JSON, and return a tidy DataFrame.\n    Adds a progress bar and guards empty bits.\n    \"\"\"\n    rows = []\n\n    jobs = []\n    for dname, ds in datasets.items():\n        if (not hasattr(ds, \"instances\")) or len(ds.instances) == 0:\n            continue\n        for mid in models:\n            for s in seeds:\n                jobs.append((dname, ds, mid, int(s)))\n\n    pbar = tqdm(jobs, desc=\"Evaluating (no-verify)\", unit=\"run\",\n                dynamic_ncols=True, disable=not TQDM_ENABLED)\n\n    for dname, ds, mid, s in pbar:\n        pbar.set_postfix(dataset=dname, model=mid, seed=s)\n\n        run_nonce = f\"{dname}|{mid}|seed{s}\"\n        out_path = os.path.join(RESULTS_DIR, f\"{safe_name(dname)}__{safe_name(mid)}__{int(s)}.json\")\n\n        # cached?\n        if os.path.exists(out_path):\n            _, _, acc, _ = _load_cached_run(out_path)\n            rows.append({\"dataset\": dname, \"model\": mid, \"seed\": int(s), \"acc\": float(acc), \"cached\": True, \"path\": out_path})\n            continue\n\n        pkg = server.run(ds, mid, run_nonce=run_nonce, n=N_EVAL, with_replacement=WITH_REPLACEMENT)\n\n        bits = np.asarray(getattr(pkg, \"bits\", []), dtype=int)\n        acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n\n        meta = {\n            \"dataset\": dname,\n            \"model\": mid,\n            \"seed\": int(s),\n            \"run_nonce\": run_nonce,\n            \"n\": int(N_EVAL),\n            \"with_replacement\": bool(WITH_REPLACEMENT),\n        }\n        save_package_noverify(pkg, out_path, meta=meta)\n\n        rows.append({\"dataset\": dname, \"model\": mid, \"seed\": int(s), \"acc\": acc, \"cached\": False, \"path\": out_path})\n        print(\"[DONE]\", dname, mid, \"seed\", int(s), \"acc\", acc)\n\n    return pd.DataFrame(rows)\n\n\n\n# =========================\n# Optional: BEFORE/AFTER runner WITHOUT verifier\n# - Keeps your LoRA-per-dataset workflow\n# - Removes all verifier checks and digest tracking\n# =========================\n\ndef run_before_after_noverify(models: List[str], datasets: Dict[str, \"TaskDataset\"], dataset_keys: List[str], seeds: List[int]) -> pd.DataFrame:\n    \"\"\"Before/After runner without verifier.\n    Adds caching + progress bar; guards empty bits.\n    \"\"\"\n    rows = []\n\n    present_keys = [k for k in dataset_keys if k in datasets]\n    total_units = len(present_keys) * len(models) * len(seeds) * 3\n    pbar = tqdm(total=total_units, desc=\"Before/After (no-verify)\", unit=\"run\",\n                dynamic_ncols=True, disable=not TQDM_ENABLED)\n\n    try:\n        for ds_key in dataset_keys:\n            if ds_key not in datasets:\n                print(\"[SKIP missing dataset]\", ds_key)\n                continue\n\n            train_ds, eval_ds = _make_train_eval_300(ds_key, datasets, LORA_TRAIN_N, LORA_EVAL_N, LORA_SPLIT_SEED)\n\n            for mid in models:\n                for s in seeds:\n                    s = int(s)\n\n                    # BEFORE: base on train + eval\n                    for split_name, ds_obj in [(\"train\", train_ds), (\"eval\", eval_ds)]:\n                        pbar.set_postfix(dataset=ds_key, model=mid, seed=s, phase=f\"before/{split_name}\")\n\n                        run_nonce = f\"{ds_key}|{mid}|base|{split_name}|seed{s}\"\n                        out_dir = _state_dir(\"before\", ds_key, split_name, mid, \"base\")\n                        os.makedirs(out_dir, exist_ok=True)\n                        out_path = os.path.join(out_dir, f\"seed{s}.json\")\n\n                        if os.path.exists(out_path):\n                            _, _, acc, _ = _load_cached_run(out_path)\n                            rows.append({\"dataset\": ds_key, \"split\": split_name, \"phase\": \"before\", \"state\": \"base\",\n                                         \"model\": mid, \"seed\": s, \"acc\": float(acc), \"cached\": True, \"path\": out_path})\n                            pbar.update(1)\n                        else:\n                            pkg = server.run(ds_obj, mid, adapter_path=None, seed_model_id=mid,\n                                             run_nonce=run_nonce, n=min(N_EVAL, len(ds_obj.instances)), with_replacement=False)\n                            bits = np.asarray(getattr(pkg, \"bits\", []), dtype=int)\n                            acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n                            save_package_noverify(pkg, out_path, meta={\"dataset\": ds_key, \"model\": mid, \"seed\": s, \"run_nonce\": run_nonce})\n                            rows.append({\"dataset\": ds_key, \"split\": split_name, \"phase\": \"before\", \"state\": \"base\",\n                                         \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": False, \"path\": out_path})\n                            print(\"[DONE]\", ds_key, mid, \"before\", split_name, \"seed\", s, \"acc\", acc)\n                            pbar.update(1)\n\n                    # AFTER: lora eval\n                    pbar.set_postfix(dataset=ds_key, model=mid, seed=s, phase=\"after/eval\")\n\n                    if LORA_PER_DATASET:\n                        adapter_dir = os.path.join(LORA_ADAPTERS_DIR, safe_name(mid), safe_name(ds_key))\n                        adapter_path = ensure_lora_adapter(mid, train_ds, adapter_dir, seed=LORA_SPLIT_SEED)\n                    else:\n                        adapter_path = None\n\n                    run_nonce = f\"{ds_key}|{mid}|lora|eval|seed{s}\"\n                    out_dir = _state_dir(\"after\", ds_key, \"eval\", mid, \"lora\" if adapter_path else \"no_lora\")\n                    os.makedirs(out_dir, exist_ok=True)\n                    out_path = os.path.join(out_dir, f\"seed{s}.json\")\n\n                    if os.path.exists(out_path):\n                        _, _, acc, _ = _load_cached_run(out_path)\n                        rows.append({\"dataset\": ds_key, \"split\": \"eval\", \"phase\": \"after\",\n                                     \"state\": \"lora\" if adapter_path else \"no_lora\",\n                                     \"model\": mid, \"seed\": s, \"acc\": float(acc), \"cached\": True, \"path\": out_path})\n                        pbar.update(1)\n                        continue\n\n                    pkg = server.run(eval_ds, mid, adapter_path=adapter_path, seed_model_id=mid,\n                                     run_nonce=run_nonce, n=min(N_EVAL, len(eval_ds.instances)), with_replacement=False)\n                    bits = np.asarray(getattr(pkg, \"bits\", []), dtype=int)\n                    acc = float(bits.mean()) if len(bits) > 0 else float(\"nan\")\n                    save_package_noverify(pkg, out_path, meta={\"dataset\": ds_key, \"model\": mid, \"seed\": s, \"run_nonce\": run_nonce})\n                    rows.append({\"dataset\": ds_key, \"split\": \"eval\", \"phase\": \"after\",\n                                 \"state\": \"lora\" if adapter_path else \"no_lora\",\n                                 \"model\": mid, \"seed\": s, \"acc\": acc, \"cached\": False, \"path\": out_path})\n                    print(\"[DONE]\", ds_key, mid, \"after eval\", \"seed\", s, \"acc\", acc)\n                    pbar.update(1)\n\n        return pd.DataFrame(rows)\n    finally:\n        try:\n            pbar.close()\n        except Exception:\n            pass\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "b4234c6b",
      "metadata": {
        "id": "b4234c6b"
      },
      "source": [
        "\n",
        "## 11) Analysis Helpers (bit-only)\n",
        "We keep:\n",
        "- binomial test for contamination evidence (vs p0),\n",
        "- paired McNemar exact test,\n",
        "- complexity-tiered curves (pass rate by cost bin) + AUC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f409eb",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c6f409eb"
      },
      "outputs": [],
      "source": "\ndef leakage_test_binomial(bits: np.ndarray, p0: float, alpha: float = 0.05) -> Dict[str, Any]:\n    n = len(bits); k = int(bits.sum())\n    pv = stats.binomtest(k, n, p=p0, alternative=\"greater\").pvalue\n    return {\"n\": n, \"k\": k, \"p_hat\": k/n, \"p0\": p0, \"p_value\": float(pv), \"reject\": bool(pv < alpha)}\n\ndef mcnemar_exact(bA: np.ndarray, bB: np.ndarray) -> float:\n    n10 = int(np.sum((bA==1) & (bB==0)))\n    n01 = int(np.sum((bA==0) & (bB==1)))\n    n = n10 + n01\n    if n == 0:\n        return 1.0\n    cdf = stats.binom.cdf(n10, n, 0.5)\n    sf = stats.binom.sf(n10-1, n, 0.5)\n    return float(min(1.0, 2.0*min(cdf, sf)))\n\ndef pass_rate_by_costbin(bits: np.ndarray, cost_bins_arr: np.ndarray, K: int) -> pd.DataFrame:\n    rows = []\n    for k in range(K):\n        mask = cost_bins_arr == k\n        nk = int(mask.sum())\n        rows.append({\"bin\": k, \"n\": nk, \"p_hat\": float(bits[mask].mean()) if nk>0 else np.nan})\n    return pd.DataFrame(rows)\n\ndef auc_mass_weighted(df: pd.DataFrame) -> float:\n    v = df.dropna()\n    w = v[\"n\"].to_numpy().astype(float)\n    if w.sum() == 0:\n        return float(\"nan\")\n    w = w / w.sum()\n    return float(np.sum(w * v[\"p_hat\"].to_numpy()))\n\ndef load_pkg(path: str) -> EvalPackage:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        obj = json.load(f)\n    bits = np.array(obj[\"bits\"], dtype=int)\n    cb = np.array(obj[\"cost_bins\"], dtype=int)\n    ev = obj[\"evidence\"]\n    sig = obj[\"signature\"]\n    return EvalPackage(bits=bits, cost_bins=cb, evidence=ev, signature=sig)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_o9_dgBHTebZ",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_o9_dgBHTebZ"
      },
      "outputs": [],
      "source": "df_runs = run_all_noverify(HF_MODELS, DATASETS_TO_RUN, EVAL_SEEDS)\ndisplay(df_runs.head())\nprint(\"Total runs:\", len(df_runs))"
    },
    {
      "cell_type": "markdown",
      "id": "fd5df5ef",
      "metadata": {
        "id": "fd5df5ef"
      },
      "source": [
        "\n",
        "## 12) Quick Discrimination Plot (complexity-tiered) for one dataset across all models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298a56c8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "298a56c8"
      },
      "outputs": [],
      "source": "\n# Pick one dataset to visualize\nexample_dataset = None\nfor cand in [\"gsm8k_answer_test\", \"gsm8k_binary\", \"aqua_binary\", \"arc_challenge_binary\"]:\n    if cand in DATASETS:\n        example_dataset = cand\n        break\n\nif example_dataset is None:\n    example_dataset = list(DATASETS.keys())[0] if len(DATASETS)>0 else None\n\nprint(\"Example dataset:\", example_dataset)\n\nif example_dataset is not None and len(df_runs) > 0:\n    K = len(COST_BINS) - 1\n    plt.figure()\n    for mid in HF_MODELS:\n        sub = df_runs[(df_runs[\"dataset\"]==example_dataset) & (df_runs[\"model\"]==mid)].head(1)\n        if len(sub)==0:\n            continue\n        p = sub.iloc[0][\"path\"]\n        pkg = load_pkg(p)\n        dfb = pass_rate_by_costbin(pkg.bits, pkg.cost_bins, K)\n        plt.plot(dfb[\"bin\"], dfb[\"p_hat\"], marker=\"o\", label=mid.split(\"/\")[-1])\n    plt.xlabel(\"Verifier-cost bin (coarse)\")\n    plt.ylabel(\"Pass rate\")\n    plt.title(f\"Complexity-tiered discrimination: {example_dataset}\")\n    plt.legend()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "id": "b0729605",
      "metadata": {
        "id": "b0729605"
      },
      "source": [
        "\n",
        "## 13) Optional: Baseline Corpora Loader (for n-gram overlap / contamination baselines)\n",
        "This just **connects** the corpora you used (train splits).  \n",
        "You can plug in your existing n-gram/PPL baseline code on top of this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c3b62ee",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8c3b62ee"
      },
      "outputs": [],
      "source": "\nbaseline_corpora: Dict[str, List[str]] = {}\n\ndef load_corpus_texts() -> Dict[str, List[str]]:\n    from datasets import load_dataset\n    corp = {}\n\n    # GSM8K train\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n    corp[\"gsm8k_train_questions\"] = [x[\"question\"] for x in ds]\n\n    # AQUA train\n    ds = load_dataset(\"aqua_rat\", split=\"train\")\n    corp[\"aqua_train_questions\"] = [x.get(\"question\",\"\") for x in ds]\n\n    # ARC train\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"train\")\n    corp[\"arc_challenge_train_stems\"] = [x[\"question\"].get(\"stem\",\"\") for x in ds]\n\n    return corp\n\nif RUN_BASELINE_CORPORA:\n    try:\n        baseline_corpora = load_corpus_texts()\n        for k,v in baseline_corpora.items():\n            print(k, \"docs:\", len(v))\n    except Exception as e:\n        print(\"Failed to load baseline corpora (need internet):\", e)\n"
    },
    {
      "cell_type": "markdown",
      "id": "1b380055",
      "metadata": {
        "id": "1b380055"
      },
      "source": [
        "\n",
        "## 14) Where your original experiment modules fit\n",
        "At this point, you have:\n",
        "- all models connected,\n",
        "- all datasets connected,\n",
        "- stable artifacts saved.\n",
        "\n",
        "Now you can run:\n",
        "- **Exp-1 auditability** (tamper tests) on any `(dataset,model)` pair by editing/altering `evidence/bits`.\n",
        "- **Exp-2 contamination** (fast proxy + LoRA) by creating a contaminated model variant and re-running `run_all`.\n",
        "- **Exp-3 discrimination** is already available (cost-binned curves).\n",
        "- **Exp-4 overhead** is computed via timing wrappers around `server.run`.\n",
        "\n",
        "If you want, I can also port your **coverage attack (S4)** blocks into this same artifact schema, but I didn't do it here because you asked specifically to connect all models and datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1f2afc",
      "metadata": {
        "id": "ef1f2afc"
      },
      "source": [
        "\n",
        "# 15) ICML Upgrade: Real LoRA Contamination Attack (GSM8K)\n",
        "This section closes the **realism gap** by training a **real cheater model** via LoRA:\n",
        "- Leak a fraction \\(\\rho\\) of a fixed GSM8K test **eval pool** into the training mix.\n",
        "- Keep training compute constant across \\(\\rho\\) (same steps, same mix size).\n",
        "- Run a BAP-style **bit-only** evaluation and plot **Reject Rate vs \\(\\rho\\)**.\n",
        "\n",
        "⚠️ Real training code. Default is OFF. Set `RUN_LORA=True` when ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ad9a8c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3ad9a8c"
      },
      "outputs": [],
      "source": "\n# =============== LoRA config knobs ===============\nRUN_LORA = True  # <<< set True to actually train\n\nLORA_BASE_MODEL = \"Qwen/Qwen3-4B\"  # fast\nLORA_RHOS = [0.0, 0.001, 0.005, 0.01, 0.05]  # contamination rates\n\n# Fixed compute across rhos\nLORA_TRAIN_MIX_SIZE = 512    # total training examples (constant)\nLORA_MAX_STEPS = 200         # constant steps (you can raise)\nLORA_LR = 2e-4\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nLORA_MAX_LEN = 512\n\n# Define leakage set on a fixed eval pool\nLORA_EVAL_POOL_SIZE = 1000\nLORA_POOL_SEED = 123\n\n# Where to store adapters\nLORA_DIR = os.path.join(RESULTS_DIR, \"lora_cheaters\")\nos.makedirs(LORA_DIR, exist_ok=True)\nprint(\"LORA_DIR:\", LORA_DIR)\n"
    },
    {
      "cell_type": "markdown",
      "id": "93f2bccb",
      "metadata": {
        "id": "93f2bccb"
      },
      "source": [
        "\n",
        "## 15.1 Load GSM8K train/test (for LoRA)\n",
        "We reuse the existing HF loader; we need the **train** split for LoRA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4071f97f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4071f97f"
      },
      "outputs": [],
      "source": "# Load GSM8K train/test for contamination-style experiments.\n# Prefer the fixed grouped split (gsm8k_train/gsm8k_test) if available; otherwise fall back to HF GSM8K.\ntry:\n    if \"gsm8k_train\" in DATASETS and \"gsm8k_test\" in DATASETS:\n        gsm8k_train_ds = DATASETS[\"gsm8k_train\"]\n        gsm8k_test_ds  = DATASETS[\"gsm8k_test\"]\n        print(\"[Grouped] GSM8K train:\", len(gsm8k_train_ds.instances), \"| test:\", len(gsm8k_test_ds.instances))\n    else:\n        if \"gsm8k_answer_test\" not in public_datasets:\n            public_datasets[\"gsm8k_answer_test\"] = load_gsm8k_answer(\"test\")\n        gsm8k_test_ds = public_datasets[\"gsm8k_answer_test\"]\n        gsm8k_train_ds = load_gsm8k_answer(\"train\")\n        print(\"[HF] GSM8K train:\", len(gsm8k_train_ds.instances), \"| test:\", len(gsm8k_test_ds.instances))\nexcept Exception as e:\n    gsm8k_train_ds = None\n    gsm8k_test_ds = None\n    print(\"Failed to load GSM8K train/test:\", e)\n"
    },
    {
      "cell_type": "markdown",
      "id": "b30c4cb8",
      "metadata": {
        "id": "b30c4cb8"
      },
      "source": [
        "\n",
        "## 15.2 LoRA training + evaluation helpers\n",
        "We avoid triple-quote docstrings here to keep notebook generation robust.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184ca028",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "184ca028"
      },
      "outputs": [],
      "source": "\ndef gsm8k_prompt_completion(inst: TaskInstance) -> str:\n    # Same \"final answer only\" style as evaluation; no CoT.\n    gt = extract_gsm8k_final(inst.ground_truth or \"\") or \"\"\n    prompt = (\n        \"Solve the following math problem. Return ONLY the final numeric answer.\\n\\n\"\n        f\"Problem:\\n{inst.prompt}\\n\\nFinal answer:\"\n    )\n    return prompt + \" \" + gt\n\ndef make_eval_pool_indices(N_test: int, pool_size: int, seed: int):\n    rng = np.random.default_rng(seed)\n    m = min(pool_size, N_test)\n    return rng.choice(N_test, size=m, replace=False).tolist()\n\ndef make_training_mix_for_rho(train_ds: TaskDataset, test_ds: TaskDataset, eval_pool_idx, rho: float, mix_size: int, seed: int = 0):\n    # Returns (train_rows, leaked_ids)\n    rng = np.random.default_rng(seed)\n    pool = [test_ds.instances[i] for i in eval_pool_idx]\n    m = int(round(rho * len(pool)))\n    leaked = pool[:m]  # monotone leakage by prefix\n    leaked_ids = set(x.instance_id for x in leaked)\n\n    rows = [{\"text\": gsm8k_prompt_completion(x)} for x in leaked]\n    need = mix_size - len(rows)\n    if need > 0:\n        idx = rng.choice(len(train_ds.instances), size=need, replace=False)\n        rows.extend({\"text\": gsm8k_prompt_completion(train_ds.instances[int(i)])} for i in idx)\n    rng.shuffle(rows)\n    return rows, leaked_ids\n\ndef train_lora_adapter(base_model_id: str, train_rows, out_dir: str, max_steps: int, lr: float, r: int, alpha: int, dropout: float, max_len: int = 512):\n    # Real LoRA training. Requires transformers + peft.\n    import torch\n    from datasets import Dataset as HFDataset\n    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n    from peft import LoraConfig, get_peft_model, TaskType\n\n    tok = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n    model.train()\n\n    hf = HFDataset.from_list(train_rows)\n\n    def tok_fn(batch):\n        return tok(batch[\"text\"], truncation=True, max_length=max_len)\n\n    hf = hf.map(tok_fn, batched=True, remove_columns=[\"text\"])\n\n    lcfg = LoraConfig(r=r, lora_alpha=alpha, lora_dropout=dropout, task_type=TaskType.CAUSAL_LM)\n    model = get_peft_model(model, lcfg)\n\n    args = TrainingArguments(\n        output_dir=out_dir,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        max_steps=max_steps,\n        learning_rate=lr,\n        logging_steps=max(1, max_steps//10),\n        save_steps=max_steps,\n        fp16=True,\n        report_to=[],\n        remove_unused_columns=False,\n    )\n\n    collator = DataCollatorForLanguageModeling(tok, mlm=False)\n    trainer = Trainer(model=model, args=args, train_dataset=hf, data_collator=collator)\n    trainer.train()\n    model.save_pretrained(out_dir)\n    tok.save_pretrained(out_dir)\n    print(\"Saved LoRA adapter:\", out_dir)\n\n# --- Minimal adapter-aware evaluator (bit-only output) ---\nfrom dataclasses import dataclass\nfrom typing import Optional, Any, Dict\n\n@dataclass\nclass EvalPkg2:\n    bits: np.ndarray\n    cost_bins: np.ndarray\n    evidence: Dict[str, Any]\n    signature: str\n\ndef digest_model_with_adapter(model_id: str, adapter_path: Optional[str]) -> str:\n    return sha256_json({\"model_id\": model_id, \"adapter_path\": adapter_path})\n\ndef load_base_or_lora(model_id: str, adapter_path: Optional[str]):\n    # Load base model and optionally attach a LoRA adapter.\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    kwargs = {\"device_map\": \"auto\"}\n    if \"USE_4BIT\" in globals() and USE_4BIT:\n        kwargs = {\"load_in_4bit\": True, \"device_map\": \"auto\"}\n\n    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n    model.eval()\n    if adapter_path is not None:\n        from peft import PeftModel\n        model = PeftModel.from_pretrained(model, adapter_path)\n        model.eval()\n    return tok, model\n\ndef generate_answer(tok, model, question: str, gen_cfg: dict):\n    import torch\n    from transformers import GenerationConfig\n    prompt = (\n        \"Solve the following math problem. Return ONLY the final numeric answer.\\n\\n\"\n        f\"Problem:\\n{question}\\n\\nFinal answer:\"\n    )\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    gcfg = GenerationConfig(**gen_cfg)\n    with torch.no_grad():\n        out = model.generate(**inputs, generation_config=gcfg)\n    text = tok.decode(out[0], skip_special_tokens=True)\n    if text.startswith(prompt):\n        return text[len(prompt):].strip()\n    return text.strip()\n\ndef bap_run_with_adapter(dataset: TaskDataset, model_id: str, adapter_path: Optional[str], run_nonce: str, n: int, with_replacement: bool):\n    # BAP-style: seed->indices->bits/costbins, plus receipt.\n    task_type = \"answer\"\n    gen_cfg = GEN_CFG_ANSWER if \"GEN_CFG_ANSWER\" in globals() else dict(max_new_tokens=256, do_sample=False, temperature=0.0)\n\n    cfg_digest = digest_config(task_type, n, with_replacement, gen_cfg)\n    ds_digest = digest_dataset(dataset, include_text=False)\n    m_digest = digest_model_with_adapter(model_id, adapter_path)\n\n    seed = derive_seed(PUBLIC_BEACON, ds_digest, m_digest, cfg_digest, run_nonce)\n    idx = sample_indices(seed, len(dataset.instances), n, with_replacement)\n    idx_commit = indices_commitment(idx)\n\n    bits = np.zeros(n, dtype=int)\n    cb = np.zeros(n, dtype=int)\n\n    if not DRY_RUN:\n        tok, model = load_base_or_lora(model_id, adapter_path)\n    for t, j in enumerate(idx):\n        inst = dataset.instances[int(j)]\n        out = \"0\" if DRY_RUN else generate_answer(tok, model, inst.prompt, gen_cfg)\n        b, steps = score_and_cost(inst, out)\n        bits[t] = b\n        cb[t] = cost_bin(int(steps))\n\n    evidence = {\n        \"code_digest\": digest_code(CODE_ID),  # reuse\n        \"dataset_digest\": ds_digest,\n        \"model_digest\": m_digest,\n        \"config_digest\": cfg_digest,\n        \"public_beacon\": PUBLIC_BEACON,\n        \"run_nonce\": run_nonce,\n        \"seed\": int(seed),\n        \"index_commitment\": idx_commit,\n        \"output_hash\": hash_outputs(bits, cb),\n    }\n    sig = sign_hmac(evidence)\n    return EvalPkg2(bits=bits, cost_bins=cb, evidence=evidence, signature=sig)\n"
    },
    {
      "cell_type": "markdown",
      "id": "5d4d92e5",
      "metadata": {
        "id": "5d4d92e5"
      },
      "source": [
        "\n",
        "## 15.3 Run the LoRA contamination curve (Reject Rate vs \\(\\rho\\))\n",
        "This produces:\n",
        "- `df_lora_curve`: per-rho per-seed results\n",
        "- a plot: reject rate vs rho\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35383d52",
      "metadata": {
        "id": "35383d52"
      },
      "outputs": [],
      "source": "\nif RUN_LORA:\n    assert gsm8k_train_ds is not None and gsm8k_test_ds is not None, \"GSM8K train/test not available.\"\n\n    eval_pool_idx = make_eval_pool_indices(len(gsm8k_test_ds.instances), LORA_EVAL_POOL_SIZE, LORA_POOL_SEED)\n\n    # Pilot p0 from base model (same BAP sampling interface)\n    p0_list = []\n    for s in EVAL_SEEDS:\n        pkg0 = bap_run_with_adapter(gsm8k_test_ds, LORA_BASE_MODEL, adapter_path=None,\n                                    run_nonce=f\"pilot|base|seed{s}\", n=N_EVAL, with_replacement=WITH_REPLACEMENT)\n        p0_list.append(float(pkg0.bits.mean()))\n    p0 = float(np.mean(p0_list))\n    print(\"Pilot p0 (base acc estimate):\", p0)\n\n    rows = []\n    leaked_registry = {}  # rho -> leaked_ids (open-track only)\n    for rho in LORA_RHOS:\n        adapter_dir = os.path.join(LORA_DIR, f\"{safe_name(LORA_BASE_MODEL)}__rho{rho}\")\n        os.makedirs(adapter_dir, exist_ok=True)\n\n        train_rows, leaked_ids = make_training_mix_for_rho(\n            gsm8k_train_ds, gsm8k_test_ds, eval_pool_idx, rho=rho, mix_size=LORA_TRAIN_MIX_SIZE, seed=int(10_000*rho)+7\n        )\n        leaked_registry[rho] = leaked_ids\n\n        if rho == 0.0:\n            adapter_use = None\n        else:\n            marker = os.path.join(adapter_dir, \"adapter_config.json\")\n            if not os.path.exists(marker):\n                train_lora_adapter(\n                    base_model_id=LORA_BASE_MODEL,\n                    train_rows=train_rows,\n                    out_dir=adapter_dir,\n                    max_steps=LORA_MAX_STEPS,\n                    lr=LORA_LR,\n                    r=LORA_R,\n                    alpha=LORA_ALPHA,\n                    dropout=LORA_DROPOUT,\n                    max_len=LORA_MAX_LEN,\n                )\n            else:\n                print(\"Adapter exists; skipping training:\", adapter_dir)\n            adapter_use = adapter_dir\n\n        for s in EVAL_SEEDS:\n            pkg = bap_run_with_adapter(gsm8k_test_ds, LORA_BASE_MODEL, adapter_path=adapter_use,\n                                       run_nonce=f\"lora|rho{rho}|seed{s}\", n=N_EVAL, with_replacement=WITH_REPLACEMENT)\n            assert verify_hmac(pkg.evidence, pkg.signature)\n            k = int(pkg.bits.sum()); n = len(pkg.bits)\n            pv = stats.binomtest(k, n, p=p0, alternative=\"greater\").pvalue\n            rows.append({\"rho\": rho, \"seed\": s, \"acc\": float(pkg.bits.mean()), \"p_value\": float(pv), \"reject\": bool(pv < 0.05)})\n\n    df_lora_curve = pd.DataFrame(rows)\n    display(df_lora_curve.groupby(\"rho\").agg(acc_mean=(\"acc\",\"mean\"), reject_rate=(\"reject\",\"mean\"), p_med=(\"p_value\",\"median\")).reset_index())\n\n    g = df_lora_curve.groupby(\"rho\")[\"reject\"].mean().reset_index()\n    plt.figure()\n    plt.plot(g[\"rho\"]*100, g[\"reject\"], marker=\"o\")\n    plt.axhline(0.05, linestyle=\"--\")\n    plt.xlabel(\"Contamination rate rho (%)\")\n    plt.ylabel(\"Reject rate (power proxy at alpha=0.05)\")\n    plt.title(\"BAP Detection Power vs REAL LoRA Contamination (GSM8K)\")\n    plt.show()\nelse:\n    print(\"RUN_LORA=False; skipping real LoRA training. Set RUN_LORA=True to run.\")\n"
    },
    {
      "cell_type": "markdown",
      "id": "c955a6b6",
      "metadata": {
        "id": "c955a6b6"
      },
      "source": [
        "\n",
        "# 16) ICML Upgrade: Baselines (N-gram overlap / PPL / Min-K% Prob)\n",
        "This section closes the **baseline gap**.\n",
        "\n",
        "Baselines:\n",
        "1. **N-gram overlap** between GSM8K test questions and GSM8K train questions.\n",
        "2. **PPL** and **Min-K% Prob** of the suspect model on the question text.\n",
        "\n",
        "We report **AUC** for identifying contaminated items (labels are known in the LoRA experiment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371289b0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "371289b0"
      },
      "outputs": [],
      "source": "\n# ---- AUC helper (no sklearn required) ----\ndef auc_from_scores(scores: np.ndarray, labels01: np.ndarray) -> float:\n    scores = np.asarray(scores, dtype=float)\n    labels01 = np.asarray(labels01, dtype=int)\n    pos = scores[labels01 == 1]\n    neg = scores[labels01 == 0]\n    if len(pos) == 0 or len(neg) == 0:\n        return float(\"nan\")\n    order = np.argsort(scores)\n    ranks = np.empty_like(order, dtype=float)\n    ranks[order] = np.arange(1, len(scores)+1)\n\n    uniq, inv, cnt = np.unique(scores, return_inverse=True, return_counts=True)\n    if np.any(cnt > 1):\n        for i, c in enumerate(cnt):\n            if c > 1:\n                idx = np.where(inv == i)[0]\n                ranks[idx] = ranks[idx].mean()\n\n    rank_sum_pos = ranks[labels01 == 1].sum()\n    n_pos = len(pos); n_neg = len(neg)\n    return float((rank_sum_pos - n_pos*(n_pos+1)/2) / (n_pos*n_neg))\n\n# ---- N-gram overlap baseline ----\n_word_re = re.compile(r\"[A-Za-z0-9]+\")\ndef word_ngrams(text: str, n: int = 5):\n    toks = [t.lower() for t in _word_re.findall(text)]\n    if len(toks) < n:\n        return []\n    return [\" \".join(toks[i:i+n]) for i in range(len(toks)-n+1)]\n\ndef hash_ngram(s: str) -> int:\n    h = hashlib.blake2b(s.encode(\"utf-8\"), digest_size=8).digest()\n    return int.from_bytes(h, \"big\")\n\ndef build_ngram_set(corpus_texts, n: int = 5):\n    hs = set()\n    for txt in corpus_texts:\n        for ng in word_ngrams(txt, n=n):\n            hs.add(hash_ngram(ng))\n    return hs\n\ndef ngram_overlap_score(text: str, ngram_set: set, n: int = 5) -> float:\n    ngs = word_ngrams(text, n=n)\n    if not ngs:\n        return 0.0\n    hit = sum((hash_ngram(ng) in ngram_set) for ng in ngs)\n    return float(hit / len(ngs))\n"
    },
    {
      "cell_type": "markdown",
      "id": "a3ba1a61",
      "metadata": {
        "id": "a3ba1a61"
      },
      "source": [
        "\n",
        "## 16.1 Build GSM8K train 5-gram set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def18871",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "def18871"
      },
      "outputs": [],
      "source": "\nngset_gsm8k_5 = None\nif gsm8k_train_ds is not None:\n    ngset_gsm8k_5 = build_ngram_set([x.prompt for x in gsm8k_train_ds.instances], n=5)\n    print(\"Built GSM8K train 5-gram set size:\", len(ngset_gsm8k_5))\nelse:\n    print(\"GSM8K train not available; cannot build n-gram baseline.\")\n"
    },
    {
      "cell_type": "markdown",
      "id": "907dfd81",
      "metadata": {
        "id": "907dfd81"
      },
      "source": [
        "\n",
        "## 16.2 PPL / Min-K% baselines (likelihood features)\n",
        "These require a forward pass on each question. Start with `BASELINE_MAX_ITEMS=256`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f61ef1b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2f61ef1b"
      },
      "outputs": [],
      "source": "\ndef compute_token_logprobs(tok, model, text: str, max_length: int = 512) -> np.ndarray:\n    import torch\n    enc = tok(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n    input_ids = enc[\"input_ids\"].to(model.device)\n    attn = enc.get(\"attention_mask\", None)\n    if attn is not None:\n        attn = attn.to(model.device)\n\n    with torch.no_grad():\n        out = model(input_ids=input_ids, attention_mask=attn)\n        logits = out.logits\n\n    shift_logits = logits[:, :-1, :]\n    shift_labels = input_ids[:, 1:]\n    log_probs = torch.log_softmax(shift_logits, dim=-1)\n    token_lp = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n    return token_lp.squeeze(0).detach().cpu().numpy()\n\ndef perplexity_from_logprobs(token_logprobs: np.ndarray) -> float:\n    nll = -float(np.mean(token_logprobs))\n    return float(np.exp(nll))\n\ndef min_k_avg_logprob(token_logprobs: np.ndarray, k_frac: float = 0.2) -> float:\n    L = len(token_logprobs)\n    if L == 0:\n        return float(\"nan\")\n    k = max(1, int(round(k_frac * L)))\n    idx = np.argsort(token_logprobs)[:k]\n    return float(np.mean(token_logprobs[idx]))\n"
    },
    {
      "cell_type": "markdown",
      "id": "75638f0e",
      "metadata": {
        "id": "75638f0e"
      },
      "source": [
        "\n",
        "## 16.3 Compute baseline AUC (requires `leaked_registry` from LoRA section)\n",
        "If you haven't run LoRA yet, this cell will safely skip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0e8fee",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "df0e8fee"
      },
      "outputs": [],
      "source": "\nBASELINE_MAX_ITEMS = 256\nK_FRAC = 0.2\n\nif \"leaked_registry\" in globals() and gsm8k_test_ds is not None and \"eval_pool_idx\" in globals():\n    # Choose a rho to evaluate (pick 1% if present; else highest rho)\n    rho_star = 0.01 if 0.01 in leaked_registry else sorted(leaked_registry.keys())[-1]\n    contam_ids = leaked_registry[rho_star]\n\n    pool_idx = eval_pool_idx[:BASELINE_MAX_ITEMS]\n    y = np.array([1 if gsm8k_test_ds.instances[i].instance_id in contam_ids else 0 for i in pool_idx], dtype=int)\n\n    # N-gram overlap AUC (higher overlap => more likely contaminated)\n    if ngset_gsm8k_5 is not None:\n        ng_scores = np.array([ngram_overlap_score(gsm8k_test_ds.instances[i].prompt, ngset_gsm8k_5, n=5) for i in pool_idx], dtype=float)\n        print(f\"AUC(N-gram overlap, rho={rho_star}):\", auc_from_scores(ng_scores, y))\n    else:\n        print(\"No n-gram set; skipping overlap baseline.\")\n\n    # Likelihood baselines under the suspect model\n    adapter_dir = os.path.join(LORA_DIR, f\"{safe_name(LORA_BASE_MODEL)}__rho{rho_star}\")\n    if rho_star == 0.0:\n        adapter_dir = None\n\n    try:\n        tok, model = load_base_or_lora(LORA_BASE_MODEL, adapter_dir)\n        ppl_scores = []\n        mink_scores = []\n        for i in pool_idx:\n            q = gsm8k_test_ds.instances[i].prompt\n            lp = compute_token_logprobs(tok, model, q, max_length=512)\n            ppl_scores.append(-perplexity_from_logprobs(lp))  # negative PPL => higher means more memorized\n            mink_scores.append(min_k_avg_logprob(lp, k_frac=K_FRAC))\n\n        ppl_scores = np.array(ppl_scores, dtype=float)\n        mink_scores = np.array(mink_scores, dtype=float)\n\n        print(f\"AUC(-PPL, rho={rho_star}):\", auc_from_scores(ppl_scores, y))\n        print(f\"AUC(Min-K avg logprob, rho={rho_star}):\", auc_from_scores(mink_scores, y))\n    except Exception as e:\n        print(\"Likelihood baselines failed (missing deps/GPU?):\", e)\nelse:\n    print(\"LoRA artifacts not found yet. Run section 15 first, then rerun this cell.\")\n"
    },
    {
      "cell_type": "markdown",
      "id": "2e986174",
      "metadata": {
        "id": "2e986174"
      },
      "source": [
        "\n",
        "# 17) ICML Upgrade: Statistical Power Analysis (justify `N_EVAL`)\n",
        "Reviewer question: “why `N_EVAL=400`?”\n",
        "\n",
        "Conservative distribution-free planning (Hoeffding-style):\n",
        "\\[\n",
        "n \\ge \\frac{2}{\\Delta^2}\\log\\Big(\\frac{1}{\\min(\\alpha,\\beta)}\\Big),\n",
        "\\]\n",
        "where \\(\\Delta = p_1 - p_0\\), \\(\\alpha\\) is significance, and \\(1-\\beta\\) is power.\n",
        "\n",
        "We provide:\n",
        "- a planning table for typical \\(\\Delta\\),\n",
        "- an empirical estimate of \\(\\Delta(\\rho)\\) from the LoRA curve (if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba251dff",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ba251dff"
      },
      "outputs": [],
      "source": "\ndef n_required_hoeffding(delta: float, alpha: float = 0.05, beta: float = 0.05) -> int:\n    if delta <= 0:\n        return math.inf\n    return int(math.ceil((2.0 / (delta**2)) * math.log(1.0 / min(alpha, beta))))\n\nprint(\"Planning table (alpha=beta=0.05):\")\nfor d in [0.01, 0.02, 0.03, 0.05, 0.08, 0.10]:\n    print(f\"delta={d:.2f} -> n_required={n_required_hoeffding(d)}\")\nprint(\"Current N_EVAL:\", N_EVAL)\n"
    },
    {
      "cell_type": "markdown",
      "id": "5a6c7965",
      "metadata": {
        "id": "5a6c7965"
      },
      "source": [
        "\n",
        "## 17.1 Empirical power from LoRA curve (if available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb40265",
      "metadata": {
        "id": "9bb40265"
      },
      "outputs": [],
      "source": "\nif \"df_lora_curve\" in globals():\n    df = df_lora_curve.copy()\n    p0 = float(df[df[\"rho\"]==0.0][\"acc\"].mean())\n    rows = []\n    for rho in sorted(df[\"rho\"].unique()):\n        p1 = float(df[df[\"rho\"]==rho][\"acc\"].mean())\n        delta = p1 - p0\n        rows.append({\"rho\": rho, \"p0\": p0, \"p1\": p1, \"delta\": delta, \"n_required(alpha=beta=0.05)\": n_required_hoeffding(delta, 0.05, 0.05)})\n    display(pd.DataFrame(rows))\n    print(\"Current N_EVAL:\", N_EVAL)\nelse:\n    print(\"No LoRA curve yet. Run section 15 to estimate delta(rho) empirically.\")\n"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}